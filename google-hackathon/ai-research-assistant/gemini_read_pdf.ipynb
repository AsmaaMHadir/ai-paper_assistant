{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the text content to Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file '' as: https://generativelanguage.googleapis.com/v1beta/files/6w0nlo0b8may\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(model_name='gemini-1.5-pro-latest')\n",
    "\n",
    "sample_file = genai.upload_file(path=\"/home/asmaa/google-hackathon/ai-research-assistant/test.txt\",\n",
    "                            display_name=\"paper\")\n",
    "\n",
    "print(f\"Uploaded file '{sample_file.display_name}' as: {sample_file.uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Key Ideas in the Holmes Benchmark Research Paper:\\n\\n* **Holmes Benchmark:** This paper introduces Holmes, a new benchmark designed to evaluate the linguistic competence of Language Models (LMs) by analyzing their internal representations through probing classifiers. \\n* **Focus on Linguistic Competence:** Unlike previous methods that relied on prompting, Holmes isolates and assesses specific linguistic phenomena like syntax, morphology, and semantics,  separating them from other cognitive abilities such as following instructions.\\n* **Comprehensive Dataset:** Holmes comprises over 200 datasets, drawing from more than 250 probing studies, to comprehensively evaluate various aspects of linguistic understanding.\\n* **Model Analysis:** The research analyzes over 50 LMs and finds that linguistic competence generally correlates with model size, aligning with existing knowledge. However, model architecture and instruction tuning also play a significant role, especially for morphology and syntax.\\n* **Efficiency with FlashHolmes:** To address the computational demands of the full Holmes benchmark, the paper proposes FlashHolmes, a streamlined version that maintains high accuracy while reducing the computational load.\\n* **Key Findings:** The results showcase the varying strengths and weaknesses of different LMs across diverse linguistic phenomena, highlighting the need for comprehensive evaluation tools like Holmes. \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input = \"Andreas Waldis∗1,2, Yotam Perlitz3, Leshem Choshen4,5, Yufang Hou6, Iryna Gurevych1\\n1Ubiquitous Knowledge Processing Lab (UKP Lab) Department of Computer Science and Hessian Center for AI (hessian.AI) Technical University of Darmstadt 2Information Systems Research Lab, Lucerne University of Applied Sciences and Arts 3IBM Research AI, 4MIT CSAIL, 5MIT-IBM Watson AI Lab, 6IBM Research Europe - Ireland www.ukp.tu-darmstadt.de www.hslu.ch\\nAbstract\\nWe introduce Holmes, a benchmark to assess the linguistic competence of language models (LMs) – their ability to grasp linguistic phe- nomena.\\nUnlike prior prompting-based evalua- tions, Holmes assesses the linguistic compe- tence of LMs via their internal representations using classifier-based probing.\\nIn doing so, we disentangle specific phenomena (e.g., part-of- speech of words) from other cognitive abilities, like following textual instructions, and meet recent calls to assess LMs’ linguistic compe- tence in isolation.\\nComposing Holmes, we review over 250 probing studies and feature more than 200 datasets to assess syntax, mor- phology, semantics, reasoning, and discourse phenomena.\\nAnalyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size.\\nHow- ever, surprisingly, model architecture and in- struction tuning also significantly influence per- formance, particularly in morphology and syn- tax.\\nFinally, we propose FlashHolmes, a streamlined version of Holmes designed to lower the high computation load while main- taining high-ranking precision.\\nFigure 1: A subset of Holmes rankings (↓) for various evaluated LMs.\\nFLAN-UL2 outperforms the others overall, while different LMs prevail for the five distinct types of linguistic phenomena.\\nholmes-benchmark.github.io\"\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n",
    "\n",
    "response = model.generate_content([\"summarize the key ideas in this snippet of a research paper.\", sample_input])\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
