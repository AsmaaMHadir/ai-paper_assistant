{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data to be considered:\n",
    "\n",
    "- author\n",
    "- Abstract\n",
    "- Sections - \n",
    "- Code \n",
    "- URLs\n",
    "- Tables\n",
    "- Images\n",
    "- References\n",
    "- Manuscript \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"/home/asmaa/google-hackathon/ai-research-assistant/sample-assets/2404.18923v1.pdf\",\n",
    "         \"/home/asmaa/google-hackathon/ai-research-assistant/sample-assets/2404.18928v1.pdf\",\n",
    "         \"/home/asmaa/google-hackathon/ai-research-assistant/sample-assets/2404.18930v1.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pdf dource files and read them as \n",
    "def read_file_layout(file_name):\n",
    "    llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "    pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "    doc = pdf_reader.read_pdf(file_name)\n",
    "    return doc\n",
    "\n",
    "def get_files_as_docs(files):\n",
    "    docs_lst = []\n",
    "\n",
    "    for file in files:\n",
    "        doc = read_file_layout(file)\n",
    "        docs_lst.append(doc)\n",
    "        \n",
    "    return docs_lst\n",
    "\n",
    "def get_chunk_paths(chunk):\n",
    "    pattern = re.compile(r'^([^\\n]+)$', re.MULTILINE)\n",
    "    matches = pattern.findall(chunk)\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_details(chunk,section_title):\n",
    "    chunk_details = []\n",
    "    for chunk in chunk:\n",
    "        chunk_text = chunk.to_context_text(include_section_info=True)\n",
    "        chunk_path =  get_chunk_paths(chunk_text)\n",
    "        chunk_page_num = chunk.page_idx\n",
    "        chunk_details.append({\"text\":chunk.to_text(),\n",
    "                                \"title\":chunk_path,\n",
    "                                \"page\":chunk_page_num,\n",
    "                                \"source_doc\":section_title})\n",
    "    return chunk_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_sections(doc):\n",
    "    \n",
    "    sections_details = []\n",
    "    sections = doc.sections()\n",
    "    for section in sections:\n",
    "        section_text = section.to_text(include_children=True,recurse=True)\n",
    "        section_title = section.title\n",
    "        section_chunks = section.chunks()\n",
    "        section_tables = section.tables()\n",
    "        section_details = {\"text\":section_text,\n",
    "                           \"title\":section_title,\n",
    "                           \"chunks\":get_chunks_details(section_chunks,section_title),\n",
    "                           \"tables\":section_tables}\n",
    "        sections_details.append(section_details)\n",
    "    return sections_details\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': 'Holmes', 'title': 'Holmes', 'chunks': [], 'tables': []},\n",
       "  {'text': 'Benchmark the Linguistic Competence of Language Models',\n",
       "   'title': 'Benchmark the Linguistic Competence of Language Models',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Andreas Waldis∗1,2, Yotam Perlitz3, Leshem Choshen4,5, Yufang Hou6, Iryna Gurevych1\\n1Ubiquitous Knowledge Processing Lab (UKP Lab) Department of Computer Science and Hessian Center for AI (hessian.AI) Technical University of Darmstadt 2Information Systems Research Lab, Lucerne University of Applied Sciences and Arts 3IBM Research AI, 4MIT CSAIL, 5MIT-IBM Watson AI Lab, 6IBM Research Europe - Ireland www.ukp.tu-darmstadt.de www.hslu.ch\\nAbstract\\nWe introduce Holmes, a benchmark to assess the linguistic competence of language models (LMs) – their ability to grasp linguistic phe- nomena.\\nUnlike prior prompting-based evalua- tions, Holmes assesses the linguistic compe- tence of LMs via their internal representations using classifier-based probing.\\nIn doing so, we disentangle specific phenomena (e.g., part-of- speech of words) from other cognitive abilities, like following textual instructions, and meet recent calls to assess LMs’ linguistic compe- tence in isolation.\\nComposing Holmes, we review over 250 probing studies and feature more than 200 datasets to assess syntax, mor- phology, semantics, reasoning, and discourse phenomena.\\nAnalyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size.\\nHow- ever, surprisingly, model architecture and in- struction tuning also significantly influence per- formance, particularly in morphology and syn- tax.\\nFinally, we propose FlashHolmes, a streamlined version of Holmes designed to lower the high computation load while main- taining high-ranking precision.\\nFigure 1: A subset of Holmes rankings (↓) for various evaluated LMs.\\nFLAN-UL2 outperforms the others overall, while different LMs prevail for the five distinct types of linguistic phenomena.\\nholmes-benchmark.github.io',\n",
       "   'title': 'Andreas Waldis∗1,2, Yotam Perlitz3, Leshem Choshen4,5, Yufang Hou6, Iryna Gurevych1',\n",
       "   'chunks': [{'text': '1Ubiquitous Knowledge Processing Lab (UKP Lab) Department of Computer Science and Hessian Center for AI (hessian.AI) Technical University of Darmstadt 2Information Systems Research Lab, Lucerne University of Applied Sciences and Arts 3IBM Research AI, 4MIT CSAIL, 5MIT-IBM Watson AI Lab, 6IBM Research Europe - Ireland www.ukp.tu-darmstadt.de www.hslu.ch',\n",
       "     'title': 'Andreas Waldis∗1,2, Yotam Perlitz3, Leshem Choshen4,5, Yufang Hou6, Iryna Gurevych1',\n",
       "     'page': 0,\n",
       "     'source_doc': 'Andreas Waldis∗1,2, Yotam Perlitz3, Leshem Choshen4,5, Yufang Hou6, Iryna Gurevych1'},\n",
       "    {'text': 'We introduce Holmes, a benchmark to assess the linguistic competence of language models (LMs) – their ability to grasp linguistic phe- nomena.\\nUnlike prior prompting-based evalua- tions, Holmes assesses the linguistic compe- tence of LMs via their internal representations using classifier-based probing.\\nIn doing so, we disentangle specific phenomena (e.g., part-of- speech of words) from other cognitive abilities, like following textual instructions, and meet recent calls to assess LMs’ linguistic compe- tence in isolation.\\nComposing Holmes, we review over 250 probing studies and feature more than 200 datasets to assess syntax, mor- phology, semantics, reasoning, and discourse phenomena.\\nAnalyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size.\\nHow- ever, surprisingly, model architecture and in- struction tuning also significantly influence per- formance, particularly in morphology and syn- tax.\\nFinally, we propose FlashHolmes, a streamlined version of Holmes designed to lower the high computation load while main- taining high-ranking precision.\\nFigure 1: A subset of Holmes rankings (↓) for various evaluated LMs.\\nFLAN-UL2 outperforms the others overall, while different LMs prevail for the five distinct types of linguistic phenomena.\\nholmes-benchmark.github.io',\n",
       "     'title': 'Andreas Waldis∗1,2, Yotam Perlitz3, Leshem Choshen4,5, Yufang Hou6, Iryna Gurevych1 > Abstract',\n",
       "     'page': 0,\n",
       "     'source_doc': 'Andreas Waldis∗1,2, Yotam Perlitz3, Leshem Choshen4,5, Yufang Hou6, Iryna Gurevych1'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Abstract\\nWe introduce Holmes, a benchmark to assess the linguistic competence of language models (LMs) – their ability to grasp linguistic phe- nomena.\\nUnlike prior prompting-based evalua- tions, Holmes assesses the linguistic compe- tence of LMs via their internal representations using classifier-based probing.\\nIn doing so, we disentangle specific phenomena (e.g., part-of- speech of words) from other cognitive abilities, like following textual instructions, and meet recent calls to assess LMs’ linguistic compe- tence in isolation.\\nComposing Holmes, we review over 250 probing studies and feature more than 200 datasets to assess syntax, mor- phology, semantics, reasoning, and discourse phenomena.\\nAnalyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size.\\nHow- ever, surprisingly, model architecture and in- struction tuning also significantly influence per- formance, particularly in morphology and syn- tax.\\nFinally, we propose FlashHolmes, a streamlined version of Holmes designed to lower the high computation load while main- taining high-ranking precision.\\nFigure 1: A subset of Holmes rankings (↓) for various evaluated LMs.\\nFLAN-UL2 outperforms the others overall, while different LMs prevail for the five distinct types of linguistic phenomena.\\nholmes-benchmark.github.io',\n",
       "   'title': 'Abstract',\n",
       "   'chunks': [{'text': 'We introduce Holmes, a benchmark to assess the linguistic competence of language models (LMs) – their ability to grasp linguistic phe- nomena.\\nUnlike prior prompting-based evalua- tions, Holmes assesses the linguistic compe- tence of LMs via their internal representations using classifier-based probing.\\nIn doing so, we disentangle specific phenomena (e.g., part-of- speech of words) from other cognitive abilities, like following textual instructions, and meet recent calls to assess LMs’ linguistic compe- tence in isolation.\\nComposing Holmes, we review over 250 probing studies and feature more than 200 datasets to assess syntax, mor- phology, semantics, reasoning, and discourse phenomena.\\nAnalyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size.\\nHow- ever, surprisingly, model architecture and in- struction tuning also significantly influence per- formance, particularly in morphology and syn- tax.\\nFinally, we propose FlashHolmes, a streamlined version of Holmes designed to lower the high computation load while main- taining high-ranking precision.\\nFigure 1: A subset of Holmes rankings (↓) for various evaluated LMs.\\nFLAN-UL2 outperforms the others overall, while different LMs prevail for the five distinct types of linguistic phenomena.\\nholmes-benchmark.github.io',\n",
       "     'title': 'Andreas Waldis∗1,2, Yotam Perlitz3, Leshem Choshen4,5, Yufang Hou6, Iryna Gurevych1 > Abstract',\n",
       "     'page': 0,\n",
       "     'source_doc': 'Abstract'}],\n",
       "   'tables': []},\n",
       "  {'text': '1 Introduction\\nquestions, benchmarks estimate cognitive abilities by providing textual instructions and evaluate LMs’ responses, as done for mathematical reasoning (Cobbe et al., 2021) or factual knowledge (Petroni et al., 2019, 2020).\\nHowever, they conflate latent abilities (like following provided instructions) with those under test, such as understanding specific linguistic phenomena, e.g., syntactic structures (Liang et al., 2023).\\nAs this entanglement makes it infeasible to draw definitive conclusions about distinct abilities (Hu and Levy, 2023), recent studies call to assess the linguistic competence of LMs comprehensively and in isolation (Lu et al., 2023; Mahowald et al., 2024).\\ncompetence is the unconscious understanding of language, like grasping grammatical rules (Chomsky, 1965).\\nAs language models (LMs) are trained on simple tasks like next word prediction (Brown et al., 2020), one might naturally wonder: What is the linguistic competence of LMs, and how do they differ?\\nTo answer such ∗* Corresponding author andreas.waldis@live.com In this work, we introduce the Holmes (Figure 2).\\nA benchmark to assess the linguistic competence of LMs (Figure 1) regarding numerous linguistic phenomena.\\nTo fully disentangle the understanding of these phenomena and other abilities of LMs, we use classifier-based probing (Tenney et al., 2019a; Hewitt and Manning, 2019; Belinkov, 2022).\\nA method that uses the LMs’ internal representations of text inputs to train linear models\\nFigure 2: Overview of Holmes (left) with the five phenomena types (right) and an example of probing-based evaluations for part-of-speech: encoding the input tokens and predicting the POS tag for cucumber, here NN.\\n(probes) to predict specific aspects of phenomena, such as words’ part-of-speech (POS).\\nWe then approximate the LMs’ grasp of these phenomena using the probes’ performance, rigorously verified using control tasks (Hewitt and Liang, 2019) and from an information theory perspective (Voita and Titov, 2020).\\nWith this particular and comprehensive scope, we thoroughly address the initially raised questions as follows:\\nMeta-Study (§ 3) The review of over 270 probing studies reveals a gap in comprehensively evaluating linguistic competence.\\nDespite covering over 200 probing tasks and 150 LMs, individual studies focus on particular tasks and LMs.\\nAs a result, only three LMs were probed on over 20% of the tasks, and one single task was evaluated for more than 20% of the reviewed LMs.\\nNotably, recent large LMs are significantly underrepresented.\\nBenchmark (§ 4) To address this identified deficiency, Holmes offers a structured framework to assess the English linguistic competence of LMs comprehensively.\\nIt features 208 distinct datasets covering morphology, syntax, semantics, reasoning, and discourse phenomena, including previously underrepresented ones like negation or rhetoric in text (Liang et al., 2023).\\nResults and Analysis (§ 5) From assessing 59 LMs (Figure 1), we find that no single one consistently excels the others and that their linguistic competence is more pronounced for morphology and syntax than the other phenomena types.\\nInstead, we find model size, model architecture, and instruction tuning fundamentally affect their linguistic competence.\\nFirst, LMs’ linguistic competence, particularly for morphology and syntax, scales with their model size.\\nThis generalizes previous findings (Tenney et al., 2019b; Zhang et al., 2021) beyond LMs with 350 million parameters.\\nSecond, contrary to prompting evaluations (Lu et al., 2023) and aligned with other work (Waldis et al., 2024a; Gautam et al., 2024), model architecture is critical.\\nThe linguis- tic competence of decoder-only LMs is less pronounced, and even 70 billion does not allow them to encode linguistic phenomena of words with comparable strength to encoder-only LMs of a similar size.\\nThird, while previous studies focused on aligning LMs with human interactions through instruction tuning (Ouyang et al., 2022; Touvron et al., 2023; Zhou et al., 2023), we show for the first time its effect on their linguistic competence.\\nIt improves morphology and syntax but has mixed effects for the other types of phenomena.\\nLastly, we contrast the results of Holmes with OpenLLM (Beeching et al., 2023), an extensive LM benchmark focusing on user-centered applications like mathematical reasoning.\\nWe find that Holmes provides a unique but supplementary perspective, as rankings partly align, especially for reasoning-related phenomena.\\nEfficiency (§ 6) Finally, to mitigate the heavy computational burden of evaluating a new LM on Holmes, we form the streamlined version FlashHolmes by selectively excluding samples not significantly influencing overall rankings (Perlitz et al., 2023).\\nSpecifically, FlashHolmes approximates Holmes rankings with high precision while requiring only ~3% of the computation.\\nWe summarize our contributions as follows:\\n• Benchmark.\\nHolmes comprehensively and thoroughly assesses the linguistic competence of LMs in isolation, providing substantial ground for advancements in NLP.\\n• Empirical insights.\\nExtensive experiments reveal that LMs’ linguistic competence is more pronounced for morphology and syntax, and size, architecture, and instruction tuning are crucial for LM differences.\\n• Ease of use.\\nWe provide tools to interactively explore Holmes results and straightforward code to evaluate upcoming LMs with efficiency in mind (FlashHolmes).',\n",
       "   'title': '1 Introduction',\n",
       "   'chunks': [{'text': 'questions, benchmarks estimate cognitive abilities by providing textual instructions and evaluate LMs’ responses, as done for mathematical reasoning (Cobbe et al., 2021) or factual knowledge (Petroni et al., 2019, 2020).\\nHowever, they conflate latent abilities (like following provided instructions) with those under test, such as understanding specific linguistic phenomena, e.g., syntactic structures (Liang et al., 2023).\\nAs this entanglement makes it infeasible to draw definitive conclusions about distinct abilities (Hu and Levy, 2023), recent studies call to assess the linguistic competence of LMs comprehensively and in isolation (Lu et al., 2023; Mahowald et al., 2024).',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 0,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': 'competence is the unconscious understanding of language, like grasping grammatical rules (Chomsky, 1965).\\nAs language models (LMs) are trained on simple tasks like next word prediction (Brown et al., 2020), one might naturally wonder: What is the linguistic competence of LMs, and how do they differ?\\nTo answer such ∗* Corresponding author andreas.waldis@live.com In this work, we introduce the Holmes (Figure 2).\\nA benchmark to assess the linguistic competence of LMs (Figure 1) regarding numerous linguistic phenomena.\\nTo fully disentangle the understanding of these phenomena and other abilities of LMs, we use classifier-based probing (Tenney et al., 2019a; Hewitt and Manning, 2019; Belinkov, 2022).\\nA method that uses the LMs’ internal representations of text inputs to train linear models',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 0,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': 'Figure 2: Overview of Holmes (left) with the five phenomena types (right) and an example of probing-based evaluations for part-of-speech: encoding the input tokens and predicting the POS tag for cucumber, here NN.',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': '(probes) to predict specific aspects of phenomena, such as words’ part-of-speech (POS).\\nWe then approximate the LMs’ grasp of these phenomena using the probes’ performance, rigorously verified using control tasks (Hewitt and Liang, 2019) and from an information theory perspective (Voita and Titov, 2020).\\nWith this particular and comprehensive scope, we thoroughly address the initially raised questions as follows:',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': 'Meta-Study (§ 3) The review of over 270 probing studies reveals a gap in comprehensively evaluating linguistic competence.\\nDespite covering over 200 probing tasks and 150 LMs, individual studies focus on particular tasks and LMs.\\nAs a result, only three LMs were probed on over 20% of the tasks, and one single task was evaluated for more than 20% of the reviewed LMs.\\nNotably, recent large LMs are significantly underrepresented.',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': 'Benchmark (§ 4) To address this identified deficiency, Holmes offers a structured framework to assess the English linguistic competence of LMs comprehensively.\\nIt features 208 distinct datasets covering morphology, syntax, semantics, reasoning, and discourse phenomena, including previously underrepresented ones like negation or rhetoric in text (Liang et al., 2023).',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': 'Results and Analysis (§ 5) From assessing 59 LMs (Figure 1), we find that no single one consistently excels the others and that their linguistic competence is more pronounced for morphology and syntax than the other phenomena types.\\nInstead, we find model size, model architecture, and instruction tuning fundamentally affect their linguistic competence.',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': 'First, LMs’ linguistic competence, particularly for morphology and syntax, scales with their model size.\\nThis generalizes previous findings (Tenney et al., 2019b; Zhang et al., 2021) beyond LMs with 350 million parameters.\\nSecond, contrary to prompting evaluations (Lu et al., 2023) and aligned with other work (Waldis et al., 2024a; Gautam et al., 2024), model architecture is critical.\\nThe linguis- tic competence of decoder-only LMs is less pronounced, and even 70 billion does not allow them to encode linguistic phenomena of words with comparable strength to encoder-only LMs of a similar size.\\nThird, while previous studies focused on aligning LMs with human interactions through instruction tuning (Ouyang et al., 2022; Touvron et al., 2023; Zhou et al., 2023), we show for the first time its effect on their linguistic competence.\\nIt improves morphology and syntax but has mixed effects for the other types of phenomena.\\nLastly, we contrast the results of Holmes with OpenLLM (Beeching et al., 2023), an extensive LM benchmark focusing on user-centered applications like mathematical reasoning.\\nWe find that Holmes provides a unique but supplementary perspective, as rankings partly align, especially for reasoning-related phenomena.',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': 'Efficiency (§ 6) Finally, to mitigate the heavy computational burden of evaluating a new LM on Holmes, we form the streamlined version FlashHolmes by selectively excluding samples not significantly influencing overall rankings (Perlitz et al., 2023).\\nSpecifically, FlashHolmes approximates Holmes rankings with high precision while requiring only ~3% of the computation.',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': 'We summarize our contributions as follows:',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': '• Benchmark.\\nHolmes comprehensively and thoroughly assesses the linguistic competence of LMs in isolation, providing substantial ground for advancements in NLP.',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': '• Empirical insights.\\nExtensive experiments reveal that LMs’ linguistic competence is more pronounced for morphology and syntax, and size, architecture, and instruction tuning are crucial for LM differences.',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'},\n",
       "    {'text': '• Ease of use.\\nWe provide tools to interactively explore Holmes results and straightforward code to evaluate upcoming LMs with efficiency in mind (FlashHolmes).',\n",
       "     'title': '1 Introduction',\n",
       "     'page': 1,\n",
       "     'source_doc': '1 Introduction'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Preliminaries\\nLanguage Models (LMs) Language Models compute probabilities for word sequences i, enabling tasks such as classifying i, textual comparisons between i and another sequence i′, and text generation based on i. We consider LMs as any model producing representations of input i, regardless of their specific type: sparse like bag-of-words (Harris, 1954); static such as GloVe (Pennington et al., 2014); or contextualized transformer-based LMs (Devlin et al., 2019; Raffel et al., 2020).\\nLinguistic Competence Following Chomsky (1965), linguistic competence is defined as the unconscious knowledge of language, encompassing the understanding of specific linguistic phenomena, including word dependencies and their distinct parts of speech (POS).\\nLinguistic Phenomena We define the linguistic competence of LMs as their ability to understand a diversity of linguistic phenomena.\\nSpecifically, we focus on five phenomena types: morphology, the structure of words; syntax, the structure of sentences; semantics, the meaning of words; reasoning, the use of words in logical deduction and other related phenomena like negation or speculation;\\ndiscourse, the context in text like rhetorical structure.\\nFollowing Mahowald et al.\\n(2024), we categorize these phenomena types into two groups: morphology and syntax are formal phenomena, which include understanding grammatical rules and statistical patterns, while functional ones (semantics, reasoning, and discourse) focus on practical abilities like interpreting text sentiment or detecting the existence of speculation.\\nDatasets We define a dataset as text examples and labels covering a specific aspect of a linguistic phenomenon, like words and their POS tag.\\nTypically, these labels are highly unambiguous to assess the specific aspect under test in isolation.\\nProbes Using probes, we empirically assess the linguistic competence of LMs regarding the featured linguistic phenomena in Holmes.\\nTo this end, we employ probing tasks using the widely recognized classifier-based probing method (Tenney et al., 2019a; Hewitt and Manning, 2019; Belinkov, 2022), or known as diagnostic classifiers (Veldhoen et al., 2016; Giulianelli et al., 2018).\\nRunning such a probing task involves training a probe (linear model) using the specific dataset to test a distinct aspect of a linguistic phenomenon in isolation.\\nTherefore, we feed the text examples, encoded with a given LM, as training inputs.\\nSubsequently, we use the probe’s performance to approximate how an LM understands the specific linguistic phenomenon under test.\\nWith a higher score, we assume the embeddings embody patterns relevant to this phenomenon, which enhances the accuracy (Tenney et al., 2019b).\\n3 Meta-Study this section, we survey 274 studies (§ 3.1), probing LMs’ linguistic competence.\\nWe analyze these studies regarding their evolution, covered probing tasks and LMs (§ 3.2), and identify the apparent need for consolidating existing resources (§ 3.3).\\n3.1 Scope\\nWe analyze 28k papers (P) from 2015 to August 2023 of major NLP conferences (TACL, ACL, AACL, COLING, EACL, EMNLP, NAACL, and corresponding workshops) expanded with selected work from other venues such as ICLR.\\nTo identify relevant work, we follow a semiautomatic approach.\\nFirst, we automatically select papers based on their meta-data and full text.1 We select a total of 493 candidate papers matching at least one of the following three criteria (P ′ = {∀p ∈ P |p ∈ P1 ∪ p ∈\\nP2 ∪\\np\\n∈ P3}):\\nP1: papers contain probing or probe in the title.\\nP2: papers contain probing or probe in the abstract and at least five times in the main content.\\nP3: papers contain probing or probe at least ten times in the main content.\\nWe manually verified these automatically curated candidates (P ′) and found 274 relevant papers (Pr).We selected them as they either evaluate LMs regarding one or more linguistic phenomena as part of the analysis or as a main contribution.\\nThis involves filtering papers using the term probing in other senses, such as probing hash tables (Bogoychev and Lopez, 2016).\\n3.2 Analysis Next, we analyze these 274 relevant studies (Pr).\\ni) Scattered evolution calls for consolidation.\\nFirst, we analyze the evolution of the relevant studies.\\nFigure 3 relates how these studies cite each other (probing citations Cp) compared to other 1We use PyPDF2 v3.0.0, DBLP and semanticscholar API.\\nFigure 3: Citation analysis considering probing citations originating from the set of relevant work and every other citation (general citations).\\nThe color scale indicates the ratio (α) between them.\\ngathered citations (general citations Cg).\\nColorized, we show the ratio α between these two measures α = |Cp|+1 |Cg |+1.\\nFirst, only a fraction of the works gained general attention, as 16 papers exceeded 200 general citations.\\nFurther, probing works cite each other rather sparsely, with an average probing citation ratio of α = 0.1.\\nTherefore, we see other fields are paying little attention to the linguistic competence of LMs.\\nPaired with scattered citation patterns, we identify the need to consolidate existing resources to solidly ground research in this field.\\nii) Probing work prioritizes tasks and analytics over methods.\\nWe categorize the selected work according to their probing focus: methodological, new methods, like control tasks (Hewitt and Liang, 2019) or minimum description length (Voita and Titov, 2020); task-focused assessing specific linguistic phenomena as main contributions, such as discourse relations in text (Koto et al., 2021); and analytical using probing tasks to analyze LMs, such as the impact of pre-training data (Zhang et al., 2021).\\nFigure 4 shows: the majority (51.8%) of studies focus on specific probing tasks like numeric scales (Zhang et al., 2020), or morphosyntactic (Shapiro et al., 2021); 35.7% use probing as a supplementary analytical tool, for example, analyzing the effect of fine-tuning (Mosbach et al., 2020a; Zhu et al., 2022a); 12.5% address methodological problems related to probing (Wu et al., 2020; Immer et al., 2022; Zhu et al., 2022b).\\niii) The dominance of classifier-based probing.\\nNext, we analyze the specific employed probing method: classifier, using linear or shallow models to probe internal representations of LMs, as demonstrated in Tenney et al.\\n(2019a); mask, letting LMs fill gaps to verify linguistic phenomena, as shown in Talmor et al.\\n(2020) or Warstadt et al.\\n(2020); at- tention, which relies on attention patterns, as used in Pandit and Hou (2021) for bridging; and other, methods not belonging to the previous three categories, such as dimension selection (Torroba Hennigen et al., 2020).\\nMost studies utilize the classifierbased probing method (74%), 20% conduct maskbased probing, and only a minority of work (∼ 3%) considers attention patterns or other approaches.\\nFigure 4: Categorization of the selected studies by their focus and their conducted probing method.\\niv) Tasks and LMs are barely broadly evaluated.\\nFinally, we analyze which tasks and LMs the relevant probing studies consider.\\nFor example, Tenney et al.\\n(2019b) considers BERT and probes POS tagging, semantic-role labeling (SRL), and other ones.\\nAggregated over all studies, we found a broad coverage of 289 unique tasks and 161 distinct LMs.\\nBelow, we delve into the details and highlight noteworthy findings.\\nWe analyze how LMs and tasks are considered jointly in Figure 5.\\nDespite the broad coverage, single studies, including fundamental ones, maintain a particular focus and consider only a fraction of LMs and tasks.\\nFor example, while most tasks (72%) were assessed on BERT, RoBERTa’s coverage has already declined to 42%.\\nConversely, part-of-speech tagging (POS), the most probed task, was only evaluated on 23% of the LMs, for example, not covering prominent examples like BART (Lewis et al., 2020).\\nNotably, more recently released larger and powerful LMs, like PYTHIA (Biderman et al., 2023), UL2 (Tay et al., 2023), or LLAMA-2 (Touvron et al., 2023), and instruction-tuned LMs (FLAN-T5 (Chung et al., 2022), LLAMA-2-Chat (Touvron et al., 2023), or TK-Instruct (Wang et al., 2022) are missing almost entirely, with single more recent exceptions (Hu and Levy, 2023; Waldis et al., 2024a).\\nAgain, these insights underscore the need to consolidate existing resources for more dense coverage.\\nThis is further evident when considering Figure 5, where we sort LMs and tasks according to how often they were mentioned in the relevant works.\\nThen, we plot\\nFigure 5: Overview of how many tasks single LMs cover and vice versa - single examples are highlighted.\\nFigure 6: Cumulative coverage of LMs and tasks, considering all relevant studies and their focus.\\ntheir cumulative coverage concerning all mentions.\\nFor example, considering all studies (red line), the top-10 most mentioned LMs account for 80% of all LMs mentions (black dot).\\nIn contrast, the other 151 unique LMs account for only 40%.\\nComparing the paper focus, we see that methodological studies rely only on 24 LMs and 36 tasks.\\nIn contrast, task-focused and analytical work covers a similar number of LMs (91 and 99, respectively).\\nHowever, due to their distinct focus, task-focused studies cover significantly more tasks (202) than analytical ones (115).\\n3.3 Summary\\nThis meta-study emphasizes the need to consolidate existing resources for a comprehensive assessment of the linguistic competence of LMs — a manifold but rather blind spot in evaluation research.\\nApart from more thorough evaluations, such a stimulus can significantly boost future research, as happened in computer vision with ImageNet (Deng et al., 2009) or in NLP with GLUE and SuperGLUE (Wang et al., 2019a,b).\\n4 Holmes Benchmark\\nWith Holmes, we provide an extensive ground to tackle these identified deficiencies in the existing literature and comprehensively investigate the English linguistic competence of LMs.\\nSpecifically, Holmes features 208 datasets addressing distinct aspects of 66 phenomena covering morphology, syntax, semantic, reasoning, and discourse.\\n4.1 Datasets\\nTo feature a total of 208 unique datasets, we leverage existing and established resources like OntoNotes (Weischedel et al., 2013), English Web Treebank (Silveira et al., 2014), or BLIMP (Warstadt et al., 2020) and create datasets addressing phenomena like the POS of words, their dependencies or determine the linguistic acceptability of sentences.\\nFurther, we include a range of less employed data, addressing contextualization of words (Klafka and Ettinger, 2020), reasoning (Talmor et al., 2020), semantic decomposition (White et al., 2016; Rudinger et al., 2018a,b; Govindarajan et al., 2019; Vashishtha et al., 2019), grammatical knowledge (Huebner et al., 2021), bridging (Pandit and Hou, 2021), and rhetorical (Carlson et al., 2001) and discourse (Webber et al., 2019) structure in text.\\nFinally, we cover rarely probed phenomena like negation (Szarvas et al., 2008; Konstantinova et al., 2012; Vahtola et al., 2022), or word complexity (Paetzold and Specia, 2016).\\n4.2 Structure\\nApart from the comprehensive scope, Holmes provides a clear structure for specific evaluations on different levels of aggregation.\\nWe first group the datasets according to the linguistic phenomena addressed.\\nThen, we categorize these phenomena into their previously introduced type (see § 2) - morphology, syntax, semantics, reasoning and discourse.\\nWe rely on the categorization provided by the specific studies whenever given.\\nThe detailed categorization is given in § A.3.\\n4.3 Experimental Setup\\nHolmes evaluation follows the primarily used classifier-based probing paradigm, as described in § 2.\\nConsidering the internal representations allows us to maximally disentangle the understanding of distinct linguistic phenomena from each other and from other cognitive abilities (like following textual instructions).\\nFurther, this method allows us to assess any type of LMs, including sparse, static, or contextualized ones.\\nBased on the specific dataset, we either select the embeddings of the specific input tokens (like single words for POS tagging) or average embeddings across a span or the whole sentence.\\nWe define a probing task as training a probe fp (linear model without intermediate layers) using these embeddings as inputs and the dataset labels as training signals.\\nIf not defined in the original data, we divide the dataset samples into train/dev/test split following a ratio of 70/10/20.\\nWe repeat this procedure five times using different random seeds and aggregate the results afterward.\\n4.4 Evaluations\\nWe approximate how well an LM encodes specific linguistic phenomena using the absolute prediction performance of the probes.\\nIn addition, we rigorously evaluate the reliability of probing results using control tasks and from an information theory perspective (Voita and Titov, 2020; Hewitt and Liang, 2019).\\nDifferent from commonly used prompting assessments, this particular evaluation protocol refrains from known fallacies in which the results and conclusions are sensible with specific instructions (Mizrahi et al., 2024; Min et al., 2022) or few-shot examples (Lu et al., 2023).\\nTask Score Metric Based on a dataset’s specific task type, we use a corresponding performance measure, macro F1 for classification or Pearson correlation for regression.\\nIn addition, we calculate the standard deviation σ of the probe across multiple seeds.\\nA lower σ indicates a better encoding of a given linguistic phenomenon since the mea- surement is robust to noise.\\nFurther, we use the task score for ranking-based evaluation of all eval- uated LMs L = {l1,, lm} within Holmes.\\nWe calculate the mean winning rate mwr (in percentage), telling us how many times one LM l1 wins against others (Liang et al., 2023).\\nWith a higher mwr, we assume an LM encodes tested linguistic phenomena better than others.\\nCompression Next, we evaluate the probes’ reliability from an information-theoretic perspective.\\nFollowing Voita and Titov (2020), we use the compression I to measure how well a probe compresses input data.\\nA higher I means fewer bits are needed, indicating that the given linguistic phenomenon is more clearly encoded in the embeddings.\\nSelectivity A reliable probe should grasp patterns relevant to the tested phenomena in the internal representations of LMs but should not be able to learn anything else.\\nTherefore, we expect high performance when evaluating the specific dataset but low performance when we randomize training signals.\\nWe check this using control tasks introduced in Hewitt and Liang (2019).\\nSpecifically, we calculate the selectivity S as the difference between the probe trained with the original labels y and the control task where we train the probe with randomly assigned labels y′.\\nWith a higher S, we assume the detected patterns are relevant for the specific phenomena under test, as random patterns do not lead to similar performance.\\n5 Holmes Results\\nUsing Holmes, we evaluate a diverse collection of 59 LMs.2 Using the results of these extensive experiments, we first answer the research question:\\nwhat is the linguistic competence of LMs?\\nIn doing so, we discuss the reliability of results (i), the linguistic competence of LMs concerning the unique structure of Holmes (ii), and how these results relate to other downstream abilities (iii).\\nSubsequently, we examine how linguistic competence varies among LMs, as we find LMs prevailing for different types of linguistic phenomena (Figure 1) and delve into the effects of model architecture (iv), size (v), and instruction tuning (vi).\\ni) The reliability of Holmes\\nFirst, we show the reliability of probing-based evaluation, using deviation σ, compression I, and selectivity S results in Figure 7.\\nSingle outliers are datasets that are too hard for all LMs, as the sample size is too small, or the linguistic phenomena under test are too complex, as the ability to detect spans causes speculations in a text.\\nWe average these metrics for every dataset across all LMs.\\nNote, for selectivity, we consider only base-sized model (10m-200m parameters) for computational efficiency.\\nFirst, we found a low average deviation (σ = 0.02), indicating the high reliability of probes across random seeds.\\nThese results also highlight the stability of probing results, compared to prompting-based ones where results across many paraphrased prompts lead to a deviation of σ = 0.07 reported in Mizrahi et al.\\n(2024).\\nNext, substantial compression (average I = 1.9) and selectivity (average S = 0.31) further confirm the probes’ reliability.\\nInterestingly, one identifies two parallel trends for selectivity.\\nHarder datasets with many labels, like POS tagging, are arranged around a selectivity of 0.1 to 0.4 and a task metric of 0.3.\\nIn contrast, for easier binary classification tasks (such as linguistic applicability), we observe selectivity around 0.2 to 0.5 and a task metric of 0.6 to 0.9.\\n2Find a complete list in Appendix § A.2.\\nFigure 7: Reliability evaluation using deviation, compression (log), and selectivity on the y-axis for all 208 probing datasets.\\nThe x-axis represents the task metrics (either person correlation or macro F1).\\nFurther, we measure a significant (p < 0.05) positive correlation between the task metrics and the compression (τ = 0.64) and selectivity (τ = 0.65).\\nThis further confirms our reliability assumption and allows us to trust the task metric as the primary evaluation measure.\\nii) The story of Holmes\\nWe focus on what Holmes tells us in general and regarding formal and functional phenomena, as defined in § 2. We report in Figure 8 the task metric, discriminability, and selectivity, averaged for every phenomena type.\\nNote, discriminability (Rodriguez et al., 2021) quantifies the alignment of LMs ranking of one specific dataset compared to the overall rankings using the Kendall Tau correlation.\\nConsidering these three metrics, all tested LMs strongly encode formal phenomena (morphology and syntax), which often depend on the local neighborhood of words.\\nTherefore, we assume that LMs approximate these co-occurrences during pre-training with high precision.\\nFor example, the specific POS tag of a word, like man (noun), primarily depends on its surroundings, such as the frequent predecessor the.\\nIn contrast, LMs encode less information about functional phenomena (semantics, reasoning, and discourse) since they show a relatively low performance regarding the task metric.\\nFor these functional phenomena, we assume more complex co-occurrences are required to capture the broad context in language, such as the rhetorical relation of two distant text spans.\\nDespite these differences between formal and functional phenomena types, they contribute to the benchmark in a balanced way.\\nA low to medium discriminability indicates that none of these types of linguistic phenomena dominates the overall LM rankings.\\nThis balanced influence of the five phenomena types is further visible when considering their ranking correlations (Figure 9, left).\\nA high average correlation of 67.8 ± 6.6 with the overall results\\nFigure 8: Average task metric, difficulty, and discriminability for each phenomena type.\\nThe dashed lines show the average measure over all datasets.\\n(last column/row) hints that they are facets of a broader occurrence but share common characteristics.\\nStill, breaking into categories is meaningful, as the phenomena types (first five columns/rows) are medium correlated (average of 53.9 ± 14.5).\\nAnalyzing the results of phenomena types further highlights the value of this distinction.\\nWhile results of morphology and syntax are similarly correlated with the overall results (68.2 and 70.2), their direct correlation (69.1) indicates their supplementary nature.\\nFurther, discourse results show the lowest correlation with others (44.8± 16.1), indicating the particular scope.\\niii) The companions of Holmes\\nWe analyze how the results of Holmes and those from other evaluations focusing on downstream applications align (Figure 9, right).\\nWe select the OpenLLM benchmark (Beeching et al., 2023), as it covers a wide range of open LMs, in contrast to others like HELM (Liang et al., 2023).\\nFirst, Holmes and OpenLLM results of jointly evaluated LMs are medium correlated, hinting that the linguistic competence of LMs is partly aligned with their downstream abilities.\\nThe nature of this alignment is further evident when focusing on morphology, reasoning, and discourse.\\nInterestingly, and in contrast to syntax and semantics, their correlation to the OpenLLM and Holmes overall results is similar.\\nTherefore, these three phenomena presumably represent skills that are more tested in the general benchmarks.\\nThese correlation patterns are consistent across the three most meaningful OpenLLM datasets (MMLU, TruthfulQA, and GSM8K).\\nAs TruthfulQA shows lower correlations with the linguistic phenomena and other datasets within OpenLLM, we presume this dataset captures distinctly different skills (possibly knowledge).3 These insights show how different benchmarks provide a different scope and supplement themselves simulta- 3Further, it’s also known that we need to expect this dataset to be fully leaked (Balloccu et al., 2024).\\nFigure 9: Kendall-tau correlation within Holmes (left) and compared to the OpenLLM benchmark (right).\\nGreen stars indicate significant correlations (p < 0.05).\\nneously.\\nFurther, the above analysis shows, again, the value of assessing the linguistic competence of LMs across different phenomena types, for finegrained analyses.\\niv) The effect of language model architecture.\\nNext, we discuss the impact of model architecture on the linguistic competence of LMs.\\nIn Figure 11 (left), we compare encoder and decoder LMs.\\nDue to the absence of big encoder LMs, we consider five encoder and six decoder LMs with up to 220m parameters.\\nEncoder LMs show a higher mwr of 52% than decoder LMs (21%).\\nThis observation is the most saturated for morphology or syntax, encompassing a variety of token-level phenomena, like part-of-speech.\\nWe assume that the missing bi-directional encoding of decoder LMs causes this lower performance because the available context of one token heavily depends on its position.\\nThus, even common tokens, like the, have different potential representations - at the beginning or in the middle of a sentence.\\nThese instabilities are further evident when considering Figure 11 (right) which reports the accuracy for the top-20 most common POS tokens (such as the) based on the pos, xpos, upos dataset.\\nGiven their high frequency, one expects stable prediction performance.\\nSurprisingly, encoder LMs (BERT and RoBERTa) show higher median accuracy and clearly lower deviations compared to the same-size decoder counterpart (GPT2).\\nWhile scaling model size to 12B (Pythia) and 70B (Llama-2) allows for improved accuracy and lower deviations, decoder LMs do not match the encoder performance, even up to 700 times bigger.\\nv) The effect of scaling parameters.\\nWe discuss how the number of parameters influences the linguistic competence of LMs.\\nGiven the variety of LMs of different sizes, we focus on the Pythia (decoder-only) and T5 (encoder-decoder) families.\\nFrom Figure 10, we observe for both Pythia and T5 that the linguistic competence scales with model size, and it is particularly pronounced after exceeding 0.5B (Pythia) and 1.0B (T5) parameters.\\nAgain, model architecture is crucial, as T5 LMs (encoderdecoder) exhibit a clearly higher mean winning rate of 40− 70% than Pythia (decoder-only) ones with mwr of 20− 60%.\\nFurther, we found formal phenomena evolving differently with increased model size than functional ones.\\nSpecifically, morphology and syntax start at a lower level, with an apparent performance jump after 0.5B (Pythia) and 1.0B (T5) parameters, followed by slow but steady growth.\\nDifferently, semantics, reasoning, and discourse start at a higher mwr, followed by a continuous improvement as the model size grows.\\nFrom these results, we assume more parameters allow LMs to better approximate simpler co-occurrences in the near neighborhood of words to understand formal phenomena like word dependencies.\\nIn contrast, more parameters do not have the same pronounced effect on functional phenomena, like rhetorical relations, which require an LM to acquire more distant and complex word co-occurrences.\\n | Model | Morphology | Syntax | Semantics | Reasoning | Discourse | Overall\\n | --- | --- | --- | --- | --- | --- | ---\\n | Llama-2-Chat | Comparison against Llama-2 -8% | +3% | with 7 billion parameters -5% | -9% | -3% | -2%\\n | FLAN-T5 | Comparison against T5 +10% | +2% | with 11 billion parameters -2% | +6% | -2% | +1%\\n | Dolly-v2 | Comparison against Pythia +4% | -3% | with 12 billion parameters -9% | -3% | +4% | -4%\\n | Tülu-2 | Comparison against Llama-2 +5% | +2% | with 13 billion parameters -15% | 0% | -30% | -8%\\n | Orca-2 | -1% | -3% | -4% | +4% | -5% | -2%\\n | Llama-2-chat | +3% | +1% | -6% | +3% | -1% | -1%\\n | Vicuna-v1.5 | +23% | +7% | -3% | +6% | -6% | +4%\\n | FLAN-UL2 | Comparison +40% | against UL2 +16% | with 20 billion +7% | parameters +13% | +1% | +13%\\n | Mixtral-Instruct | Comparison against Mixtral +4% | +3% | with ~47 billion parameters 0% | +6% | -2% | +2%\\n | Tülu-2 | Comparison against Llama-2 +15% | 0% | with 70 billion parameters -11% | -3% | 0% | -2%\\n | Llama-2-Chat +23% +14% +2% +4% +17% +10%\\n | Average | +10% | +4% | -3% | +4% | -2% | +1%\\n\\nTable 1: Effect of instruction tuning on the mean winning rate compared to the pre-trained LMs.\\nvi) The effect of instruction tuning.\\nFinally, we focus on how instruction tuning affects LMs’ linguistic competence and compare the tuned LMs with their base models—for example, FLAN-UL2 vs. UL2.\\nFrom results in Table 1, we note less saturated effects for the overall scope while being more pronounced for the five phenomenon types again emphasizing the structured and comprehensive evaluation of linguistic competence.\\nOn average, we found instruction tuning has the highest\\nFigure 10: Effect of scaling LM parameters considering the T5 and Pythia model families providing eight and five different sizes.\\nWe address the overall scope (left) and the different types of linguistic phenomena (right).\\nFigure 11: Comparison of the phenomenon types for encoder and decoder LMs (left) and on the right, the accuracy of the top-20 most common tokens of the three part-of-speech probing datasets for BERT, RoBERTa, GPT2, Pythia, and Llama-2.\\neffect on morphology (+10%) followed by syntax (+4%), reasoning (+4%), and a negative effect for semantics −3% and discourse −2%.\\nThese results confirm previous assumptions that instruction tuning updates are often superficial (Yadav et al., 2023; Hershcovitch et al., 2024; Sharma et al., 2023) and that LMs are better at mimicking language (formal phenomena) than understanding it, measured with functional phenomena (Mahowald et al., 2024).\\nFurther, larger models benefit more from instruction tuning.\\nLlama-2-70b-Chat and FLANUL2 gain up to +23% and +40% for morphology and +10% and +13% on average.\\nIn addition, decoder-only LMs (Llama-2 and Pythia) tend to show less pronounced positive effects than encoderdecoder LMs (FLAN-T5-XXL and FLAN-UL2).\\nHowever, they better understand reasoning phenomena.\\nWhen comparing LMs based on Llama-213b, we see that specific fine-tuning methods shape the LMs differently.\\nThe top-ranked 13b LM for Holmes and OpenLLM, Vicuna, was trained on 125k instructions, less than other models.\\nThus, high quality is more important than the number of instructions for LMs’ linguistic competence.\\nTülu loses performance while being trained on a large mixture of data (approx. 330k instructions), the same for its 70b version.\\nFinally, the focus of Orca2 on reasoning is also reflected in its embedding space.\\nThese insights show again that while providing a particular perspective, Holmes shows clear differences between LMs and allows us to map them to methodological decisions.\\n6 Efficiency\\neasy, cost-effective integration of new LMs is crucial for widely adopting a benchmark.\\nAs Holmes covers many datasets and examples, it is computationally heavy in encoding text and training the probes.\\nIt takes approx.\\n6 GPU days to encode the 70 million tokens (∼230k pages of text) and 2 days to run the 208 probes for a 70b model.\\nTo account for this issue, we introduce FlashHolmes, a streamlined version of Holmes.\\nIt allows the evaluation of new LMs with a fraction of the compute while maintaining evaluation integrity.\\nBesides excluding licensed data (18 probing datasets), we analyze the effect of discarding training instances.\\nAs a result, we reduce the computation for encoding and the actual probing simultaneously.\\nWe follow Perlitz et al.\\n(2023) and calculate the rank resolution, 95% CI of model rank difference.\\nThis measure indicates the maximum expected rank deviation from evaluating an LM on FlashHolmes compared to Holmes.\\nFor example, a rank resolution of one means that an LM evaluated on FlashHolmes and Holmes has the same rank or switch place with its neighbors with a probability of 95%.\\nFigure 12 shows the resulting rank resolution when training only on a fraction of the instances, from 1/2 to 1/512.\\nSolely focusing on efficiency (1/512) still provides a decent rank resolution of ~2.7.\\nIn contrast, considering 1/2 of the training data results in the best reliability of ~1.0.\\nTo balance benchmark reliability and efficiency, we compose FlashHolmes using 1/32 of the training instances.\\nPrecisely, it reduces the computation expenses of evaluating LMs to ~3% of what Holmes would have required while pre- serving a high rank-correlation of ~1.3.\\nFigure 12: Analysis of the reliability vs. efficiency trade-off when reducing the number of training data.\\n7 Related Work\\nBenchmarking LMs Benchmarks approximate LMs abilities like general language understanding (Wang et al., 2019b,a), out-of-distribution generalization (Yang et al., 2023; Waldis et al., 2024b), adversarial scenarios (Nie et al., 2020; Wang et al., 2021), or retrieval like BEIR (Thakur et al., 2021) or MTEB (Muennighoff et al., 2023).\\nWith the advent of larger LMs, the methodological focus shifted to prompting-based evaluations which evaluate the LMs’ response to provided instructions (Brown et al., 2020; Hendrycks et al., 2021; Srivastava et al., 2022) covering application-oriented tasks (Liang et al., 2023), or mathematical reasoning (e.g., GSM8K (Cobbe et al., 2021)).\\nAssessing the Linguistic Competence of LMs The analysis of LMs’ linguistic competence ranges from analyzing static word vectors (Köhn, 2015), sentence embeddings (Conneau et al., 2018; Adi et al., 2017), the internals of translation models (Shi et al., 2016; Bau et al., 2019), or contextualized LMs (Tenney et al., 2019b,a; Hewitt and Manning, 2019).\\nOther work addressed methodological aspects, such as using control tasks (Hewitt and Liang, 2019), assessing LMs from an information theory perspective (Voita and Titov, 2020;\\nPimentel et al., 2020), or evaluating causal effects in LMs (Elazar et al., 2021).\\nFinally, another line of work focuses on whether LMs follow human understanding of linguistic competence when solving downstream tasks (Belinkov, 2022; Aw et al., 2023; Mahowald et al., 2024).\\nHowever, Mosbach et al.\\n(2020b) and Waldis et al.\\n(2024a) found finetuning for downstream tasks actually hurting the understanding of linguistic phenomena.\\nWhile prior studies assessing the linguistic competence of LMs tend to focus on a limited set of linguistic phenomena or models, Holmes provides extensive coverage of both phenomena and eval- uated LMs.\\nUnlike recent evaluations based on prompting methods (Blevins et al., 2023; Liang et al., 2023; Amouyal et al., 2024), Holmes assesses the internal representations of LMs directly.\\nThis approach allows for detailed analysis of specific model characteristics, such as architecture, and helps separate the linguistic competence from other cognitive abilities.\\nThereby, we respond to recent calls for a thorough and explicit evaluation of linguistic phenomena (Hu and Levy, 2023; Lu et al., 2023; Mahowald et al., 2024).\\n8 Conclusion\\nmarks the most up-to-date and extensive consolidation of existing resources addressing the need to assess the linguistic competence of LMs in isolation.\\nOur experiments demonstrate that LMs’ linguistic competence is pronounced regarding formal phenomena but lacks functional ones when information about broader textual contexts, such as rhetorical structure, is required.\\nFurther, size, architecture, and instruction tuning crucially account for differences among LMs.\\nAs LM and resources in the landscape of linguistics continue to grow, we will actively extend Holmes with further probing datasets, evaluate upcoming LMs, and plan to incorporate multilingualism.',\n",
       "   'title': 'Preliminaries',\n",
       "   'chunks': [{'text': 'Language Models (LMs) Language Models compute probabilities for word sequences i, enabling tasks such as classifying i, textual comparisons between i and another sequence i′, and text generation based on i. We consider LMs as any model producing representations of input i, regardless of their specific type: sparse like bag-of-words (Harris, 1954); static such as GloVe (Pennington et al., 2014); or contextualized transformer-based LMs (Devlin et al., 2019; Raffel et al., 2020).',\n",
       "     'title': 'Preliminaries',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Linguistic Competence Following Chomsky (1965), linguistic competence is defined as the unconscious knowledge of language, encompassing the understanding of specific linguistic phenomena, including word dependencies and their distinct parts of speech (POS).',\n",
       "     'title': 'Preliminaries',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Linguistic Phenomena We define the linguistic competence of LMs as their ability to understand a diversity of linguistic phenomena.\\nSpecifically, we focus on five phenomena types: morphology, the structure of words; syntax, the structure of sentences; semantics, the meaning of words; reasoning, the use of words in logical deduction and other related phenomena like negation or speculation;',\n",
       "     'title': 'Preliminaries',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'discourse, the context in text like rhetorical structure.\\nFollowing Mahowald et al.\\n(2024), we categorize these phenomena types into two groups: morphology and syntax are formal phenomena, which include understanding grammatical rules and statistical patterns, while functional ones (semantics, reasoning, and discourse) focus on practical abilities like interpreting text sentiment or detecting the existence of speculation.',\n",
       "     'title': 'Preliminaries',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Datasets We define a dataset as text examples and labels covering a specific aspect of a linguistic phenomenon, like words and their POS tag.\\nTypically, these labels are highly unambiguous to assess the specific aspect under test in isolation.',\n",
       "     'title': 'Preliminaries',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Probes Using probes, we empirically assess the linguistic competence of LMs regarding the featured linguistic phenomena in Holmes.\\nTo this end, we employ probing tasks using the widely recognized classifier-based probing method (Tenney et al., 2019a; Hewitt and Manning, 2019; Belinkov, 2022), or known as diagnostic classifiers (Veldhoen et al., 2016; Giulianelli et al., 2018).\\nRunning such a probing task involves training a probe (linear model) using the specific dataset to test a distinct aspect of a linguistic phenomenon in isolation.\\nTherefore, we feed the text examples, encoded with a given LM, as training inputs.\\nSubsequently, we use the probe’s performance to approximate how an LM understands the specific linguistic phenomenon under test.\\nWith a higher score, we assume the embeddings embody patterns relevant to this phenomenon, which enhances the accuracy (Tenney et al., 2019b).',\n",
       "     'title': 'Preliminaries',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '3 Meta-Study this section, we survey 274 studies (§ 3.1), probing LMs’ linguistic competence.\\nWe analyze these studies regarding their evolution, covered probing tasks and LMs (§ 3.2), and identify the apparent need for consolidating existing resources (§ 3.3).',\n",
       "     'title': 'Preliminaries',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'We analyze 28k papers (P) from 2015 to August 2023 of major NLP conferences (TACL, ACL, AACL, COLING, EACL, EMNLP, NAACL, and corresponding workshops) expanded with selected work from other venues such as ICLR.\\nTo identify relevant work, we follow a semiautomatic approach.\\nFirst, we automatically select papers based on their meta-data and full text.1 We select a total of 493 candidate papers matching at least one of the following three criteria (P ′ = {∀p ∈ P |p ∈ P1 ∪ p ∈',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'P2 ∪\\np\\n∈ P3}):',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '3.2 Analysis Next, we analyze these 274 relevant studies (Pr).',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'First, we analyze the evolution of the relevant studies.\\nFigure 3 relates how these studies cite each other (probing citations Cp) compared to other 1We use PyPDF2 v3.0.0, DBLP and semanticscholar API.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 2,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Figure 3: Citation analysis considering probing citations originating from the set of relevant work and every other citation (general citations).\\nThe color scale indicates the ratio (α) between them.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'gathered citations (general citations Cg).\\nColorized, we show the ratio α between these two measures α = |Cp|+1 |Cg |+1.\\nFirst, only a fraction of the works gained general attention, as 16 papers exceeded 200 general citations.\\nFurther, probing works cite each other rather sparsely, with an average probing citation ratio of α = 0.1.\\nTherefore, we see other fields are paying little attention to the linguistic competence of LMs.\\nPaired with scattered citation patterns, we identify the need to consolidate existing resources to solidly ground research in this field.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'ii) Probing work prioritizes tasks and analytics over methods.\\nWe categorize the selected work according to their probing focus: methodological, new methods, like control tasks (Hewitt and Liang, 2019) or minimum description length (Voita and Titov, 2020); task-focused assessing specific linguistic phenomena as main contributions, such as discourse relations in text (Koto et al., 2021); and analytical using probing tasks to analyze LMs, such as the impact of pre-training data (Zhang et al., 2021).\\nFigure 4 shows: the majority (51.8%) of studies focus on specific probing tasks like numeric scales (Zhang et al., 2020), or morphosyntactic (Shapiro et al., 2021); 35.7% use probing as a supplementary analytical tool, for example, analyzing the effect of fine-tuning (Mosbach et al., 2020a; Zhu et al., 2022a); 12.5% address methodological problems related to probing (Wu et al., 2020; Immer et al., 2022; Zhu et al., 2022b).',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'iii) The dominance of classifier-based probing.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Next, we analyze the specific employed probing method: classifier, using linear or shallow models to probe internal representations of LMs, as demonstrated in Tenney et al.\\n(2019a); mask, letting LMs fill gaps to verify linguistic phenomena, as shown in Talmor et al.\\n(2020) or Warstadt et al.\\n(2020); at- tention, which relies on attention patterns, as used in Pandit and Hou (2021) for bridging; and other, methods not belonging to the previous three categories, such as dimension selection (Torroba Hennigen et al., 2020).\\nMost studies utilize the classifierbased probing method (74%), 20% conduct maskbased probing, and only a minority of work (∼ 3%) considers attention patterns or other approaches.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Figure 4: Categorization of the selected studies by their focus and their conducted probing method.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Figure 5: Overview of how many tasks single LMs cover and vice versa - single examples are highlighted.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Figure 6: Cumulative coverage of LMs and tasks, considering all relevant studies and their focus.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'their cumulative coverage concerning all mentions.\\nFor example, considering all studies (red line), the top-10 most mentioned LMs account for 80% of all LMs mentions (black dot).\\nIn contrast, the other 151 unique LMs account for only 40%.\\nComparing the paper focus, we see that methodological studies rely only on 24 LMs and 36 tasks.\\nIn contrast, task-focused and analytical work covers a similar number of LMs (91 and 99, respectively).\\nHowever, due to their distinct focus, task-focused studies cover significantly more tasks (202) than analytical ones (115).',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'This meta-study emphasizes the need to consolidate existing resources for a comprehensive assessment of the linguistic competence of LMs — a manifold but rather blind spot in evaluation research.\\nApart from more thorough evaluations, such a stimulus can significantly boost future research, as happened in computer vision with ImageNet (Deng et al., 2009) or in NLP with GLUE and SuperGLUE (Wang et al., 2019a,b).',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '4 Holmes Benchmark',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'With Holmes, we provide an extensive ground to tackle these identified deficiencies in the existing literature and comprehensively investigate the English linguistic competence of LMs.\\nSpecifically, Holmes features 208 datasets addressing distinct aspects of 66 phenomena covering morphology, syntax, semantic, reasoning, and discourse.',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '4.1 Datasets',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'To feature a total of 208 unique datasets, we leverage existing and established resources like OntoNotes (Weischedel et al., 2013), English Web Treebank (Silveira et al., 2014), or BLIMP (Warstadt et al., 2020) and create datasets addressing phenomena like the POS of words, their dependencies or determine the linguistic acceptability of sentences.\\nFurther, we include a range of less employed data, addressing contextualization of words (Klafka and Ettinger, 2020), reasoning (Talmor et al., 2020), semantic decomposition (White et al., 2016; Rudinger et al., 2018a,b; Govindarajan et al., 2019; Vashishtha et al., 2019), grammatical knowledge (Huebner et al., 2021), bridging (Pandit and Hou, 2021), and rhetorical (Carlson et al., 2001) and discourse (Webber et al., 2019) structure in text.\\nFinally, we cover rarely probed phenomena like negation (Szarvas et al., 2008; Konstantinova et al., 2012; Vahtola et al., 2022), or word complexity (Paetzold and Specia, 2016).',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '4.2 Structure',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Apart from the comprehensive scope, Holmes provides a clear structure for specific evaluations on different levels of aggregation.\\nWe first group the datasets according to the linguistic phenomena addressed.\\nThen, we categorize these phenomena into their previously introduced type (see § 2) - morphology, syntax, semantics, reasoning and discourse.\\nWe rely on the categorization provided by the specific studies whenever given.\\nThe detailed categorization is given in § A.3.',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Holmes evaluation follows the primarily used classifier-based probing paradigm, as described in § 2.\\nConsidering the internal representations allows us to maximally disentangle the understanding of distinct linguistic phenomena from each other and from other cognitive abilities (like following textual instructions).\\nFurther, this method allows us to assess any type of LMs, including sparse, static, or contextualized ones.\\nBased on the specific dataset, we either select the embeddings of the specific input tokens (like single words for POS tagging) or average embeddings across a span or the whole sentence.\\nWe define a probing task as training a probe fp (linear model without intermediate layers) using these embeddings as inputs and the dataset labels as training signals.\\nIf not defined in the original data, we divide the dataset samples into train/dev/test split following a ratio of 70/10/20.\\nWe repeat this procedure five times using different random seeds and aggregate the results afterward.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.3 Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'We approximate how well an LM encodes specific linguistic phenomena using the absolute prediction performance of the probes.\\nIn addition, we rigorously evaluate the reliability of probing results using control tasks and from an information theory perspective (Voita and Titov, 2020; Hewitt and Liang, 2019).\\nDifferent from commonly used prompting assessments, this particular evaluation protocol refrains from known fallacies in which the results and conclusions are sensible with specific instructions (Mizrahi et al., 2024; Min et al., 2022) or few-shot examples (Lu et al., 2023).',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Task Score Metric Based on a dataset’s specific task type, we use a corresponding performance measure, macro F1 for classification or Pearson correlation for regression.\\nIn addition, we calculate the standard deviation σ of the probe across multiple seeds.\\nA lower σ indicates a better encoding of a given linguistic phenomenon since the mea- surement is robust to noise.\\nFurther, we use the task score for ranking-based evaluation of all eval- uated LMs L = {l1,, lm} within Holmes.\\nWe calculate the mean winning rate mwr (in percentage), telling us how many times one LM l1 wins against others (Liang et al., 2023).\\nWith a higher mwr, we assume an LM encodes tested linguistic phenomena better than others.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Compression Next, we evaluate the probes’ reliability from an information-theoretic perspective.\\nFollowing Voita and Titov (2020), we use the compression I to measure how well a probe compresses input data.\\nA higher I means fewer bits are needed, indicating that the given linguistic phenomenon is more clearly encoded in the embeddings.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Selectivity A reliable probe should grasp patterns relevant to the tested phenomena in the internal representations of LMs but should not be able to learn anything else.\\nTherefore, we expect high performance when evaluating the specific dataset but low performance when we randomize training signals.\\nWe check this using control tasks introduced in Hewitt and Liang (2019).\\nSpecifically, we calculate the selectivity S as the difference between the probe trained with the original labels y and the control task where we train the probe with randomly assigned labels y′.\\nWith a higher S, we assume the detected patterns are relevant for the specific phenomena under test, as random patterns do not lead to similar performance.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '5 Holmes Results',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Using Holmes, we evaluate a diverse collection of 59 LMs.2 Using the results of these extensive experiments, we first answer the research question:',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'what is the linguistic competence of LMs?\\nIn doing so, we discuss the reliability of results (i), the linguistic competence of LMs concerning the unique structure of Holmes (ii), and how these results relate to other downstream abilities (iii).\\nSubsequently, we examine how linguistic competence varies among LMs, as we find LMs prevailing for different types of linguistic phenomena (Figure 1) and delve into the effects of model architecture (iv), size (v), and instruction tuning (vi).',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'First, we show the reliability of probing-based evaluation, using deviation σ, compression I, and selectivity S results in Figure 7.\\nSingle outliers are datasets that are too hard for all LMs, as the sample size is too small, or the linguistic phenomena under test are too complex, as the ability to detect spans causes speculations in a text.\\nWe average these metrics for every dataset across all LMs.\\nNote, for selectivity, we consider only base-sized model (10m-200m parameters) for computational efficiency.',\n",
       "     'title': 'Preliminaries > i) The reliability of Holmes',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'First, we found a low average deviation (σ = 0.02), indicating the high reliability of probes across random seeds.\\nThese results also highlight the stability of probing results, compared to prompting-based ones where results across many paraphrased prompts lead to a deviation of σ = 0.07 reported in Mizrahi et al.\\n(2024).\\nNext, substantial compression (average I = 1.9) and selectivity (average S = 0.31) further confirm the probes’ reliability.\\nInterestingly, one identifies two parallel trends for selectivity.\\nHarder datasets with many labels, like POS tagging, are arranged around a selectivity of 0.1 to 0.4 and a task metric of 0.3.\\nIn contrast, for easier binary classification tasks (such as linguistic applicability), we observe selectivity around 0.2 to 0.5 and a task metric of 0.6 to 0.9.',\n",
       "     'title': 'Preliminaries > i) The reliability of Holmes',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '2Find a complete list in Appendix § A.2.',\n",
       "     'title': 'Preliminaries > i) The reliability of Holmes',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Figure 7: Reliability evaluation using deviation, compression (log), and selectivity on the y-axis for all 208 probing datasets.\\nThe x-axis represents the task metrics (either person correlation or macro F1).',\n",
       "     'title': 'Preliminaries > i) The reliability of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Further, we measure a significant (p < 0.05) positive correlation between the task metrics and the compression (τ = 0.64) and selectivity (τ = 0.65).\\nThis further confirms our reliability assumption and allows us to trust the task metric as the primary evaluation measure.',\n",
       "     'title': 'Preliminaries > i) The reliability of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'We focus on what Holmes tells us in general and regarding formal and functional phenomena, as defined in § 2. We report in Figure 8 the task metric, discriminability, and selectivity, averaged for every phenomena type.\\nNote, discriminability (Rodriguez et al., 2021) quantifies the alignment of LMs ranking of one specific dataset compared to the overall rankings using the Kendall Tau correlation.\\nConsidering these three metrics, all tested LMs strongly encode formal phenomena (morphology and syntax), which often depend on the local neighborhood of words.\\nTherefore, we assume that LMs approximate these co-occurrences during pre-training with high precision.\\nFor example, the specific POS tag of a word, like man (noun), primarily depends on its surroundings, such as the frequent predecessor the.\\nIn contrast, LMs encode less information about functional phenomena (semantics, reasoning, and discourse) since they show a relatively low performance regarding the task metric.\\nFor these functional phenomena, we assume more complex co-occurrences are required to capture the broad context in language, such as the rhetorical relation of two distant text spans.\\nDespite these differences between formal and functional phenomena types, they contribute to the benchmark in a balanced way.\\nA low to medium discriminability indicates that none of these types of linguistic phenomena dominates the overall LM rankings.',\n",
       "     'title': 'Preliminaries > ii) The story of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'This balanced influence of the five phenomena types is further visible when considering their ranking correlations (Figure 9, left).\\nA high average correlation of 67.8 ± 6.6 with the overall results',\n",
       "     'title': 'Preliminaries > ii) The story of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Figure 8: Average task metric, difficulty, and discriminability for each phenomena type.\\nThe dashed lines show the average measure over all datasets.',\n",
       "     'title': 'Preliminaries > ii) The story of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '(last column/row) hints that they are facets of a broader occurrence but share common characteristics.\\nStill, breaking into categories is meaningful, as the phenomena types (first five columns/rows) are medium correlated (average of 53.9 ± 14.5).\\nAnalyzing the results of phenomena types further highlights the value of this distinction.\\nWhile results of morphology and syntax are similarly correlated with the overall results (68.2 and 70.2), their direct correlation (69.1) indicates their supplementary nature.\\nFurther, discourse results show the lowest correlation with others (44.8± 16.1), indicating the particular scope.',\n",
       "     'title': 'Preliminaries > ii) The story of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'We analyze how the results of Holmes and those from other evaluations focusing on downstream applications align (Figure 9, right).\\nWe select the OpenLLM benchmark (Beeching et al., 2023), as it covers a wide range of open LMs, in contrast to others like HELM (Liang et al., 2023).\\nFirst, Holmes and OpenLLM results of jointly evaluated LMs are medium correlated, hinting that the linguistic competence of LMs is partly aligned with their downstream abilities.\\nThe nature of this alignment is further evident when focusing on morphology, reasoning, and discourse.\\nInterestingly, and in contrast to syntax and semantics, their correlation to the OpenLLM and Holmes overall results is similar.\\nTherefore, these three phenomena presumably represent skills that are more tested in the general benchmarks.\\nThese correlation patterns are consistent across the three most meaningful OpenLLM datasets (MMLU, TruthfulQA, and GSM8K).\\nAs TruthfulQA shows lower correlations with the linguistic phenomena and other datasets within OpenLLM, we presume this dataset captures distinctly different skills (possibly knowledge).3 These insights show how different benchmarks provide a different scope and supplement themselves simulta- 3Further, it’s also known that we need to expect this dataset to be fully leaked (Balloccu et al., 2024).',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Figure 9: Kendall-tau correlation within Holmes (left) and compared to the OpenLLM benchmark (right).\\nGreen stars indicate significant correlations (p < 0.05).\\nneously.\\nFurther, the above analysis shows, again, the value of assessing the linguistic competence of LMs across different phenomena types, for finegrained analyses.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'iv) The effect of language model architecture.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Next, we discuss the impact of model architecture on the linguistic competence of LMs.\\nIn Figure 11 (left), we compare encoder and decoder LMs.\\nDue to the absence of big encoder LMs, we consider five encoder and six decoder LMs with up to 220m parameters.\\nEncoder LMs show a higher mwr of 52% than decoder LMs (21%).\\nThis observation is the most saturated for morphology or syntax, encompassing a variety of token-level phenomena, like part-of-speech.\\nWe assume that the missing bi-directional encoding of decoder LMs causes this lower performance because the available context of one token heavily depends on its position.\\nThus, even common tokens, like the, have different potential representations - at the beginning or in the middle of a sentence.\\nThese instabilities are further evident when considering Figure 11 (right) which reports the accuracy for the top-20 most common POS tokens (such as the) based on the pos, xpos, upos dataset.\\nGiven their high frequency, one expects stable prediction performance.\\nSurprisingly, encoder LMs (BERT and RoBERTa) show higher median accuracy and clearly lower deviations compared to the same-size decoder counterpart (GPT2).\\nWhile scaling model size to 12B (Pythia) and 70B (Llama-2) allows for improved accuracy and lower deviations, decoder LMs do not match the encoder performance, even up to 700 times bigger.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'From Figure 10, we observe for both Pythia and T5 that the linguistic competence scales with model size, and it is particularly pronounced after exceeding 0.5B (Pythia) and 1.0B (T5) parameters.\\nAgain, model architecture is crucial, as T5 LMs (encoderdecoder) exhibit a clearly higher mean winning rate of 40− 70% than Pythia (decoder-only) ones with mwr of 20− 60%.\\nFurther, we found formal phenomena evolving differently with increased model size than functional ones.\\nSpecifically, morphology and syntax start at a lower level, with an apparent performance jump after 0.5B (Pythia) and 1.0B (T5) parameters, followed by slow but steady growth.\\nDifferently, semantics, reasoning, and discourse start at a higher mwr, followed by a continuous improvement as the model size grows.\\nFrom these results, we assume more parameters allow LMs to better approximate simpler co-occurrences in the near neighborhood of words to understand formal phenomena like word dependencies.\\nIn contrast, more parameters do not have the same pronounced effect on functional phenomena, like rhetorical relations, which require an LM to acquire more distant and complex word co-occurrences.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': ' | Model | Morphology | Syntax | Semantics | Reasoning | Discourse | Overall\\n | --- | --- | --- | --- | --- | --- | ---\\n | Llama-2-Chat | Comparison against Llama-2 -8% | +3% | with 7 billion parameters -5% | -9% | -3% | -2%\\n | FLAN-T5 | Comparison against T5 +10% | +2% | with 11 billion parameters -2% | +6% | -2% | +1%\\n | Dolly-v2 | Comparison against Pythia +4% | -3% | with 12 billion parameters -9% | -3% | +4% | -4%\\n | Tülu-2 | Comparison against Llama-2 +5% | +2% | with 13 billion parameters -15% | 0% | -30% | -8%\\n | Orca-2 | -1% | -3% | -4% | +4% | -5% | -2%\\n | Llama-2-chat | +3% | +1% | -6% | +3% | -1% | -1%\\n | Vicuna-v1.5 | +23% | +7% | -3% | +6% | -6% | +4%\\n | FLAN-UL2 | Comparison +40% | against UL2 +16% | with 20 billion +7% | parameters +13% | +1% | +13%\\n | Mixtral-Instruct | Comparison against Mixtral +4% | +3% | with ~47 billion parameters 0% | +6% | -2% | +2%\\n | Tülu-2 | Comparison against Llama-2 +15% | 0% | with 70 billion parameters -11% | -3% | 0% | -2%\\n | Llama-2-Chat +23% +14% +2% +4% +17% +10%\\n | Average | +10% | +4% | -3% | +4% | -2% | +1%\\n',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Table 1: Effect of instruction tuning on the mean winning rate compared to the pre-trained LMs.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'vi) The effect of instruction tuning.\\nFinally, we focus on how instruction tuning affects LMs’ linguistic competence and compare the tuned LMs with their base models—for example, FLAN-UL2 vs. UL2.\\nFrom results in Table 1, we note less saturated effects for the overall scope while being more pronounced for the five phenomenon types again emphasizing the structured and comprehensive evaluation of linguistic competence.\\nOn average, we found instruction tuning has the highest',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Figure 10: Effect of scaling LM parameters considering the T5 and Pythia model families providing eight and five different sizes.\\nWe address the overall scope (left) and the different types of linguistic phenomena (right).',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Figure 11: Comparison of the phenomenon types for encoder and decoder LMs (left) and on the right, the accuracy of the top-20 most common tokens of the three part-of-speech probing datasets for BERT, RoBERTa, GPT2, Pythia, and Llama-2.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'effect on morphology (+10%) followed by syntax (+4%), reasoning (+4%), and a negative effect for semantics −3% and discourse −2%.\\nThese results confirm previous assumptions that instruction tuning updates are often superficial (Yadav et al., 2023; Hershcovitch et al., 2024; Sharma et al., 2023) and that LMs are better at mimicking language (formal phenomena) than understanding it, measured with functional phenomena (Mahowald et al., 2024).\\nFurther, larger models benefit more from instruction tuning.\\nLlama-2-70b-Chat and FLANUL2 gain up to +23% and +40% for morphology and +10% and +13% on average.\\nIn addition, decoder-only LMs (Llama-2 and Pythia) tend to show less pronounced positive effects than encoderdecoder LMs (FLAN-T5-XXL and FLAN-UL2).\\nHowever, they better understand reasoning phenomena.\\nWhen comparing LMs based on Llama-213b, we see that specific fine-tuning methods shape the LMs differently.\\nThe top-ranked 13b LM for Holmes and OpenLLM, Vicuna, was trained on 125k instructions, less than other models.\\nThus, high quality is more important than the number of instructions for LMs’ linguistic competence.\\nTülu loses performance while being trained on a large mixture of data (approx. 330k instructions), the same for its 70b version.\\nFinally, the focus of Orca2 on reasoning is also reflected in its embedding space.\\nThese insights show again that while providing a particular perspective, Holmes shows clear differences between LMs and allows us to map them to methodological decisions.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '6 Efficiency',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'easy, cost-effective integration of new LMs is crucial for widely adopting a benchmark.\\nAs Holmes covers many datasets and examples, it is computationally heavy in encoding text and training the probes.\\nIt takes approx.\\n6 GPU days to encode the 70 million tokens (∼230k pages of text) and 2 days to run the 208 probes for a 70b model.\\nTo account for this issue, we introduce FlashHolmes, a streamlined version of Holmes.\\nIt allows the evaluation of new LMs with a fraction of the compute while maintaining evaluation integrity.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Besides excluding licensed data (18 probing datasets), we analyze the effect of discarding training instances.\\nAs a result, we reduce the computation for encoding and the actual probing simultaneously.\\nWe follow Perlitz et al.\\n(2023) and calculate the rank resolution, 95% CI of model rank difference.\\nThis measure indicates the maximum expected rank deviation from evaluating an LM on FlashHolmes compared to Holmes.\\nFor example, a rank resolution of one means that an LM evaluated on FlashHolmes and Holmes has the same rank or switch place with its neighbors with a probability of 95%.\\nFigure 12 shows the resulting rank resolution when training only on a fraction of the instances, from 1/2 to 1/512.\\nSolely focusing on efficiency (1/512) still provides a decent rank resolution of ~2.7.\\nIn contrast, considering 1/2 of the training data results in the best reliability of ~1.0.\\nTo balance benchmark reliability and efficiency, we compose FlashHolmes using 1/32 of the training instances.\\nPrecisely, it reduces the computation expenses of evaluating LMs to ~3% of what Holmes would have required while pre- serving a high rank-correlation of ~1.3.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Figure 12: Analysis of the reliability vs. efficiency trade-off when reducing the number of training data.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '7 Related Work',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Benchmarking LMs Benchmarks approximate LMs abilities like general language understanding (Wang et al., 2019b,a), out-of-distribution generalization (Yang et al., 2023; Waldis et al., 2024b), adversarial scenarios (Nie et al., 2020; Wang et al., 2021), or retrieval like BEIR (Thakur et al., 2021) or MTEB (Muennighoff et al., 2023).\\nWith the advent of larger LMs, the methodological focus shifted to prompting-based evaluations which evaluate the LMs’ response to provided instructions (Brown et al., 2020; Hendrycks et al., 2021; Srivastava et al., 2022) covering application-oriented tasks (Liang et al., 2023), or mathematical reasoning (e.g., GSM8K (Cobbe et al., 2021)).',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Assessing the Linguistic Competence of LMs The analysis of LMs’ linguistic competence ranges from analyzing static word vectors (Köhn, 2015), sentence embeddings (Conneau et al., 2018; Adi et al., 2017), the internals of translation models (Shi et al., 2016; Bau et al., 2019), or contextualized LMs (Tenney et al., 2019b,a; Hewitt and Manning, 2019).\\nOther work addressed methodological aspects, such as using control tasks (Hewitt and Liang, 2019), assessing LMs from an information theory perspective (Voita and Titov, 2020;',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'Pimentel et al., 2020), or evaluating causal effects in LMs (Elazar et al., 2021).\\nFinally, another line of work focuses on whether LMs follow human understanding of linguistic competence when solving downstream tasks (Belinkov, 2022; Aw et al., 2023; Mahowald et al., 2024).\\nHowever, Mosbach et al.\\n(2020b) and Waldis et al.\\n(2024a) found finetuning for downstream tasks actually hurting the understanding of linguistic phenomena.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'While prior studies assessing the linguistic competence of LMs tend to focus on a limited set of linguistic phenomena or models, Holmes provides extensive coverage of both phenomena and eval- uated LMs.\\nUnlike recent evaluations based on prompting methods (Blevins et al., 2023; Liang et al., 2023; Amouyal et al., 2024), Holmes assesses the internal representations of LMs directly.\\nThis approach allows for detailed analysis of specific model characteristics, such as architecture, and helps separate the linguistic competence from other cognitive abilities.\\nThereby, we respond to recent calls for a thorough and explicit evaluation of linguistic phenomena (Hu and Levy, 2023; Lu et al., 2023; Mahowald et al., 2024).',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': '8 Conclusion',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Preliminaries'},\n",
       "    {'text': 'marks the most up-to-date and extensive consolidation of existing resources addressing the need to assess the linguistic competence of LMs in isolation.\\nOur experiments demonstrate that LMs’ linguistic competence is pronounced regarding formal phenomena but lacks functional ones when information about broader textual contexts, such as rhetorical structure, is required.\\nFurther, size, architecture, and instruction tuning crucially account for differences among LMs.\\nAs LM and resources in the landscape of linguistics continue to grow, we will actively extend Holmes with further probing datasets, evaluate upcoming LMs, and plan to incorporate multilingualism.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Preliminaries'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff3041a1c60>]},\n",
       "  {'text': '3.1 Scope\\nWe analyze 28k papers (P) from 2015 to August 2023 of major NLP conferences (TACL, ACL, AACL, COLING, EACL, EMNLP, NAACL, and corresponding workshops) expanded with selected work from other venues such as ICLR.\\nTo identify relevant work, we follow a semiautomatic approach.\\nFirst, we automatically select papers based on their meta-data and full text.1 We select a total of 493 candidate papers matching at least one of the following three criteria (P ′ = {∀p ∈ P |p ∈ P1 ∪ p ∈\\nP2 ∪\\np\\n∈ P3}):\\nP1: papers contain probing or probe in the title.\\nP2: papers contain probing or probe in the abstract and at least five times in the main content.\\nP3: papers contain probing or probe at least ten times in the main content.\\nWe manually verified these automatically curated candidates (P ′) and found 274 relevant papers (Pr).We selected them as they either evaluate LMs regarding one or more linguistic phenomena as part of the analysis or as a main contribution.\\nThis involves filtering papers using the term probing in other senses, such as probing hash tables (Bogoychev and Lopez, 2016).\\n3.2 Analysis Next, we analyze these 274 relevant studies (Pr).\\ni) Scattered evolution calls for consolidation.\\nFirst, we analyze the evolution of the relevant studies.\\nFigure 3 relates how these studies cite each other (probing citations Cp) compared to other 1We use PyPDF2 v3.0.0, DBLP and semanticscholar API.\\nFigure 3: Citation analysis considering probing citations originating from the set of relevant work and every other citation (general citations).\\nThe color scale indicates the ratio (α) between them.\\ngathered citations (general citations Cg).\\nColorized, we show the ratio α between these two measures α = |Cp|+1 |Cg |+1.\\nFirst, only a fraction of the works gained general attention, as 16 papers exceeded 200 general citations.\\nFurther, probing works cite each other rather sparsely, with an average probing citation ratio of α = 0.1.\\nTherefore, we see other fields are paying little attention to the linguistic competence of LMs.\\nPaired with scattered citation patterns, we identify the need to consolidate existing resources to solidly ground research in this field.\\nii) Probing work prioritizes tasks and analytics over methods.\\nWe categorize the selected work according to their probing focus: methodological, new methods, like control tasks (Hewitt and Liang, 2019) or minimum description length (Voita and Titov, 2020); task-focused assessing specific linguistic phenomena as main contributions, such as discourse relations in text (Koto et al., 2021); and analytical using probing tasks to analyze LMs, such as the impact of pre-training data (Zhang et al., 2021).\\nFigure 4 shows: the majority (51.8%) of studies focus on specific probing tasks like numeric scales (Zhang et al., 2020), or morphosyntactic (Shapiro et al., 2021); 35.7% use probing as a supplementary analytical tool, for example, analyzing the effect of fine-tuning (Mosbach et al., 2020a; Zhu et al., 2022a); 12.5% address methodological problems related to probing (Wu et al., 2020; Immer et al., 2022; Zhu et al., 2022b).\\niii) The dominance of classifier-based probing.\\nNext, we analyze the specific employed probing method: classifier, using linear or shallow models to probe internal representations of LMs, as demonstrated in Tenney et al.\\n(2019a); mask, letting LMs fill gaps to verify linguistic phenomena, as shown in Talmor et al.\\n(2020) or Warstadt et al.\\n(2020); at- tention, which relies on attention patterns, as used in Pandit and Hou (2021) for bridging; and other, methods not belonging to the previous three categories, such as dimension selection (Torroba Hennigen et al., 2020).\\nMost studies utilize the classifierbased probing method (74%), 20% conduct maskbased probing, and only a minority of work (∼ 3%) considers attention patterns or other approaches.\\nFigure 4: Categorization of the selected studies by their focus and their conducted probing method.\\niv) Tasks and LMs are barely broadly evaluated.\\nFinally, we analyze which tasks and LMs the relevant probing studies consider.\\nFor example, Tenney et al.\\n(2019b) considers BERT and probes POS tagging, semantic-role labeling (SRL), and other ones.\\nAggregated over all studies, we found a broad coverage of 289 unique tasks and 161 distinct LMs.\\nBelow, we delve into the details and highlight noteworthy findings.\\nWe analyze how LMs and tasks are considered jointly in Figure 5.\\nDespite the broad coverage, single studies, including fundamental ones, maintain a particular focus and consider only a fraction of LMs and tasks.\\nFor example, while most tasks (72%) were assessed on BERT, RoBERTa’s coverage has already declined to 42%.\\nConversely, part-of-speech tagging (POS), the most probed task, was only evaluated on 23% of the LMs, for example, not covering prominent examples like BART (Lewis et al., 2020).\\nNotably, more recently released larger and powerful LMs, like PYTHIA (Biderman et al., 2023), UL2 (Tay et al., 2023), or LLAMA-2 (Touvron et al., 2023), and instruction-tuned LMs (FLAN-T5 (Chung et al., 2022), LLAMA-2-Chat (Touvron et al., 2023), or TK-Instruct (Wang et al., 2022) are missing almost entirely, with single more recent exceptions (Hu and Levy, 2023; Waldis et al., 2024a).\\nAgain, these insights underscore the need to consolidate existing resources for more dense coverage.\\nThis is further evident when considering Figure 5, where we sort LMs and tasks according to how often they were mentioned in the relevant works.\\nThen, we plot\\nFigure 5: Overview of how many tasks single LMs cover and vice versa - single examples are highlighted.\\nFigure 6: Cumulative coverage of LMs and tasks, considering all relevant studies and their focus.\\ntheir cumulative coverage concerning all mentions.\\nFor example, considering all studies (red line), the top-10 most mentioned LMs account for 80% of all LMs mentions (black dot).\\nIn contrast, the other 151 unique LMs account for only 40%.\\nComparing the paper focus, we see that methodological studies rely only on 24 LMs and 36 tasks.\\nIn contrast, task-focused and analytical work covers a similar number of LMs (91 and 99, respectively).\\nHowever, due to their distinct focus, task-focused studies cover significantly more tasks (202) than analytical ones (115).',\n",
       "   'title': '3.1 Scope',\n",
       "   'chunks': [{'text': 'We analyze 28k papers (P) from 2015 to August 2023 of major NLP conferences (TACL, ACL, AACL, COLING, EACL, EMNLP, NAACL, and corresponding workshops) expanded with selected work from other venues such as ICLR.\\nTo identify relevant work, we follow a semiautomatic approach.\\nFirst, we automatically select papers based on their meta-data and full text.1 We select a total of 493 candidate papers matching at least one of the following three criteria (P ′ = {∀p ∈ P |p ∈ P1 ∪ p ∈',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 2,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'P2 ∪\\np\\n∈ P3}):',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 2,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': '3.2 Analysis Next, we analyze these 274 relevant studies (Pr).',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 2,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'First, we analyze the evolution of the relevant studies.\\nFigure 3 relates how these studies cite each other (probing citations Cp) compared to other 1We use PyPDF2 v3.0.0, DBLP and semanticscholar API.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 2,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'Figure 3: Citation analysis considering probing citations originating from the set of relevant work and every other citation (general citations).\\nThe color scale indicates the ratio (α) between them.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'gathered citations (general citations Cg).\\nColorized, we show the ratio α between these two measures α = |Cp|+1 |Cg |+1.\\nFirst, only a fraction of the works gained general attention, as 16 papers exceeded 200 general citations.\\nFurther, probing works cite each other rather sparsely, with an average probing citation ratio of α = 0.1.\\nTherefore, we see other fields are paying little attention to the linguistic competence of LMs.\\nPaired with scattered citation patterns, we identify the need to consolidate existing resources to solidly ground research in this field.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'ii) Probing work prioritizes tasks and analytics over methods.\\nWe categorize the selected work according to their probing focus: methodological, new methods, like control tasks (Hewitt and Liang, 2019) or minimum description length (Voita and Titov, 2020); task-focused assessing specific linguistic phenomena as main contributions, such as discourse relations in text (Koto et al., 2021); and analytical using probing tasks to analyze LMs, such as the impact of pre-training data (Zhang et al., 2021).\\nFigure 4 shows: the majority (51.8%) of studies focus on specific probing tasks like numeric scales (Zhang et al., 2020), or morphosyntactic (Shapiro et al., 2021); 35.7% use probing as a supplementary analytical tool, for example, analyzing the effect of fine-tuning (Mosbach et al., 2020a; Zhu et al., 2022a); 12.5% address methodological problems related to probing (Wu et al., 2020; Immer et al., 2022; Zhu et al., 2022b).',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'iii) The dominance of classifier-based probing.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'Next, we analyze the specific employed probing method: classifier, using linear or shallow models to probe internal representations of LMs, as demonstrated in Tenney et al.\\n(2019a); mask, letting LMs fill gaps to verify linguistic phenomena, as shown in Talmor et al.\\n(2020) or Warstadt et al.\\n(2020); at- tention, which relies on attention patterns, as used in Pandit and Hou (2021) for bridging; and other, methods not belonging to the previous three categories, such as dimension selection (Torroba Hennigen et al., 2020).\\nMost studies utilize the classifierbased probing method (74%), 20% conduct maskbased probing, and only a minority of work (∼ 3%) considers attention patterns or other approaches.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'Figure 4: Categorization of the selected studies by their focus and their conducted probing method.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'Figure 5: Overview of how many tasks single LMs cover and vice versa - single examples are highlighted.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'Figure 6: Cumulative coverage of LMs and tasks, considering all relevant studies and their focus.',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.1 Scope'},\n",
       "    {'text': 'their cumulative coverage concerning all mentions.\\nFor example, considering all studies (red line), the top-10 most mentioned LMs account for 80% of all LMs mentions (black dot).\\nIn contrast, the other 151 unique LMs account for only 40%.\\nComparing the paper focus, we see that methodological studies rely only on 24 LMs and 36 tasks.\\nIn contrast, task-focused and analytical work covers a similar number of LMs (91 and 99, respectively).\\nHowever, due to their distinct focus, task-focused studies cover significantly more tasks (202) than analytical ones (115).',\n",
       "     'title': 'Preliminaries > 3.1 Scope',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.1 Scope'}],\n",
       "   'tables': []},\n",
       "  {'text': '3.3 Summary\\nThis meta-study emphasizes the need to consolidate existing resources for a comprehensive assessment of the linguistic competence of LMs — a manifold but rather blind spot in evaluation research.\\nApart from more thorough evaluations, such a stimulus can significantly boost future research, as happened in computer vision with ImageNet (Deng et al., 2009) or in NLP with GLUE and SuperGLUE (Wang et al., 2019a,b).\\n4 Holmes Benchmark\\nWith Holmes, we provide an extensive ground to tackle these identified deficiencies in the existing literature and comprehensively investigate the English linguistic competence of LMs.\\nSpecifically, Holmes features 208 datasets addressing distinct aspects of 66 phenomena covering morphology, syntax, semantic, reasoning, and discourse.\\n4.1 Datasets\\nTo feature a total of 208 unique datasets, we leverage existing and established resources like OntoNotes (Weischedel et al., 2013), English Web Treebank (Silveira et al., 2014), or BLIMP (Warstadt et al., 2020) and create datasets addressing phenomena like the POS of words, their dependencies or determine the linguistic acceptability of sentences.\\nFurther, we include a range of less employed data, addressing contextualization of words (Klafka and Ettinger, 2020), reasoning (Talmor et al., 2020), semantic decomposition (White et al., 2016; Rudinger et al., 2018a,b; Govindarajan et al., 2019; Vashishtha et al., 2019), grammatical knowledge (Huebner et al., 2021), bridging (Pandit and Hou, 2021), and rhetorical (Carlson et al., 2001) and discourse (Webber et al., 2019) structure in text.\\nFinally, we cover rarely probed phenomena like negation (Szarvas et al., 2008; Konstantinova et al., 2012; Vahtola et al., 2022), or word complexity (Paetzold and Specia, 2016).\\n4.2 Structure\\nApart from the comprehensive scope, Holmes provides a clear structure for specific evaluations on different levels of aggregation.\\nWe first group the datasets according to the linguistic phenomena addressed.\\nThen, we categorize these phenomena into their previously introduced type (see § 2) - morphology, syntax, semantics, reasoning and discourse.\\nWe rely on the categorization provided by the specific studies whenever given.\\nThe detailed categorization is given in § A.3.\\n4.3 Experimental Setup\\nHolmes evaluation follows the primarily used classifier-based probing paradigm, as described in § 2.\\nConsidering the internal representations allows us to maximally disentangle the understanding of distinct linguistic phenomena from each other and from other cognitive abilities (like following textual instructions).\\nFurther, this method allows us to assess any type of LMs, including sparse, static, or contextualized ones.\\nBased on the specific dataset, we either select the embeddings of the specific input tokens (like single words for POS tagging) or average embeddings across a span or the whole sentence.\\nWe define a probing task as training a probe fp (linear model without intermediate layers) using these embeddings as inputs and the dataset labels as training signals.\\nIf not defined in the original data, we divide the dataset samples into train/dev/test split following a ratio of 70/10/20.\\nWe repeat this procedure five times using different random seeds and aggregate the results afterward.\\n4.4 Evaluations\\nWe approximate how well an LM encodes specific linguistic phenomena using the absolute prediction performance of the probes.\\nIn addition, we rigorously evaluate the reliability of probing results using control tasks and from an information theory perspective (Voita and Titov, 2020; Hewitt and Liang, 2019).\\nDifferent from commonly used prompting assessments, this particular evaluation protocol refrains from known fallacies in which the results and conclusions are sensible with specific instructions (Mizrahi et al., 2024; Min et al., 2022) or few-shot examples (Lu et al., 2023).\\nTask Score Metric Based on a dataset’s specific task type, we use a corresponding performance measure, macro F1 for classification or Pearson correlation for regression.\\nIn addition, we calculate the standard deviation σ of the probe across multiple seeds.\\nA lower σ indicates a better encoding of a given linguistic phenomenon since the mea- surement is robust to noise.\\nFurther, we use the task score for ranking-based evaluation of all eval- uated LMs L = {l1,, lm} within Holmes.\\nWe calculate the mean winning rate mwr (in percentage), telling us how many times one LM l1 wins against others (Liang et al., 2023).\\nWith a higher mwr, we assume an LM encodes tested linguistic phenomena better than others.\\nCompression Next, we evaluate the probes’ reliability from an information-theoretic perspective.\\nFollowing Voita and Titov (2020), we use the compression I to measure how well a probe compresses input data.\\nA higher I means fewer bits are needed, indicating that the given linguistic phenomenon is more clearly encoded in the embeddings.\\nSelectivity A reliable probe should grasp patterns relevant to the tested phenomena in the internal representations of LMs but should not be able to learn anything else.\\nTherefore, we expect high performance when evaluating the specific dataset but low performance when we randomize training signals.\\nWe check this using control tasks introduced in Hewitt and Liang (2019).\\nSpecifically, we calculate the selectivity S as the difference between the probe trained with the original labels y and the control task where we train the probe with randomly assigned labels y′.\\nWith a higher S, we assume the detected patterns are relevant for the specific phenomena under test, as random patterns do not lead to similar performance.\\n5 Holmes Results\\nUsing Holmes, we evaluate a diverse collection of 59 LMs.2 Using the results of these extensive experiments, we first answer the research question:\\nwhat is the linguistic competence of LMs?\\nIn doing so, we discuss the reliability of results (i), the linguistic competence of LMs concerning the unique structure of Holmes (ii), and how these results relate to other downstream abilities (iii).\\nSubsequently, we examine how linguistic competence varies among LMs, as we find LMs prevailing for different types of linguistic phenomena (Figure 1) and delve into the effects of model architecture (iv), size (v), and instruction tuning (vi).',\n",
       "   'title': '3.3 Summary',\n",
       "   'chunks': [{'text': 'This meta-study emphasizes the need to consolidate existing resources for a comprehensive assessment of the linguistic competence of LMs — a manifold but rather blind spot in evaluation research.\\nApart from more thorough evaluations, such a stimulus can significantly boost future research, as happened in computer vision with ImageNet (Deng et al., 2009) or in NLP with GLUE and SuperGLUE (Wang et al., 2019a,b).',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': '4 Holmes Benchmark',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': 'With Holmes, we provide an extensive ground to tackle these identified deficiencies in the existing literature and comprehensively investigate the English linguistic competence of LMs.\\nSpecifically, Holmes features 208 datasets addressing distinct aspects of 66 phenomena covering morphology, syntax, semantic, reasoning, and discourse.',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': '4.1 Datasets',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': 'To feature a total of 208 unique datasets, we leverage existing and established resources like OntoNotes (Weischedel et al., 2013), English Web Treebank (Silveira et al., 2014), or BLIMP (Warstadt et al., 2020) and create datasets addressing phenomena like the POS of words, their dependencies or determine the linguistic acceptability of sentences.\\nFurther, we include a range of less employed data, addressing contextualization of words (Klafka and Ettinger, 2020), reasoning (Talmor et al., 2020), semantic decomposition (White et al., 2016; Rudinger et al., 2018a,b; Govindarajan et al., 2019; Vashishtha et al., 2019), grammatical knowledge (Huebner et al., 2021), bridging (Pandit and Hou, 2021), and rhetorical (Carlson et al., 2001) and discourse (Webber et al., 2019) structure in text.\\nFinally, we cover rarely probed phenomena like negation (Szarvas et al., 2008; Konstantinova et al., 2012; Vahtola et al., 2022), or word complexity (Paetzold and Specia, 2016).',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': '4.2 Structure',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': 'Apart from the comprehensive scope, Holmes provides a clear structure for specific evaluations on different levels of aggregation.\\nWe first group the datasets according to the linguistic phenomena addressed.\\nThen, we categorize these phenomena into their previously introduced type (see § 2) - morphology, syntax, semantics, reasoning and discourse.\\nWe rely on the categorization provided by the specific studies whenever given.\\nThe detailed categorization is given in § A.3.',\n",
       "     'title': 'Preliminaries > 3.3 Summary',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': 'Holmes evaluation follows the primarily used classifier-based probing paradigm, as described in § 2.\\nConsidering the internal representations allows us to maximally disentangle the understanding of distinct linguistic phenomena from each other and from other cognitive abilities (like following textual instructions).\\nFurther, this method allows us to assess any type of LMs, including sparse, static, or contextualized ones.\\nBased on the specific dataset, we either select the embeddings of the specific input tokens (like single words for POS tagging) or average embeddings across a span or the whole sentence.\\nWe define a probing task as training a probe fp (linear model without intermediate layers) using these embeddings as inputs and the dataset labels as training signals.\\nIf not defined in the original data, we divide the dataset samples into train/dev/test split following a ratio of 70/10/20.\\nWe repeat this procedure five times using different random seeds and aggregate the results afterward.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.3 Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': 'We approximate how well an LM encodes specific linguistic phenomena using the absolute prediction performance of the probes.\\nIn addition, we rigorously evaluate the reliability of probing results using control tasks and from an information theory perspective (Voita and Titov, 2020; Hewitt and Liang, 2019).\\nDifferent from commonly used prompting assessments, this particular evaluation protocol refrains from known fallacies in which the results and conclusions are sensible with specific instructions (Mizrahi et al., 2024; Min et al., 2022) or few-shot examples (Lu et al., 2023).',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': 'Task Score Metric Based on a dataset’s specific task type, we use a corresponding performance measure, macro F1 for classification or Pearson correlation for regression.\\nIn addition, we calculate the standard deviation σ of the probe across multiple seeds.\\nA lower σ indicates a better encoding of a given linguistic phenomenon since the mea- surement is robust to noise.\\nFurther, we use the task score for ranking-based evaluation of all eval- uated LMs L = {l1,, lm} within Holmes.\\nWe calculate the mean winning rate mwr (in percentage), telling us how many times one LM l1 wins against others (Liang et al., 2023).\\nWith a higher mwr, we assume an LM encodes tested linguistic phenomena better than others.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': 'Compression Next, we evaluate the probes’ reliability from an information-theoretic perspective.\\nFollowing Voita and Titov (2020), we use the compression I to measure how well a probe compresses input data.\\nA higher I means fewer bits are needed, indicating that the given linguistic phenomenon is more clearly encoded in the embeddings.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': 'Selectivity A reliable probe should grasp patterns relevant to the tested phenomena in the internal representations of LMs but should not be able to learn anything else.\\nTherefore, we expect high performance when evaluating the specific dataset but low performance when we randomize training signals.\\nWe check this using control tasks introduced in Hewitt and Liang (2019).\\nSpecifically, we calculate the selectivity S as the difference between the probe trained with the original labels y and the control task where we train the probe with randomly assigned labels y′.\\nWith a higher S, we assume the detected patterns are relevant for the specific phenomena under test, as random patterns do not lead to similar performance.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': '5 Holmes Results',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': 'Using Holmes, we evaluate a diverse collection of 59 LMs.2 Using the results of these extensive experiments, we first answer the research question:',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '3.3 Summary'},\n",
       "    {'text': 'what is the linguistic competence of LMs?\\nIn doing so, we discuss the reliability of results (i), the linguistic competence of LMs concerning the unique structure of Holmes (ii), and how these results relate to other downstream abilities (iii).\\nSubsequently, we examine how linguistic competence varies among LMs, as we find LMs prevailing for different types of linguistic phenomena (Figure 1) and delve into the effects of model architecture (iv), size (v), and instruction tuning (vi).',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '3.3 Summary'}],\n",
       "   'tables': []},\n",
       "  {'text': '4.3 Experimental Setup\\nHolmes evaluation follows the primarily used classifier-based probing paradigm, as described in § 2.\\nConsidering the internal representations allows us to maximally disentangle the understanding of distinct linguistic phenomena from each other and from other cognitive abilities (like following textual instructions).\\nFurther, this method allows us to assess any type of LMs, including sparse, static, or contextualized ones.\\nBased on the specific dataset, we either select the embeddings of the specific input tokens (like single words for POS tagging) or average embeddings across a span or the whole sentence.\\nWe define a probing task as training a probe fp (linear model without intermediate layers) using these embeddings as inputs and the dataset labels as training signals.\\nIf not defined in the original data, we divide the dataset samples into train/dev/test split following a ratio of 70/10/20.\\nWe repeat this procedure five times using different random seeds and aggregate the results afterward.',\n",
       "   'title': '4.3 Experimental Setup',\n",
       "   'chunks': [{'text': 'Holmes evaluation follows the primarily used classifier-based probing paradigm, as described in § 2.\\nConsidering the internal representations allows us to maximally disentangle the understanding of distinct linguistic phenomena from each other and from other cognitive abilities (like following textual instructions).\\nFurther, this method allows us to assess any type of LMs, including sparse, static, or contextualized ones.\\nBased on the specific dataset, we either select the embeddings of the specific input tokens (like single words for POS tagging) or average embeddings across a span or the whole sentence.\\nWe define a probing task as training a probe fp (linear model without intermediate layers) using these embeddings as inputs and the dataset labels as training signals.\\nIf not defined in the original data, we divide the dataset samples into train/dev/test split following a ratio of 70/10/20.\\nWe repeat this procedure five times using different random seeds and aggregate the results afterward.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.3 Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.3 Experimental Setup'}],\n",
       "   'tables': []},\n",
       "  {'text': '4.4 Evaluations\\nWe approximate how well an LM encodes specific linguistic phenomena using the absolute prediction performance of the probes.\\nIn addition, we rigorously evaluate the reliability of probing results using control tasks and from an information theory perspective (Voita and Titov, 2020; Hewitt and Liang, 2019).\\nDifferent from commonly used prompting assessments, this particular evaluation protocol refrains from known fallacies in which the results and conclusions are sensible with specific instructions (Mizrahi et al., 2024; Min et al., 2022) or few-shot examples (Lu et al., 2023).\\nTask Score Metric Based on a dataset’s specific task type, we use a corresponding performance measure, macro F1 for classification or Pearson correlation for regression.\\nIn addition, we calculate the standard deviation σ of the probe across multiple seeds.\\nA lower σ indicates a better encoding of a given linguistic phenomenon since the mea- surement is robust to noise.\\nFurther, we use the task score for ranking-based evaluation of all eval- uated LMs L = {l1,, lm} within Holmes.\\nWe calculate the mean winning rate mwr (in percentage), telling us how many times one LM l1 wins against others (Liang et al., 2023).\\nWith a higher mwr, we assume an LM encodes tested linguistic phenomena better than others.\\nCompression Next, we evaluate the probes’ reliability from an information-theoretic perspective.\\nFollowing Voita and Titov (2020), we use the compression I to measure how well a probe compresses input data.\\nA higher I means fewer bits are needed, indicating that the given linguistic phenomenon is more clearly encoded in the embeddings.\\nSelectivity A reliable probe should grasp patterns relevant to the tested phenomena in the internal representations of LMs but should not be able to learn anything else.\\nTherefore, we expect high performance when evaluating the specific dataset but low performance when we randomize training signals.\\nWe check this using control tasks introduced in Hewitt and Liang (2019).\\nSpecifically, we calculate the selectivity S as the difference between the probe trained with the original labels y and the control task where we train the probe with randomly assigned labels y′.\\nWith a higher S, we assume the detected patterns are relevant for the specific phenomena under test, as random patterns do not lead to similar performance.\\n5 Holmes Results\\nUsing Holmes, we evaluate a diverse collection of 59 LMs.2 Using the results of these extensive experiments, we first answer the research question:\\nwhat is the linguistic competence of LMs?\\nIn doing so, we discuss the reliability of results (i), the linguistic competence of LMs concerning the unique structure of Holmes (ii), and how these results relate to other downstream abilities (iii).\\nSubsequently, we examine how linguistic competence varies among LMs, as we find LMs prevailing for different types of linguistic phenomena (Figure 1) and delve into the effects of model architecture (iv), size (v), and instruction tuning (vi).',\n",
       "   'title': '4.4 Evaluations',\n",
       "   'chunks': [{'text': 'We approximate how well an LM encodes specific linguistic phenomena using the absolute prediction performance of the probes.\\nIn addition, we rigorously evaluate the reliability of probing results using control tasks and from an information theory perspective (Voita and Titov, 2020; Hewitt and Liang, 2019).\\nDifferent from commonly used prompting assessments, this particular evaluation protocol refrains from known fallacies in which the results and conclusions are sensible with specific instructions (Mizrahi et al., 2024; Min et al., 2022) or few-shot examples (Lu et al., 2023).',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.4 Evaluations'},\n",
       "    {'text': 'Task Score Metric Based on a dataset’s specific task type, we use a corresponding performance measure, macro F1 for classification or Pearson correlation for regression.\\nIn addition, we calculate the standard deviation σ of the probe across multiple seeds.\\nA lower σ indicates a better encoding of a given linguistic phenomenon since the mea- surement is robust to noise.\\nFurther, we use the task score for ranking-based evaluation of all eval- uated LMs L = {l1,, lm} within Holmes.\\nWe calculate the mean winning rate mwr (in percentage), telling us how many times one LM l1 wins against others (Liang et al., 2023).\\nWith a higher mwr, we assume an LM encodes tested linguistic phenomena better than others.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.4 Evaluations'},\n",
       "    {'text': 'Compression Next, we evaluate the probes’ reliability from an information-theoretic perspective.\\nFollowing Voita and Titov (2020), we use the compression I to measure how well a probe compresses input data.\\nA higher I means fewer bits are needed, indicating that the given linguistic phenomenon is more clearly encoded in the embeddings.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.4 Evaluations'},\n",
       "    {'text': 'Selectivity A reliable probe should grasp patterns relevant to the tested phenomena in the internal representations of LMs but should not be able to learn anything else.\\nTherefore, we expect high performance when evaluating the specific dataset but low performance when we randomize training signals.\\nWe check this using control tasks introduced in Hewitt and Liang (2019).\\nSpecifically, we calculate the selectivity S as the difference between the probe trained with the original labels y and the control task where we train the probe with randomly assigned labels y′.\\nWith a higher S, we assume the detected patterns are relevant for the specific phenomena under test, as random patterns do not lead to similar performance.',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.4 Evaluations'},\n",
       "    {'text': '5 Holmes Results',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.4 Evaluations'},\n",
       "    {'text': 'Using Holmes, we evaluate a diverse collection of 59 LMs.2 Using the results of these extensive experiments, we first answer the research question:',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.4 Evaluations'},\n",
       "    {'text': 'what is the linguistic competence of LMs?\\nIn doing so, we discuss the reliability of results (i), the linguistic competence of LMs concerning the unique structure of Holmes (ii), and how these results relate to other downstream abilities (iii).\\nSubsequently, we examine how linguistic competence varies among LMs, as we find LMs prevailing for different types of linguistic phenomena (Figure 1) and delve into the effects of model architecture (iv), size (v), and instruction tuning (vi).',\n",
       "     'title': 'Preliminaries > 3.3 Summary > 4.4 Evaluations',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.4 Evaluations'}],\n",
       "   'tables': []},\n",
       "  {'text': 'i) The reliability of Holmes\\nFirst, we show the reliability of probing-based evaluation, using deviation σ, compression I, and selectivity S results in Figure 7.\\nSingle outliers are datasets that are too hard for all LMs, as the sample size is too small, or the linguistic phenomena under test are too complex, as the ability to detect spans causes speculations in a text.\\nWe average these metrics for every dataset across all LMs.\\nNote, for selectivity, we consider only base-sized model (10m-200m parameters) for computational efficiency.\\nFirst, we found a low average deviation (σ = 0.02), indicating the high reliability of probes across random seeds.\\nThese results also highlight the stability of probing results, compared to prompting-based ones where results across many paraphrased prompts lead to a deviation of σ = 0.07 reported in Mizrahi et al.\\n(2024).\\nNext, substantial compression (average I = 1.9) and selectivity (average S = 0.31) further confirm the probes’ reliability.\\nInterestingly, one identifies two parallel trends for selectivity.\\nHarder datasets with many labels, like POS tagging, are arranged around a selectivity of 0.1 to 0.4 and a task metric of 0.3.\\nIn contrast, for easier binary classification tasks (such as linguistic applicability), we observe selectivity around 0.2 to 0.5 and a task metric of 0.6 to 0.9.\\n2Find a complete list in Appendix § A.2.\\nFigure 7: Reliability evaluation using deviation, compression (log), and selectivity on the y-axis for all 208 probing datasets.\\nThe x-axis represents the task metrics (either person correlation or macro F1).\\nFurther, we measure a significant (p < 0.05) positive correlation between the task metrics and the compression (τ = 0.64) and selectivity (τ = 0.65).\\nThis further confirms our reliability assumption and allows us to trust the task metric as the primary evaluation measure.',\n",
       "   'title': 'i) The reliability of Holmes',\n",
       "   'chunks': [{'text': 'First, we show the reliability of probing-based evaluation, using deviation σ, compression I, and selectivity S results in Figure 7.\\nSingle outliers are datasets that are too hard for all LMs, as the sample size is too small, or the linguistic phenomena under test are too complex, as the ability to detect spans causes speculations in a text.\\nWe average these metrics for every dataset across all LMs.\\nNote, for selectivity, we consider only base-sized model (10m-200m parameters) for computational efficiency.',\n",
       "     'title': 'Preliminaries > i) The reliability of Holmes',\n",
       "     'page': 5,\n",
       "     'source_doc': 'i) The reliability of Holmes'},\n",
       "    {'text': 'First, we found a low average deviation (σ = 0.02), indicating the high reliability of probes across random seeds.\\nThese results also highlight the stability of probing results, compared to prompting-based ones where results across many paraphrased prompts lead to a deviation of σ = 0.07 reported in Mizrahi et al.\\n(2024).\\nNext, substantial compression (average I = 1.9) and selectivity (average S = 0.31) further confirm the probes’ reliability.\\nInterestingly, one identifies two parallel trends for selectivity.\\nHarder datasets with many labels, like POS tagging, are arranged around a selectivity of 0.1 to 0.4 and a task metric of 0.3.\\nIn contrast, for easier binary classification tasks (such as linguistic applicability), we observe selectivity around 0.2 to 0.5 and a task metric of 0.6 to 0.9.',\n",
       "     'title': 'Preliminaries > i) The reliability of Holmes',\n",
       "     'page': 5,\n",
       "     'source_doc': 'i) The reliability of Holmes'},\n",
       "    {'text': '2Find a complete list in Appendix § A.2.',\n",
       "     'title': 'Preliminaries > i) The reliability of Holmes',\n",
       "     'page': 5,\n",
       "     'source_doc': 'i) The reliability of Holmes'},\n",
       "    {'text': 'Figure 7: Reliability evaluation using deviation, compression (log), and selectivity on the y-axis for all 208 probing datasets.\\nThe x-axis represents the task metrics (either person correlation or macro F1).',\n",
       "     'title': 'Preliminaries > i) The reliability of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'i) The reliability of Holmes'},\n",
       "    {'text': 'Further, we measure a significant (p < 0.05) positive correlation between the task metrics and the compression (τ = 0.64) and selectivity (τ = 0.65).\\nThis further confirms our reliability assumption and allows us to trust the task metric as the primary evaluation measure.',\n",
       "     'title': 'Preliminaries > i) The reliability of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'i) The reliability of Holmes'}],\n",
       "   'tables': []},\n",
       "  {'text': 'ii) The story of Holmes\\nWe focus on what Holmes tells us in general and regarding formal and functional phenomena, as defined in § 2. We report in Figure 8 the task metric, discriminability, and selectivity, averaged for every phenomena type.\\nNote, discriminability (Rodriguez et al., 2021) quantifies the alignment of LMs ranking of one specific dataset compared to the overall rankings using the Kendall Tau correlation.\\nConsidering these three metrics, all tested LMs strongly encode formal phenomena (morphology and syntax), which often depend on the local neighborhood of words.\\nTherefore, we assume that LMs approximate these co-occurrences during pre-training with high precision.\\nFor example, the specific POS tag of a word, like man (noun), primarily depends on its surroundings, such as the frequent predecessor the.\\nIn contrast, LMs encode less information about functional phenomena (semantics, reasoning, and discourse) since they show a relatively low performance regarding the task metric.\\nFor these functional phenomena, we assume more complex co-occurrences are required to capture the broad context in language, such as the rhetorical relation of two distant text spans.\\nDespite these differences between formal and functional phenomena types, they contribute to the benchmark in a balanced way.\\nA low to medium discriminability indicates that none of these types of linguistic phenomena dominates the overall LM rankings.\\nThis balanced influence of the five phenomena types is further visible when considering their ranking correlations (Figure 9, left).\\nA high average correlation of 67.8 ± 6.6 with the overall results\\nFigure 8: Average task metric, difficulty, and discriminability for each phenomena type.\\nThe dashed lines show the average measure over all datasets.\\n(last column/row) hints that they are facets of a broader occurrence but share common characteristics.\\nStill, breaking into categories is meaningful, as the phenomena types (first five columns/rows) are medium correlated (average of 53.9 ± 14.5).\\nAnalyzing the results of phenomena types further highlights the value of this distinction.\\nWhile results of morphology and syntax are similarly correlated with the overall results (68.2 and 70.2), their direct correlation (69.1) indicates their supplementary nature.\\nFurther, discourse results show the lowest correlation with others (44.8± 16.1), indicating the particular scope.',\n",
       "   'title': 'ii) The story of Holmes',\n",
       "   'chunks': [{'text': 'We focus on what Holmes tells us in general and regarding formal and functional phenomena, as defined in § 2. We report in Figure 8 the task metric, discriminability, and selectivity, averaged for every phenomena type.\\nNote, discriminability (Rodriguez et al., 2021) quantifies the alignment of LMs ranking of one specific dataset compared to the overall rankings using the Kendall Tau correlation.\\nConsidering these three metrics, all tested LMs strongly encode formal phenomena (morphology and syntax), which often depend on the local neighborhood of words.\\nTherefore, we assume that LMs approximate these co-occurrences during pre-training with high precision.\\nFor example, the specific POS tag of a word, like man (noun), primarily depends on its surroundings, such as the frequent predecessor the.\\nIn contrast, LMs encode less information about functional phenomena (semantics, reasoning, and discourse) since they show a relatively low performance regarding the task metric.\\nFor these functional phenomena, we assume more complex co-occurrences are required to capture the broad context in language, such as the rhetorical relation of two distant text spans.\\nDespite these differences between formal and functional phenomena types, they contribute to the benchmark in a balanced way.\\nA low to medium discriminability indicates that none of these types of linguistic phenomena dominates the overall LM rankings.',\n",
       "     'title': 'Preliminaries > ii) The story of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'ii) The story of Holmes'},\n",
       "    {'text': 'This balanced influence of the five phenomena types is further visible when considering their ranking correlations (Figure 9, left).\\nA high average correlation of 67.8 ± 6.6 with the overall results',\n",
       "     'title': 'Preliminaries > ii) The story of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'ii) The story of Holmes'},\n",
       "    {'text': 'Figure 8: Average task metric, difficulty, and discriminability for each phenomena type.\\nThe dashed lines show the average measure over all datasets.',\n",
       "     'title': 'Preliminaries > ii) The story of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'ii) The story of Holmes'},\n",
       "    {'text': '(last column/row) hints that they are facets of a broader occurrence but share common characteristics.\\nStill, breaking into categories is meaningful, as the phenomena types (first five columns/rows) are medium correlated (average of 53.9 ± 14.5).\\nAnalyzing the results of phenomena types further highlights the value of this distinction.\\nWhile results of morphology and syntax are similarly correlated with the overall results (68.2 and 70.2), their direct correlation (69.1) indicates their supplementary nature.\\nFurther, discourse results show the lowest correlation with others (44.8± 16.1), indicating the particular scope.',\n",
       "     'title': 'Preliminaries > ii) The story of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'ii) The story of Holmes'}],\n",
       "   'tables': []},\n",
       "  {'text': 'iii) The companions of Holmes\\nWe analyze how the results of Holmes and those from other evaluations focusing on downstream applications align (Figure 9, right).\\nWe select the OpenLLM benchmark (Beeching et al., 2023), as it covers a wide range of open LMs, in contrast to others like HELM (Liang et al., 2023).\\nFirst, Holmes and OpenLLM results of jointly evaluated LMs are medium correlated, hinting that the linguistic competence of LMs is partly aligned with their downstream abilities.\\nThe nature of this alignment is further evident when focusing on morphology, reasoning, and discourse.\\nInterestingly, and in contrast to syntax and semantics, their correlation to the OpenLLM and Holmes overall results is similar.\\nTherefore, these three phenomena presumably represent skills that are more tested in the general benchmarks.\\nThese correlation patterns are consistent across the three most meaningful OpenLLM datasets (MMLU, TruthfulQA, and GSM8K).\\nAs TruthfulQA shows lower correlations with the linguistic phenomena and other datasets within OpenLLM, we presume this dataset captures distinctly different skills (possibly knowledge).3 These insights show how different benchmarks provide a different scope and supplement themselves simulta- 3Further, it’s also known that we need to expect this dataset to be fully leaked (Balloccu et al., 2024).\\nFigure 9: Kendall-tau correlation within Holmes (left) and compared to the OpenLLM benchmark (right).\\nGreen stars indicate significant correlations (p < 0.05).\\nneously.\\nFurther, the above analysis shows, again, the value of assessing the linguistic competence of LMs across different phenomena types, for finegrained analyses.\\niv) The effect of language model architecture.\\nNext, we discuss the impact of model architecture on the linguistic competence of LMs.\\nIn Figure 11 (left), we compare encoder and decoder LMs.\\nDue to the absence of big encoder LMs, we consider five encoder and six decoder LMs with up to 220m parameters.\\nEncoder LMs show a higher mwr of 52% than decoder LMs (21%).\\nThis observation is the most saturated for morphology or syntax, encompassing a variety of token-level phenomena, like part-of-speech.\\nWe assume that the missing bi-directional encoding of decoder LMs causes this lower performance because the available context of one token heavily depends on its position.\\nThus, even common tokens, like the, have different potential representations - at the beginning or in the middle of a sentence.\\nThese instabilities are further evident when considering Figure 11 (right) which reports the accuracy for the top-20 most common POS tokens (such as the) based on the pos, xpos, upos dataset.\\nGiven their high frequency, one expects stable prediction performance.\\nSurprisingly, encoder LMs (BERT and RoBERTa) show higher median accuracy and clearly lower deviations compared to the same-size decoder counterpart (GPT2).\\nWhile scaling model size to 12B (Pythia) and 70B (Llama-2) allows for improved accuracy and lower deviations, decoder LMs do not match the encoder performance, even up to 700 times bigger.\\nv) The effect of scaling parameters.\\nWe discuss how the number of parameters influences the linguistic competence of LMs.\\nGiven the variety of LMs of different sizes, we focus on the Pythia (decoder-only) and T5 (encoder-decoder) families.\\nFrom Figure 10, we observe for both Pythia and T5 that the linguistic competence scales with model size, and it is particularly pronounced after exceeding 0.5B (Pythia) and 1.0B (T5) parameters.\\nAgain, model architecture is crucial, as T5 LMs (encoderdecoder) exhibit a clearly higher mean winning rate of 40− 70% than Pythia (decoder-only) ones with mwr of 20− 60%.\\nFurther, we found formal phenomena evolving differently with increased model size than functional ones.\\nSpecifically, morphology and syntax start at a lower level, with an apparent performance jump after 0.5B (Pythia) and 1.0B (T5) parameters, followed by slow but steady growth.\\nDifferently, semantics, reasoning, and discourse start at a higher mwr, followed by a continuous improvement as the model size grows.\\nFrom these results, we assume more parameters allow LMs to better approximate simpler co-occurrences in the near neighborhood of words to understand formal phenomena like word dependencies.\\nIn contrast, more parameters do not have the same pronounced effect on functional phenomena, like rhetorical relations, which require an LM to acquire more distant and complex word co-occurrences.\\n | Model | Morphology | Syntax | Semantics | Reasoning | Discourse | Overall\\n | --- | --- | --- | --- | --- | --- | ---\\n | Llama-2-Chat | Comparison against Llama-2 -8% | +3% | with 7 billion parameters -5% | -9% | -3% | -2%\\n | FLAN-T5 | Comparison against T5 +10% | +2% | with 11 billion parameters -2% | +6% | -2% | +1%\\n | Dolly-v2 | Comparison against Pythia +4% | -3% | with 12 billion parameters -9% | -3% | +4% | -4%\\n | Tülu-2 | Comparison against Llama-2 +5% | +2% | with 13 billion parameters -15% | 0% | -30% | -8%\\n | Orca-2 | -1% | -3% | -4% | +4% | -5% | -2%\\n | Llama-2-chat | +3% | +1% | -6% | +3% | -1% | -1%\\n | Vicuna-v1.5 | +23% | +7% | -3% | +6% | -6% | +4%\\n | FLAN-UL2 | Comparison +40% | against UL2 +16% | with 20 billion +7% | parameters +13% | +1% | +13%\\n | Mixtral-Instruct | Comparison against Mixtral +4% | +3% | with ~47 billion parameters 0% | +6% | -2% | +2%\\n | Tülu-2 | Comparison against Llama-2 +15% | 0% | with 70 billion parameters -11% | -3% | 0% | -2%\\n | Llama-2-Chat +23% +14% +2% +4% +17% +10%\\n | Average | +10% | +4% | -3% | +4% | -2% | +1%\\n\\nTable 1: Effect of instruction tuning on the mean winning rate compared to the pre-trained LMs.\\nvi) The effect of instruction tuning.\\nFinally, we focus on how instruction tuning affects LMs’ linguistic competence and compare the tuned LMs with their base models—for example, FLAN-UL2 vs. UL2.\\nFrom results in Table 1, we note less saturated effects for the overall scope while being more pronounced for the five phenomenon types again emphasizing the structured and comprehensive evaluation of linguistic competence.\\nOn average, we found instruction tuning has the highest\\nFigure 10: Effect of scaling LM parameters considering the T5 and Pythia model families providing eight and five different sizes.\\nWe address the overall scope (left) and the different types of linguistic phenomena (right).\\nFigure 11: Comparison of the phenomenon types for encoder and decoder LMs (left) and on the right, the accuracy of the top-20 most common tokens of the three part-of-speech probing datasets for BERT, RoBERTa, GPT2, Pythia, and Llama-2.\\neffect on morphology (+10%) followed by syntax (+4%), reasoning (+4%), and a negative effect for semantics −3% and discourse −2%.\\nThese results confirm previous assumptions that instruction tuning updates are often superficial (Yadav et al., 2023; Hershcovitch et al., 2024; Sharma et al., 2023) and that LMs are better at mimicking language (formal phenomena) than understanding it, measured with functional phenomena (Mahowald et al., 2024).\\nFurther, larger models benefit more from instruction tuning.\\nLlama-2-70b-Chat and FLANUL2 gain up to +23% and +40% for morphology and +10% and +13% on average.\\nIn addition, decoder-only LMs (Llama-2 and Pythia) tend to show less pronounced positive effects than encoderdecoder LMs (FLAN-T5-XXL and FLAN-UL2).\\nHowever, they better understand reasoning phenomena.\\nWhen comparing LMs based on Llama-213b, we see that specific fine-tuning methods shape the LMs differently.\\nThe top-ranked 13b LM for Holmes and OpenLLM, Vicuna, was trained on 125k instructions, less than other models.\\nThus, high quality is more important than the number of instructions for LMs’ linguistic competence.\\nTülu loses performance while being trained on a large mixture of data (approx. 330k instructions), the same for its 70b version.\\nFinally, the focus of Orca2 on reasoning is also reflected in its embedding space.\\nThese insights show again that while providing a particular perspective, Holmes shows clear differences between LMs and allows us to map them to methodological decisions.\\n6 Efficiency\\neasy, cost-effective integration of new LMs is crucial for widely adopting a benchmark.\\nAs Holmes covers many datasets and examples, it is computationally heavy in encoding text and training the probes.\\nIt takes approx.\\n6 GPU days to encode the 70 million tokens (∼230k pages of text) and 2 days to run the 208 probes for a 70b model.\\nTo account for this issue, we introduce FlashHolmes, a streamlined version of Holmes.\\nIt allows the evaluation of new LMs with a fraction of the compute while maintaining evaluation integrity.\\nBesides excluding licensed data (18 probing datasets), we analyze the effect of discarding training instances.\\nAs a result, we reduce the computation for encoding and the actual probing simultaneously.\\nWe follow Perlitz et al.\\n(2023) and calculate the rank resolution, 95% CI of model rank difference.\\nThis measure indicates the maximum expected rank deviation from evaluating an LM on FlashHolmes compared to Holmes.\\nFor example, a rank resolution of one means that an LM evaluated on FlashHolmes and Holmes has the same rank or switch place with its neighbors with a probability of 95%.\\nFigure 12 shows the resulting rank resolution when training only on a fraction of the instances, from 1/2 to 1/512.\\nSolely focusing on efficiency (1/512) still provides a decent rank resolution of ~2.7.\\nIn contrast, considering 1/2 of the training data results in the best reliability of ~1.0.\\nTo balance benchmark reliability and efficiency, we compose FlashHolmes using 1/32 of the training instances.\\nPrecisely, it reduces the computation expenses of evaluating LMs to ~3% of what Holmes would have required while pre- serving a high rank-correlation of ~1.3.\\nFigure 12: Analysis of the reliability vs. efficiency trade-off when reducing the number of training data.\\n7 Related Work\\nBenchmarking LMs Benchmarks approximate LMs abilities like general language understanding (Wang et al., 2019b,a), out-of-distribution generalization (Yang et al., 2023; Waldis et al., 2024b), adversarial scenarios (Nie et al., 2020; Wang et al., 2021), or retrieval like BEIR (Thakur et al., 2021) or MTEB (Muennighoff et al., 2023).\\nWith the advent of larger LMs, the methodological focus shifted to prompting-based evaluations which evaluate the LMs’ response to provided instructions (Brown et al., 2020; Hendrycks et al., 2021; Srivastava et al., 2022) covering application-oriented tasks (Liang et al., 2023), or mathematical reasoning (e.g., GSM8K (Cobbe et al., 2021)).\\nAssessing the Linguistic Competence of LMs The analysis of LMs’ linguistic competence ranges from analyzing static word vectors (Köhn, 2015), sentence embeddings (Conneau et al., 2018; Adi et al., 2017), the internals of translation models (Shi et al., 2016; Bau et al., 2019), or contextualized LMs (Tenney et al., 2019b,a; Hewitt and Manning, 2019).\\nOther work addressed methodological aspects, such as using control tasks (Hewitt and Liang, 2019), assessing LMs from an information theory perspective (Voita and Titov, 2020;\\nPimentel et al., 2020), or evaluating causal effects in LMs (Elazar et al., 2021).\\nFinally, another line of work focuses on whether LMs follow human understanding of linguistic competence when solving downstream tasks (Belinkov, 2022; Aw et al., 2023; Mahowald et al., 2024).\\nHowever, Mosbach et al.\\n(2020b) and Waldis et al.\\n(2024a) found finetuning for downstream tasks actually hurting the understanding of linguistic phenomena.\\nWhile prior studies assessing the linguistic competence of LMs tend to focus on a limited set of linguistic phenomena or models, Holmes provides extensive coverage of both phenomena and eval- uated LMs.\\nUnlike recent evaluations based on prompting methods (Blevins et al., 2023; Liang et al., 2023; Amouyal et al., 2024), Holmes assesses the internal representations of LMs directly.\\nThis approach allows for detailed analysis of specific model characteristics, such as architecture, and helps separate the linguistic competence from other cognitive abilities.\\nThereby, we respond to recent calls for a thorough and explicit evaluation of linguistic phenomena (Hu and Levy, 2023; Lu et al., 2023; Mahowald et al., 2024).\\n8 Conclusion\\nmarks the most up-to-date and extensive consolidation of existing resources addressing the need to assess the linguistic competence of LMs in isolation.\\nOur experiments demonstrate that LMs’ linguistic competence is pronounced regarding formal phenomena but lacks functional ones when information about broader textual contexts, such as rhetorical structure, is required.\\nFurther, size, architecture, and instruction tuning crucially account for differences among LMs.\\nAs LM and resources in the landscape of linguistics continue to grow, we will actively extend Holmes with further probing datasets, evaluate upcoming LMs, and plan to incorporate multilingualism.',\n",
       "   'title': 'iii) The companions of Holmes',\n",
       "   'chunks': [{'text': 'We analyze how the results of Holmes and those from other evaluations focusing on downstream applications align (Figure 9, right).\\nWe select the OpenLLM benchmark (Beeching et al., 2023), as it covers a wide range of open LMs, in contrast to others like HELM (Liang et al., 2023).\\nFirst, Holmes and OpenLLM results of jointly evaluated LMs are medium correlated, hinting that the linguistic competence of LMs is partly aligned with their downstream abilities.\\nThe nature of this alignment is further evident when focusing on morphology, reasoning, and discourse.\\nInterestingly, and in contrast to syntax and semantics, their correlation to the OpenLLM and Holmes overall results is similar.\\nTherefore, these three phenomena presumably represent skills that are more tested in the general benchmarks.\\nThese correlation patterns are consistent across the three most meaningful OpenLLM datasets (MMLU, TruthfulQA, and GSM8K).\\nAs TruthfulQA shows lower correlations with the linguistic phenomena and other datasets within OpenLLM, we presume this dataset captures distinctly different skills (possibly knowledge).3 These insights show how different benchmarks provide a different scope and supplement themselves simulta- 3Further, it’s also known that we need to expect this dataset to be fully leaked (Balloccu et al., 2024).',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 6,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'Figure 9: Kendall-tau correlation within Holmes (left) and compared to the OpenLLM benchmark (right).\\nGreen stars indicate significant correlations (p < 0.05).\\nneously.\\nFurther, the above analysis shows, again, the value of assessing the linguistic competence of LMs across different phenomena types, for finegrained analyses.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'iv) The effect of language model architecture.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'Next, we discuss the impact of model architecture on the linguistic competence of LMs.\\nIn Figure 11 (left), we compare encoder and decoder LMs.\\nDue to the absence of big encoder LMs, we consider five encoder and six decoder LMs with up to 220m parameters.\\nEncoder LMs show a higher mwr of 52% than decoder LMs (21%).\\nThis observation is the most saturated for morphology or syntax, encompassing a variety of token-level phenomena, like part-of-speech.\\nWe assume that the missing bi-directional encoding of decoder LMs causes this lower performance because the available context of one token heavily depends on its position.\\nThus, even common tokens, like the, have different potential representations - at the beginning or in the middle of a sentence.\\nThese instabilities are further evident when considering Figure 11 (right) which reports the accuracy for the top-20 most common POS tokens (such as the) based on the pos, xpos, upos dataset.\\nGiven their high frequency, one expects stable prediction performance.\\nSurprisingly, encoder LMs (BERT and RoBERTa) show higher median accuracy and clearly lower deviations compared to the same-size decoder counterpart (GPT2).\\nWhile scaling model size to 12B (Pythia) and 70B (Llama-2) allows for improved accuracy and lower deviations, decoder LMs do not match the encoder performance, even up to 700 times bigger.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'From Figure 10, we observe for both Pythia and T5 that the linguistic competence scales with model size, and it is particularly pronounced after exceeding 0.5B (Pythia) and 1.0B (T5) parameters.\\nAgain, model architecture is crucial, as T5 LMs (encoderdecoder) exhibit a clearly higher mean winning rate of 40− 70% than Pythia (decoder-only) ones with mwr of 20− 60%.\\nFurther, we found formal phenomena evolving differently with increased model size than functional ones.\\nSpecifically, morphology and syntax start at a lower level, with an apparent performance jump after 0.5B (Pythia) and 1.0B (T5) parameters, followed by slow but steady growth.\\nDifferently, semantics, reasoning, and discourse start at a higher mwr, followed by a continuous improvement as the model size grows.\\nFrom these results, we assume more parameters allow LMs to better approximate simpler co-occurrences in the near neighborhood of words to understand formal phenomena like word dependencies.\\nIn contrast, more parameters do not have the same pronounced effect on functional phenomena, like rhetorical relations, which require an LM to acquire more distant and complex word co-occurrences.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': ' | Model | Morphology | Syntax | Semantics | Reasoning | Discourse | Overall\\n | --- | --- | --- | --- | --- | --- | ---\\n | Llama-2-Chat | Comparison against Llama-2 -8% | +3% | with 7 billion parameters -5% | -9% | -3% | -2%\\n | FLAN-T5 | Comparison against T5 +10% | +2% | with 11 billion parameters -2% | +6% | -2% | +1%\\n | Dolly-v2 | Comparison against Pythia +4% | -3% | with 12 billion parameters -9% | -3% | +4% | -4%\\n | Tülu-2 | Comparison against Llama-2 +5% | +2% | with 13 billion parameters -15% | 0% | -30% | -8%\\n | Orca-2 | -1% | -3% | -4% | +4% | -5% | -2%\\n | Llama-2-chat | +3% | +1% | -6% | +3% | -1% | -1%\\n | Vicuna-v1.5 | +23% | +7% | -3% | +6% | -6% | +4%\\n | FLAN-UL2 | Comparison +40% | against UL2 +16% | with 20 billion +7% | parameters +13% | +1% | +13%\\n | Mixtral-Instruct | Comparison against Mixtral +4% | +3% | with ~47 billion parameters 0% | +6% | -2% | +2%\\n | Tülu-2 | Comparison against Llama-2 +15% | 0% | with 70 billion parameters -11% | -3% | 0% | -2%\\n | Llama-2-Chat +23% +14% +2% +4% +17% +10%\\n | Average | +10% | +4% | -3% | +4% | -2% | +1%\\n',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'Table 1: Effect of instruction tuning on the mean winning rate compared to the pre-trained LMs.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'vi) The effect of instruction tuning.\\nFinally, we focus on how instruction tuning affects LMs’ linguistic competence and compare the tuned LMs with their base models—for example, FLAN-UL2 vs. UL2.\\nFrom results in Table 1, we note less saturated effects for the overall scope while being more pronounced for the five phenomenon types again emphasizing the structured and comprehensive evaluation of linguistic competence.\\nOn average, we found instruction tuning has the highest',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 7,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'Figure 10: Effect of scaling LM parameters considering the T5 and Pythia model families providing eight and five different sizes.\\nWe address the overall scope (left) and the different types of linguistic phenomena (right).',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'Figure 11: Comparison of the phenomenon types for encoder and decoder LMs (left) and on the right, the accuracy of the top-20 most common tokens of the three part-of-speech probing datasets for BERT, RoBERTa, GPT2, Pythia, and Llama-2.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'effect on morphology (+10%) followed by syntax (+4%), reasoning (+4%), and a negative effect for semantics −3% and discourse −2%.\\nThese results confirm previous assumptions that instruction tuning updates are often superficial (Yadav et al., 2023; Hershcovitch et al., 2024; Sharma et al., 2023) and that LMs are better at mimicking language (formal phenomena) than understanding it, measured with functional phenomena (Mahowald et al., 2024).\\nFurther, larger models benefit more from instruction tuning.\\nLlama-2-70b-Chat and FLANUL2 gain up to +23% and +40% for morphology and +10% and +13% on average.\\nIn addition, decoder-only LMs (Llama-2 and Pythia) tend to show less pronounced positive effects than encoderdecoder LMs (FLAN-T5-XXL and FLAN-UL2).\\nHowever, they better understand reasoning phenomena.\\nWhen comparing LMs based on Llama-213b, we see that specific fine-tuning methods shape the LMs differently.\\nThe top-ranked 13b LM for Holmes and OpenLLM, Vicuna, was trained on 125k instructions, less than other models.\\nThus, high quality is more important than the number of instructions for LMs’ linguistic competence.\\nTülu loses performance while being trained on a large mixture of data (approx. 330k instructions), the same for its 70b version.\\nFinally, the focus of Orca2 on reasoning is also reflected in its embedding space.\\nThese insights show again that while providing a particular perspective, Holmes shows clear differences between LMs and allows us to map them to methodological decisions.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': '6 Efficiency',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'easy, cost-effective integration of new LMs is crucial for widely adopting a benchmark.\\nAs Holmes covers many datasets and examples, it is computationally heavy in encoding text and training the probes.\\nIt takes approx.\\n6 GPU days to encode the 70 million tokens (∼230k pages of text) and 2 days to run the 208 probes for a 70b model.\\nTo account for this issue, we introduce FlashHolmes, a streamlined version of Holmes.\\nIt allows the evaluation of new LMs with a fraction of the compute while maintaining evaluation integrity.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'Besides excluding licensed data (18 probing datasets), we analyze the effect of discarding training instances.\\nAs a result, we reduce the computation for encoding and the actual probing simultaneously.\\nWe follow Perlitz et al.\\n(2023) and calculate the rank resolution, 95% CI of model rank difference.\\nThis measure indicates the maximum expected rank deviation from evaluating an LM on FlashHolmes compared to Holmes.\\nFor example, a rank resolution of one means that an LM evaluated on FlashHolmes and Holmes has the same rank or switch place with its neighbors with a probability of 95%.\\nFigure 12 shows the resulting rank resolution when training only on a fraction of the instances, from 1/2 to 1/512.\\nSolely focusing on efficiency (1/512) still provides a decent rank resolution of ~2.7.\\nIn contrast, considering 1/2 of the training data results in the best reliability of ~1.0.\\nTo balance benchmark reliability and efficiency, we compose FlashHolmes using 1/32 of the training instances.\\nPrecisely, it reduces the computation expenses of evaluating LMs to ~3% of what Holmes would have required while pre- serving a high rank-correlation of ~1.3.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 8,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'Figure 12: Analysis of the reliability vs. efficiency trade-off when reducing the number of training data.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': '7 Related Work',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'Benchmarking LMs Benchmarks approximate LMs abilities like general language understanding (Wang et al., 2019b,a), out-of-distribution generalization (Yang et al., 2023; Waldis et al., 2024b), adversarial scenarios (Nie et al., 2020; Wang et al., 2021), or retrieval like BEIR (Thakur et al., 2021) or MTEB (Muennighoff et al., 2023).\\nWith the advent of larger LMs, the methodological focus shifted to prompting-based evaluations which evaluate the LMs’ response to provided instructions (Brown et al., 2020; Hendrycks et al., 2021; Srivastava et al., 2022) covering application-oriented tasks (Liang et al., 2023), or mathematical reasoning (e.g., GSM8K (Cobbe et al., 2021)).',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'Assessing the Linguistic Competence of LMs The analysis of LMs’ linguistic competence ranges from analyzing static word vectors (Köhn, 2015), sentence embeddings (Conneau et al., 2018; Adi et al., 2017), the internals of translation models (Shi et al., 2016; Bau et al., 2019), or contextualized LMs (Tenney et al., 2019b,a; Hewitt and Manning, 2019).\\nOther work addressed methodological aspects, such as using control tasks (Hewitt and Liang, 2019), assessing LMs from an information theory perspective (Voita and Titov, 2020;',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'Pimentel et al., 2020), or evaluating causal effects in LMs (Elazar et al., 2021).\\nFinally, another line of work focuses on whether LMs follow human understanding of linguistic competence when solving downstream tasks (Belinkov, 2022; Aw et al., 2023; Mahowald et al., 2024).\\nHowever, Mosbach et al.\\n(2020b) and Waldis et al.\\n(2024a) found finetuning for downstream tasks actually hurting the understanding of linguistic phenomena.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'While prior studies assessing the linguistic competence of LMs tend to focus on a limited set of linguistic phenomena or models, Holmes provides extensive coverage of both phenomena and eval- uated LMs.\\nUnlike recent evaluations based on prompting methods (Blevins et al., 2023; Liang et al., 2023; Amouyal et al., 2024), Holmes assesses the internal representations of LMs directly.\\nThis approach allows for detailed analysis of specific model characteristics, such as architecture, and helps separate the linguistic competence from other cognitive abilities.\\nThereby, we respond to recent calls for a thorough and explicit evaluation of linguistic phenomena (Hu and Levy, 2023; Lu et al., 2023; Mahowald et al., 2024).',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': '8 Conclusion',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'iii) The companions of Holmes'},\n",
       "    {'text': 'marks the most up-to-date and extensive consolidation of existing resources addressing the need to assess the linguistic competence of LMs in isolation.\\nOur experiments demonstrate that LMs’ linguistic competence is pronounced regarding formal phenomena but lacks functional ones when information about broader textual contexts, such as rhetorical structure, is required.\\nFurther, size, architecture, and instruction tuning crucially account for differences among LMs.\\nAs LM and resources in the landscape of linguistics continue to grow, we will actively extend Holmes with further probing datasets, evaluate upcoming LMs, and plan to incorporate multilingualism.',\n",
       "     'title': 'Preliminaries > iii) The companions of Holmes',\n",
       "     'page': 9,\n",
       "     'source_doc': 'iii) The companions of Holmes'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff3041a1c60>]},\n",
       "  {'text': 'Ethical Considerations and Limitations\\nLanguage Holmes as well as FlashHolmes solely assess linguistic phenomena for the English language.\\nAs we plan to expand the benchmark and scope of multilingual data, we focus momentarily on English because of the widespread availability of resources, including curated corpora and the diversity of available LMs.\\nPhenomena and LM Coverage We agree with Liang et al.\\n(2023) and see one fundamental aspect in composing a benchmark in acknowledging its incompleteness.\\nBoth linguistic phenomena and LMs are a subset of the variety of available resources.\\nWe consolidated them carefully to provide a comprehensive scope of the linguistic competence and various LMs.\\nHowever, as benchmarks evolve as tools to assess LMs, we will further expand Holmes both with the existing and upcoming LMs and data resources.\\nData Availability Linguistic annotations, in particular more complex ones targeting phenomena like discourse, are money and time-wise expensive.\\nOut of 208 datasets included in Holmes, 18 probing datasets are based on licensed resources and are not freely available.\\nHowever, with FlashHolmes, we provide an effective and efficient alternative based on open-access resources.\\nFurthermore, upon confirming the granted access, we are happy to share our probing datasets, including those based on the licensed resources.\\nBias As Holmes relies on existing resources, it inherits the bias embodied in this data.\\nExamples of such bias are gender equality or gender fairness, like the use of neo pronouns such as em in Lauscher et al.\\n(2023).',\n",
       "   'title': 'Ethical Considerations and Limitations',\n",
       "   'chunks': [{'text': 'Language Holmes as well as FlashHolmes solely assess linguistic phenomena for the English language.\\nAs we plan to expand the benchmark and scope of multilingual data, we focus momentarily on English because of the widespread availability of resources, including curated corpora and the diversity of available LMs.',\n",
       "     'title': 'Ethical Considerations and Limitations',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Ethical Considerations and Limitations'},\n",
       "    {'text': 'Phenomena and LM Coverage We agree with Liang et al.\\n(2023) and see one fundamental aspect in composing a benchmark in acknowledging its incompleteness.\\nBoth linguistic phenomena and LMs are a subset of the variety of available resources.\\nWe consolidated them carefully to provide a comprehensive scope of the linguistic competence and various LMs.\\nHowever, as benchmarks evolve as tools to assess LMs, we will further expand Holmes both with the existing and upcoming LMs and data resources.',\n",
       "     'title': 'Ethical Considerations and Limitations',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Ethical Considerations and Limitations'},\n",
       "    {'text': 'Data Availability Linguistic annotations, in particular more complex ones targeting phenomena like discourse, are money and time-wise expensive.\\nOut of 208 datasets included in Holmes, 18 probing datasets are based on licensed resources and are not freely available.\\nHowever, with FlashHolmes, we provide an effective and efficient alternative based on open-access resources.\\nFurthermore, upon confirming the granted access, we are happy to share our probing datasets, including those based on the licensed resources.',\n",
       "     'title': 'Ethical Considerations and Limitations',\n",
       "     'page': 9,\n",
       "     'source_doc': 'Ethical Considerations and Limitations'},\n",
       "    {'text': 'Bias As Holmes relies on existing resources, it inherits the bias embodied in this data.\\nExamples of such bias are gender equality or gender fairness, like the use of neo pronouns such as em in Lauscher et al.\\n(2023).',\n",
       "     'title': 'Ethical Considerations and Limitations',\n",
       "     'page': 10,\n",
       "     'source_doc': 'Ethical Considerations and Limitations'}],\n",
       "   'tables': []},\n",
       "  {'text': 'References\\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg.\\n2017.\\nFine-grained analysis of sentence embeddings using auxiliary prediction tasks.\\nIn 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\\nOpenReview.net.\\nEhsan Aghazadeh, Mohsen Fayyaz, and Yadollah Yaghoobzadeh.\\n2022.\\nMetaphors in pre-trained language models: Probing and generalization across datasets and languages.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2037– 2050, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nSamuel Amouyal, Aya Meltzer-Asscher, and Jonathan Berant.\\n2024.\\nLarge language models for psycholinguistic plausibility pretesting.\\nIn Findings of the Association for Computational Linguistics: EACL 2024, pages 166–181, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.\\nKhai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, and Antoine Bosselut.\\n2023.\\nInstruction-tuning aligns llms to the human brain.\\nCoRR, abs/2312.00575.\\nSimone Balloccu, Patrícia Schmidtová, Mateusz Lango, and Ondrej Dusek.\\n2024.\\nLeak, cheat, repeat: Data contamination and evaluation malpractices in closedsource LLMs.\\nIn Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 67–93, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.\\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James R. Glass.\\n2019.\\nIdentifying and controlling important neurons in neural machine translation.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nEdward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.\\n2023.\\nOpen llm leaderboard.\\nhttps://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard.\\nYonatan Belinkov.\\n2022.\\nProbing classifiers: Promises, shortcomings, and advances.\\nComputational Linguistics, 48(1):207–219.\\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass.\\n2017.\\nWhat do neural machine translation models learn about morphology?\\nIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861–872, Vancouver, Canada.\\nAssociation for Computational Linguistics.\\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal.\\n2023.\\nPythia: A suite for analyzing large language models across training and scaling.\\nIn International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2397–2430.\\nPMLR.\\nJulia Birke and Anoop Sarkar.\\n2006.\\nA clustering approach for nearly unsupervised recognition of nonliteral language.\\nIn 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 329–336, Trento, Italy.\\nAssociation for Computational Linguistics.\\nTerra Blevins, Hila Gonen, and Luke Zettlemoyer.\\n2023.\\nPrompting language models for linguistic structure.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6649–6663, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nNikolay Bogoychev and Adam Lopez.\\n2016.\\nN-gram language models for massively parallel devices.\\nIn\\nProceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 1944–1953, Berlin, Germany.\\nAssociation for Computational Linguistics.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\\n2020.\\nLanguage models are few-shot learners.\\nIn Advances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nLynn Carlson, Daniel Marcu, and Mary Ellen Okurovsky.\\n2001.\\nBuilding a discourse-tagged corpus in the framework of rhetorical structure theory.\\nIn Proceedings of the SIGDIAL 2001 Workshop, The 2nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, Saturday, September 1, 2001 to Sunday, September 2, 2001, Aalborg, Denmark.\\nThe Association for Computer Linguistics.\\nNoam Chomsky.\\n1965.\\nAspects of the Theory of Syntax.\\nThe MIT Press, Cambridge.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\\n2022.\\nScaling instruction-finetuned language models.\\nCoRR, abs/2210.11416.\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.\\n2020.\\nELECTRA: pretraining text encoders as discriminators rather than generators.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\\n2021.\\nTraining verifiers to solve math word problems.\\nCoRR, abs/2110.14168.\\nAlexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni.\\n2018.\\nWhat you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.\\nIn\\nProceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 2126–2136, Melbourne, Australia.\\nAssociation for Computational Linguistics.\\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.\\n2023.\\nFree dolly: Introducing the world’s first truly open instructiontuned llm.\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei.\\n2009.\\nImagenet: A large-scale hierarchical image database.\\n2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\n2019.\\nBERT: Pre-training of deep bidirectional transformers for language under- standing.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.\\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg.\\n2021.\\nAmnesic probing: Behavioral explanation with amnesic counterfactuals.\\nTransactions of the Association for Computational Linguistics, 9:160– 175.\\nRudolf Franz Flesch.\\n1948.\\nA new readability yardstick.\\nThe Journal of applied psychology, 32(3):221–233.\\nWilliam Gantt, Lelia Glass, and Aaron Steven White.\\n2022.\\nDecomposing and recomposing event structure.\\nTransactions of the Association for Computational Linguistics, 10:17–34.\\nVagrant Gautam, Eileen Bingert, D. Zhu, Anne Lauscher, and Dietrich Klakow.\\n2024.\\nRobust pronoun use fidelity with english llms: Are they reasoning, repeating, or just biased?\\nCoRR, abs/2404.03134.\\nMario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema.\\n2018.\\nUnder the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 240–248, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nVenkata Govindarajan, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nDecomposing generalization: Models of generic, habitual, and episodic statements.\\nTransactions of the Association for Computational Linguistics, 7:501–517.\\nZellig S Harris.\\n1954.\\nDistributional structure.\\nWord, 10(2-3):146–162.\\nPengcheng He, Jianfeng Gao, and Weizhu Chen.\\n2023.\\nDebertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.\\n2021.\\nDeberta: decoding-enhanced bert with disentangled attention.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.\\n2010.\\nSemEval-2010 task 8: Multiway classification of semantic relations between pairs of nominals.\\nIn Proceedings of the 5th International\\nWorkshop on Semantic Evaluation, pages 33–38, Uppsala, Sweden.\\nAssociation for Computational Linguistics.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021.\\nMeasuring massive multitask language understanding.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nMoshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, Swaminathan Sundararaman, and Danny Harnik.\\n2024.\\nLossless and nearlossless compression for foundation models.\\nCoRR, abs/2404.15198.\\nJohn Hewitt and Percy Liang.\\n2019.\\nDesigning and interpreting probes with control tasks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–2743, Hong Kong, China.\\nAssociation for Computational Linguistics.\\nJohn Hewitt and Christopher D. Manning.\\n2019.\\nA structural probe for finding syntax in word representations.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.\\nYufang Hou.\\n2018.\\nEnhanced word representations for bridging anaphora resolution.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 1–7, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.\\nYufang Hou.\\n2020.\\nBridging anaphora resolution as question answering.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1428–1438, Online.\\nAssociation for Computational Linguistics.\\nJennifer Hu and Roger Levy.\\n2023.\\nPrompting is not a substitute for probability measurements in large language models.\\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5040–5060, Singapore.\\nAssociation for Computational Linguistics.\\nPhilip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth.\\n2021.\\nBabyBERTa: Learning more grammar with small-scale child-directed language.\\nIn Proceedings of the 25th Conference on Computational Natural Language Learning, pages 624–646, Online.\\nAssociation for Computational Linguistics.\\nAlexander Immer, Lucas Torroba Hennigen, Vincent Fortuin, and Ryan Cotterell.\\n2022.\\nProbing as quantifying inductive bias.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1839– 1851, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2023.\\nMistral 7b.\\nCoRR, abs/2310.06825.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2024.\\nMixtral of experts.\\nCoRR, abs/2401.04088.\\nJosef Klafka and Allyson Ettinger.\\n2020.\\nSpying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4801–4811, Online.\\nAssociation for Computational Linguistics.\\nArne Köhn.\\n2015.\\nWhat’s in an embedding?\\nanalyzing word embeddings through multilingual evaluation.\\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073, Lisbon, Portugal.\\nAssociation for Computational Linguistics.\\nNatalia Konstantinova, Sheila C.M.\\nde Sousa, Noa P. Cruz, Manuel J. Maña, Maite Taboada, and Ruslan Mitkov.\\n2012.\\nA review corpus annotated for negation, speculation and their scope.\\nIn Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 3190–3195, Istanbul, Turkey.\\nEuropean Language Resources Association (ELRA).\\nFajri Koto, Jey Han Lau, and Timothy Baldwin.\\n2021.\\nDiscourse probing of pretrained language models.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3849–3864, Online.\\nAssociation for Computational Linguistics.\\nKatarzyna Krasnowska-Kieraś and Alina Wróblewska.\\n2019.\\nEmpirical linguistic study of sentence embeddings.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5729–5739, Florence, Italy.\\nAssociation for Computational Linguistics.\\nMurathan Kurfalı and Robert Östling.\\n2021.\\nProbing multilingual language models for discourse.\\nIn\\nProceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 8–19, Online.\\nAssociation for Computational Linguistics.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\\n2020.\\nALBERT: A lite BERT for self-supervised learning of language representations.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.\\nAnne Lauscher, Debora Nozza, Ehm Miltersen, Archie Crowley, and Dirk Hovy.\\n2023.\\nWhat about “em”?\\nhow commercial machine translation fails to handle (neo-)pronouns.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 377–392, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020.\\nBART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online.\\nAssociation for Computational Linguistics.\\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana AcostaNavas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.\\n2023.\\nHolistic evaluation of language models.\\nTransactions on Machine Learning Research.\\nFeatured Certification, Expert Certification.\\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\\n2016.\\nAssessing the ability of LSTMs to learn syntaxsensitive dependencies.\\nTransactions of the Association for Computational Linguistics, 4:521–535.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019.\\nRoberta: A robustly optimized BERT pretraining approach.\\nCoRR, abs/1907.11692.\\nIlya Loshchilov and Frank Hutter.\\n2019.\\nDecoupled weight decay regularization.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenRe- view.net.\\nSheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych.\\n2023.\\nAre emergent abilities in large language models just in-context learning?\\nCoRR, abs/2309.01809.\\nKyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko.\\n2024.\\nDissociating language and thought in large language models.\\nTrends in Cognitive Sciences.\\nGeorge A. Miller.\\n1995.\\nWordnet: A lexical database for english.\\nCommunications of the ACM, 38(11):39– 41.\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.\\n2022.\\nRethinking the role of demonstrations:\\nWhat makes in-context learning work?\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048–11064, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nArindam Mitra, Luciano Del Corro, Shweti Mahajan, Andrés Codas, Clarisse Simões, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah.\\n2023.\\nOrca 2: Teaching small language models how to reason.\\nCoRR, abs/2311.11045.\\nMoran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky.\\n2024.\\nState of what art? A call for multi-prompt LLM evaluation.\\nCoRR, abs/2401.00595.\\nMichael Mohler, Mary Brunson, Bryan Rink, and Marc Tomlinson.\\n2016.\\nIntroducing the LCC metaphor datasets.\\nIn Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 4221–4227, Portorož, Slovenia.\\nEuropean Language Resources Association (ELRA).\\nRoser Morante and Eduardo Blanco.\\n2012.\\n*SEM 2012 shared task: Resolving the scope and focus of negation.\\nIn *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 265–274, Montréal, Canada.\\nAssociation for Computational Linguistics.\\nMarius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020a.\\nOn the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers.\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 68–82, Online.\\nAssociation for Computational Linguistics.\\nMarius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020b.\\nOn the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2502–2516, Online.\\nAssociation for Computational Linguistics.\\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers.\\n2023.\\nMTEB: Massive text embedding benchmark.\\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014–2037, Dubrovnik, Croatia.\\nAssociation for Computational Linguistics.\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\\n2018.\\nDon’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for extreme summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nAllen Nie, Erin Bennett, and Noah Goodman.\\n2019.\\nDisSent: Learning sentence representations from explicit discourse relations.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4497–4510, Florence, Italy.\\nAssociation for Computational Linguistics.\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\\n2020.\\nAdversarial NLI: A new benchmark for natural language understanding.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885–4901, Online.\\nAssociation for Computational Linguistics.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022.\\nTraining language models to follow instructions with human feedback.\\nIn Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.\\nGustavo Paetzold and Lucia Specia.\\n2016.\\nSemEval 2016 task 11: Complex word identification.\\nIn Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 560–569, San Diego, California.\\nAssociation for Computational Linguistics.\\nOnkar Pandit and Yufang Hou.\\n2021.\\nProbing for bridging inference in transformer language models.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4153–4163, Online.\\nAssociation for Computational Linguistics.\\nJeffrey Pennington, Richard Socher, and Christopher Manning.\\n2014.\\nGloVe: Global vectors for word representation.\\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar.\\nAssociation for Computational Linguistics.\\nYotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen.\\n2023.\\nEfficient benchmarking (of language models).\\nCoRR, abs/2308.11696.\\nFabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel.\\n2020.\\nHow context affects language models’ factual predictions.\\nIn Conference on Automated Knowledge Base Construction, AKBC 2020, Virtual, June 22-24, 2020.\\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller.\\n2019.\\nLanguage models as knowledge bases?\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2463–2473.\\nAssociation for Computational Linguistics.\\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell.\\n2020.\\nInformation-theoretic probing for linguistic structure.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4609–4622, Online.\\nAssociation for Computational Linguistics.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\n2019.\\nLanguage models are unsupervised multitask learners.\\nOpenAI blog, 1(8):9.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\\n2020.\\nExploring the limits of transfer learning with a unified text-to-text transformer.\\nJournal of Machine Learning Research, 21(140):1–67.\\nPedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, and Jordan BoydGraber.\\n2021.\\nEvaluation examples are not equally informative: How should that change NLP leaderboards?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4486–4503, Online.\\nAssociation for Computational Linguistics.\\nRachel Rudinger, Adam Teichert, Ryan Culkin, Sheng Zhang, and Benjamin Van Durme.\\n2018a.\\nNeuralDavidsonian semantic proto-role labeling.\\nIn Pro-\\nceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 944–955, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nRachel Rudinger, Aaron Steven White, and Benjamin Van Durme.\\n2018b.\\nNeural models of factuality.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 731–744, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.\\nNaomi Shapiro, Amandalynne Paullada, and Shane Steinert-Threlkeld.\\n2021.\\nA multilabel approach to morphosyntactic probing.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4486–4524, Punta Cana, Dominican Republic.\\nAssociation for Computational Linguistics.\\nPratyusha Sharma, Jordan T. Ash, and Dipendra Misra.\\n2023.\\nThe truth is in there: Improving reasoning in language models with layer-selective rank reduction.\\nCoRR, abs/2312.13558.\\nXing Shi, Inkit Padhi, and Kevin Knight.\\n2016.\\nDoes string-based neural MT learn source syntax?\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526– 1534, Austin, Texas.\\nAssociation for Computational Linguistics.\\nNatalia Silveira, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning.\\n2014.\\nA gold standard dependency corpus for English.\\nIn Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2897– 2904, Reykjavik, Iceland.\\nEuropean Language Resources Association (ELRA).\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts.\\n2013.\\nRecursive deep models for semantic compositionality over a sentiment treebank.\\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA.\\nAssociation for Computational Linguistics.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al.\\n2022.\\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.\\nCoRR, abs/2206.04615.\\nGerard J Steen, Aletta G Dorst, J Berenike Herrmann, Anna A Kaal, Tina Krennmayr, Tryntje Pasma, et al.\\n2010.\\nA method for linguistic metaphor identification.\\nConverging evidence in language and communication research.\\nJohn Benjamins Publishing Company Amsterdam.\\nShivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, and Akash Srivastava.\\n2024.\\nLAB: large-scale alignment for chatbots.\\nCoRR, abs/2403.01081.\\nGyörgy Szarvas, Veronika Vincze, Richárd Farkas, and János Csirik.\\n2008.\\nThe BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts.\\nIn Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 38–45, Columbus, Ohio.\\nAssociation for Computational Linguistics.\\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant.\\n2020. oLMpics-On What Language Model Pre-training Captures.\\nTransactions of the Association for Computational Linguistics, 8:743–758.\\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler.\\n2023.\\nUL2: unifying language learning paradigms.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.\\nIan Tenney, Dipanjan Das, and Ellie Pavlick.\\n2019a.\\nBERT rediscovers the classical NLP pipeline.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy.\\nAssociation for Computational Linguistics.\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick.\\n2019b.\\nWhat do you learn from context?\\nprobing for sentence structure in contextualized word representations.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych.\\n2021.\\nBEIR:\\nA heterogeneous benchmark for zero-shot evaluation of information retrieval models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\nLucas Torroba Hennigen, Adina Williams, and Ryan Cotterell.\\n2020.\\nIntrinsic probing through dimension selection.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 197–216, Online.\\nAssociation for Computational Linguistics.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\n2023.\\nLlama 2: Open foundation and finetuned chat models.\\nCoRR, abs/2307.09288.\\nTeemu Vahtola, Mathias Creutz, and Jörg Tiedemann.\\n2022.\\nIt is not easy to detect paraphrases: Analysing semantic similarity with antonyms and negation using the new SemAntoNeg benchmark.\\nIn Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 249–262, Abu Dhabi, United Arab Emirates (Hybrid).\\nAssociation for Computational Linguistics.\\nSiddharth Vashishtha, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nFine-grained temporal relation extraction.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2906–2919, Florence, Italy.\\nAssociation for Computational Linguistics.\\nSara Veldhoen, Dieuwke Hupkes, and Willem H. Zuidema.\\n2016.\\nDiagnostic classifiers revealing how neural networks process hierarchical structure.\\nIn Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings.\\nCEUR-WS.org.\\nElena Voita and Ivan Titov.\\n2020.\\nInformation-theoretic probing with minimum description length.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183–196, Online.\\nAssociation for Computational Linguistics.\\nAndreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024a.\\nDive into the chasm: Probing the gap between in- and cross-topic generalization.\\nIn Findings of the Association for Computational Linguistics: EACL 2024, pages 2197–2214, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.\\nAndreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024b.\\nHow to handle different types of out-ofdistribution scenarios in computational argumentation?\\na comprehensive and fine-grained field study.\\nCoRR, abs/2309.08316.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\n2019a.\\nSuperglue: A stickier benchmark for general-purpose language understanding systems.\\nIn Advances in Neural Information Processing Systems, volume 32.\\nCurran Associates, Inc. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\n2019b.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nBoxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li.\\n2021.\\nAdversarial GLUE: A multitask benchmark for robustness evaluation of language models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\\n2023.\\nHow far can camels go?\\nexploring the state of instruction tuning on open resources.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen.\\n2022.\\nSuper-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085–5109, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman.\\n2020.\\nBLiMP: The benchmark of linguistic minimal pairs for English.\\nTransactions of the Association for Computational Linguistics, 8:377– 392.\\nBonnie Webber, Rashmi Prasad, Alan Lee, and Aravind Joshi.\\n2019.\\nThe penn discourse treebank 3.0 annotation manual.\\nPhiladelphia, University of Pennsylvania.\\nRalph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al.\\n2013.\\nOntonotes release 5.0.\\nLinguistic Data Consortium, Philadelphia, PA, 23:170.\\nAaron Steven White, Drew Reisinger, Keisuke Sakaguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme.\\n2016.\\nUniversal decompositional semantics on Universal Dependencies.\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1713–1723, Austin, Texas.\\nAssociation for Computational Linguistics.\\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu.\\n2020.\\nPerturbed masking: Parameter-free probing for analyzing and interpreting BERT.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4166–4176, Online.\\nAssociation for Computational Linguistics.\\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.\\n2023.\\nWizardlm: Empowering large language models to follow complex instructions.\\nCoRR, abs/2304.12244.\\nPrateek Yadav, Leshem Choshen, Colin Raffel, and Mohit Bansal.\\n2023.\\nCompeft: Compression for communicating parameter efficient updates via sparsification and quantization.\\nCoRR, abs/2311.13171.\\nLinyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang.\\n2023.\\nGLUE-X: Evaluating natural language understanding models from an out-ofdistribution generalization perspective.\\nIn Findings of the Association for Computational Linguistics:\\nACL 2023, pages 12731–12750, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nAmir Zeldes.\\n2017.\\nThe GUM corpus: Creating multilayer resources in the classroom.\\nLanguage Resources and Evaluation, 51(3):581–612.\\nXikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth.\\n2020. Do language embeddings capture scales?\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 292–299, Online.\\nAssociation for Computational Linguistics.\\nYian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman.\\n2021.\\nWhen do you need billions of words of pretraining data?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1112–1125, Online.\\nAssociation for Computational Linguistics.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\n2023.\\nJudging llm-as-a-judge with mt-bench and chatbot arena.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\\n2023.\\nLIMA: less is more for alignment.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nZining Zhu, Soroosh Shahtalebi, and Frank Rudzicz.\\n2022a.\\nPredicting fine-tuning performance with probing.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11534–11547, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nZining Zhu, Jixuan Wang, Bai Li, and Frank Rudzicz.\\n2022b.\\nOn the data requirements of probing.\\nIn Findings of the Association for Computational Linguistics: ACL 2022, pages 4132–4147, Dublin, Ireland.\\nAssociation for Computational Linguistics.',\n",
       "   'title': 'References',\n",
       "   'chunks': [{'text': 'Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg.\\n2017.\\nFine-grained analysis of sentence embeddings using auxiliary prediction tasks.\\nIn 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\\nOpenReview.net.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Ehsan Aghazadeh, Mohsen Fayyaz, and Yadollah Yaghoobzadeh.\\n2022.\\nMetaphors in pre-trained language models: Probing and generalization across datasets and languages.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2037– 2050, Dublin, Ireland.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Samuel Amouyal, Aya Meltzer-Asscher, and Jonathan Berant.\\n2024.\\nLarge language models for psycholinguistic plausibility pretesting.\\nIn Findings of the Association for Computational Linguistics: EACL 2024, pages 166–181, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, and Antoine Bosselut.\\n2023.\\nInstruction-tuning aligns llms to the human brain.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'CoRR, abs/2312.00575.\\nSimone Balloccu, Patrícia Schmidtová, Mateusz Lango, and Ondrej Dusek.\\n2024.\\nLeak, cheat, repeat: Data contamination and evaluation malpractices in closedsource LLMs.\\nIn Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 67–93, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.\\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James R. Glass.\\n2019.\\nIdentifying and controlling important neurons in neural machine translation.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.\\n2023.\\nOpen llm leaderboard.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'https://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yonatan Belinkov.\\n2022.\\nProbing classifiers: Promises, shortcomings, and advances.\\nComputational Linguistics, 48(1):207–219.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass.\\n2017.\\nWhat do neural machine translation models learn about morphology?\\nIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861–872, Vancouver, Canada.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal.\\n2023.\\nPythia: A suite for analyzing large language models across training and scaling.\\nIn International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2397–2430.\\nPMLR.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Julia Birke and Anoop Sarkar.\\n2006.\\nA clustering approach for nearly unsupervised recognition of nonliteral language.\\nIn 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 329–336, Trento, Italy.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Terra Blevins, Hila Gonen, and Luke Zettlemoyer.\\n2023.\\nPrompting language models for linguistic structure.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6649–6663, Toronto, Canada.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Nikolay Bogoychev and Adam Lopez.\\n2016.\\nN-gram language models for massively parallel devices.\\nIn',\n",
       "     'title': 'References',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Long Papers), pages 1944–1953, Berlin, Germany.\\nAssociation for Computational Linguistics.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\\n2020.\\nLanguage models are few-shot learners.\\nIn Advances in Neural Information Processing Systems 33:',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Lynn Carlson, Daniel Marcu, and Mary Ellen Okurovsky.\\n2001.\\nBuilding a discourse-tagged corpus in the framework of rhetorical structure theory.\\nIn Proceedings of the SIGDIAL 2001 Workshop, The 2nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, Saturday, September 1, 2001 to Sunday, September 2, 2001, Aalborg, Denmark.\\nThe Association for Computer Linguistics.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Noam Chomsky.\\n1965.\\nAspects of the Theory of Syntax.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'The MIT Press, Cambridge.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\\n2022.\\nScaling instruction-finetuned language models.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'CoRR, abs/2210.11416.\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.\\n2020.\\nELECTRA: pretraining text encoders as discriminators rather than generators.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\\n2021.\\nTraining verifiers to solve math word problems.\\nCoRR, abs/2110.14168.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni.\\n2018.\\nWhat you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.\\nIn',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Long Papers), pages 2126–2136, Melbourne, Australia.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.\\n2023.\\nFree dolly: Introducing the world’s first truly open instructiontuned llm.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei.\\n2009.\\nImagenet: A large-scale hierarchical image database.\\n2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\n2019.\\nBERT: Pre-training of deep bidirectional transformers for language under- standing.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg.\\n2021.\\nAmnesic probing: Behavioral explanation with amnesic counterfactuals.\\nTransactions of the Association for Computational Linguistics, 9:160– 175.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Rudolf Franz Flesch.\\n1948.\\nA new readability yardstick.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'The Journal of applied psychology, 32(3):221–233.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'William Gantt, Lelia Glass, and Aaron Steven White.\\n2022.\\nDecomposing and recomposing event structure.\\nTransactions of the Association for Computational Linguistics, 10:17–34.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Vagrant Gautam, Eileen Bingert, D. Zhu, Anne Lauscher, and Dietrich Klakow.\\n2024.\\nRobust pronoun use fidelity with english llms: Are they reasoning, repeating, or just biased?\\nCoRR, abs/2404.03134.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema.\\n2018.\\nUnder the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 240–248, Brussels, Belgium.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Venkata Govindarajan, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nDecomposing generalization: Models of generic, habitual, and episodic statements.\\nTransactions of the Association for Computational Linguistics, 7:501–517.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Zellig S Harris.\\n1954.\\nDistributional structure.\\nWord, 10(2-3):146–162.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Pengcheng He, Jianfeng Gao, and Weizhu Chen.\\n2023.\\nDebertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.\\n2021.\\nDeberta: decoding-enhanced bert with disentangled attention.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.\\n2010.\\nSemEval-2010 task 8: Multiway classification of semantic relations between pairs of nominals.\\nIn Proceedings of the 5th International',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Workshop on Semantic Evaluation, pages 33–38, Uppsala, Sweden.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021.\\nMeasuring massive multitask language understanding.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Moshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, Swaminathan Sundararaman, and Danny Harnik.\\n2024.\\nLossless and nearlossless compression for foundation models.\\nCoRR, abs/2404.15198.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'John Hewitt and Percy Liang.\\n2019.\\nDesigning and interpreting probes with control tasks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–2743, Hong Kong, China.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'John Hewitt and Christopher D. Manning.\\n2019.\\nA structural probe for finding syntax in word representations.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yufang Hou.\\n2018.\\nEnhanced word representations for bridging anaphora resolution.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 1–7, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yufang Hou.\\n2020.\\nBridging anaphora resolution as question answering.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1428–1438, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Jennifer Hu and Roger Levy.\\n2023.\\nPrompting is not a substitute for probability measurements in large language models.\\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5040–5060, Singapore.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Philip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth.\\n2021.\\nBabyBERTa: Learning more grammar with small-scale child-directed language.\\nIn Proceedings of the 25th Conference on Computational Natural Language Learning, pages 624–646, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Alexander Immer, Lucas Torroba Hennigen, Vincent Fortuin, and Ryan Cotterell.\\n2022.\\nProbing as quantifying inductive bias.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1839– 1851, Dublin, Ireland.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2023.\\nMistral 7b.\\nCoRR, abs/2310.06825.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2024.\\nMixtral of experts.\\nCoRR, abs/2401.04088.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Josef Klafka and Allyson Ettinger.\\n2020.\\nSpying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4801–4811, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Arne Köhn.\\n2015.\\nWhat’s in an embedding?\\nanalyzing word embeddings through multilingual evaluation.\\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073, Lisbon, Portugal.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Natalia Konstantinova, Sheila C.M.\\nde Sousa, Noa P. Cruz, Manuel J. Maña, Maite Taboada, and Ruslan Mitkov.\\n2012.\\nA review corpus annotated for negation, speculation and their scope.\\nIn Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 3190–3195, Istanbul, Turkey.\\nEuropean Language Resources Association (ELRA).',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Fajri Koto, Jey Han Lau, and Timothy Baldwin.\\n2021.\\nDiscourse probing of pretrained language models.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3849–3864, Online.\\nAssociation for Computational Linguistics.\\nKatarzyna Krasnowska-Kieraś and Alina Wróblewska.\\n2019.\\nEmpirical linguistic study of sentence embeddings.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5729–5739, Florence, Italy.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Murathan Kurfalı and Robert Östling.\\n2021.\\nProbing multilingual language models for discourse.\\nIn',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 8–19, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\\n2020.\\nALBERT: A lite BERT for self-supervised learning of language representations.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Anne Lauscher, Debora Nozza, Ehm Miltersen, Archie Crowley, and Dirk Hovy.\\n2023.\\nWhat about “em”?\\nhow commercial machine translation fails to handle (neo-)pronouns.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 377–392, Toronto, Canada.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020.\\nBART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana AcostaNavas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.\\n2023.\\nHolistic evaluation of language models.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Transactions on Machine Learning Research.\\nFeatured Certification, Expert Certification.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\\n2016.\\nAssessing the ability of LSTMs to learn syntaxsensitive dependencies.\\nTransactions of the Association for Computational Linguistics, 4:521–535.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019.\\nRoberta: A robustly optimized BERT pretraining approach.\\nCoRR, abs/1907.11692.\\nIlya Loshchilov and Frank Hutter.\\n2019.\\nDecoupled weight decay regularization.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenRe- view.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych.\\n2023.\\nAre emergent abilities in large language models just in-context learning?\\nCoRR, abs/2309.01809.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko.\\n2024.\\nDissociating language and thought in large language models.\\nTrends in Cognitive Sciences.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'George A. Miller.\\n1995.\\nWordnet: A lexical database for english.\\nCommunications of the ACM, 38(11):39– 41.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.\\n2022.\\nRethinking the role of demonstrations:',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'What makes in-context learning work?\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048–11064, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andrés Codas, Clarisse Simões, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah.\\n2023.\\nOrca 2: Teaching small language models how to reason.\\nCoRR, abs/2311.11045.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky.\\n2024.\\nState of what art? A call for multi-prompt LLM evaluation.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'CoRR, abs/2401.00595.\\nMichael Mohler, Mary Brunson, Bryan Rink, and Marc Tomlinson.\\n2016.\\nIntroducing the LCC metaphor datasets.\\nIn Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 4221–4227, Portorož, Slovenia.\\nEuropean Language Resources Association (ELRA).',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Roser Morante and Eduardo Blanco.\\n2012.\\n*SEM 2012 shared task: Resolving the scope and focus of negation.\\nIn *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 265–274, Montréal, Canada.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Marius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020a.\\nOn the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers.\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 68–82, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Marius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020b.\\nOn the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2502–2516, Online.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Association for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers.\\n2023.\\nMTEB: Massive text embedding benchmark.\\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014–2037, Dubrovnik, Croatia.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Shashi Narayan, Shay B. Cohen, and Mirella Lapata.\\n2018.\\nDon’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for extreme summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Allen Nie, Erin Bennett, and Noah Goodman.\\n2019.\\nDisSent: Learning sentence representations from explicit discourse relations.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4497–4510, Florence, Italy.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\\n2020.\\nAdversarial NLI: A new benchmark for natural language understanding.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885–4901, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022.\\nTraining language models to follow instructions with human feedback.\\nIn Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Gustavo Paetzold and Lucia Specia.\\n2016.\\nSemEval 2016 task 11: Complex word identification.\\nIn Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 560–569, San Diego, California.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Onkar Pandit and Yufang Hou.\\n2021.\\nProbing for bridging inference in transformer language models.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4153–4163, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Jeffrey Pennington, Richard Socher, and Christopher Manning.\\n2014.\\nGloVe: Global vectors for word representation.\\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen.\\n2023.\\nEfficient benchmarking (of language models).\\nCoRR, abs/2308.11696.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Fabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel.\\n2020.\\nHow context affects language models’ factual predictions.\\nIn Conference on Automated Knowledge Base Construction, AKBC 2020, Virtual, June 22-24, 2020.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller.\\n2019.\\nLanguage models as knowledge bases?\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2463–2473.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell.\\n2020.\\nInformation-theoretic probing for linguistic structure.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4609–4622, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\n2019.\\nLanguage models are unsupervised multitask learners.\\nOpenAI blog, 1(8):9.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\\n2020.\\nExploring the limits of transfer learning with a unified text-to-text transformer.\\nJournal of Machine Learning Research, 21(140):1–67.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, and Jordan BoydGraber.\\n2021.\\nEvaluation examples are not equally informative: How should that change NLP leaderboards?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4486–4503, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Rachel Rudinger, Adam Teichert, Ryan Culkin, Sheng Zhang, and Benjamin Van Durme.\\n2018a.\\nNeuralDavidsonian semantic proto-role labeling.\\nIn Pro-',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 944–955, Brussels, Belgium.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Rachel Rudinger, Aaron Steven White, and Benjamin Van Durme.\\n2018b.\\nNeural models of factuality.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 731–744, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Naomi Shapiro, Amandalynne Paullada, and Shane Steinert-Threlkeld.\\n2021.\\nA multilabel approach to morphosyntactic probing.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4486–4524, Punta Cana, Dominican Republic.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Pratyusha Sharma, Jordan T. Ash, and Dipendra Misra.\\n2023.\\nThe truth is in there: Improving reasoning in language models with layer-selective rank reduction.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'CoRR, abs/2312.13558.\\nXing Shi, Inkit Padhi, and Kevin Knight.\\n2016.\\nDoes string-based neural MT learn source syntax?\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526– 1534, Austin, Texas.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Natalia Silveira, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning.\\n2014.\\nA gold standard dependency corpus for English.\\nIn Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2897– 2904, Reykjavik, Iceland.\\nEuropean Language Resources Association (ELRA).',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts.\\n2013.\\nRecursive deep models for semantic compositionality over a sentiment treebank.\\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al.\\n2022.\\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'CoRR, abs/2206.04615.\\nGerard J Steen, Aletta G Dorst, J Berenike Herrmann, Anna A Kaal, Tina Krennmayr, Tryntje Pasma, et al.\\n2010.\\nA method for linguistic metaphor identification.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Converging evidence in language and communication research.\\nJohn Benjamins Publishing Company Amsterdam.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, and Akash Srivastava.\\n2024.\\nLAB: large-scale alignment for chatbots.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'CoRR, abs/2403.01081.\\nGyörgy Szarvas, Veronika Vincze, Richárd Farkas, and János Csirik.\\n2008.\\nThe BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts.\\nIn Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 38–45, Columbus, Ohio.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant.\\n2020. oLMpics-On What Language Model Pre-training Captures.\\nTransactions of the Association for Computational Linguistics, 8:743–758.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler.\\n2023.\\nUL2: unifying language learning paradigms.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Ian Tenney, Dipanjan Das, and Ellie Pavlick.\\n2019a.\\nBERT rediscovers the classical NLP pipeline.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick.\\n2019b.\\nWhat do you learn from context?\\nprobing for sentence structure in contextualized word representations.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych.\\n2021.\\nBEIR:',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'A heterogeneous benchmark for zero-shot evaluation of information retrieval models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Lucas Torroba Hennigen, Adina Williams, and Ryan Cotterell.\\n2020.\\nIntrinsic probing through dimension selection.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 197–216, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\n2023.\\nLlama 2: Open foundation and finetuned chat models.\\nCoRR, abs/2307.09288.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Teemu Vahtola, Mathias Creutz, and Jörg Tiedemann.\\n2022.\\nIt is not easy to detect paraphrases: Analysing semantic similarity with antonyms and negation using the new SemAntoNeg benchmark.\\nIn Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 249–262, Abu Dhabi, United Arab Emirates (Hybrid).\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Siddharth Vashishtha, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nFine-grained temporal relation extraction.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2906–2919, Florence, Italy.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Sara Veldhoen, Dieuwke Hupkes, and Willem H. Zuidema.\\n2016.\\nDiagnostic classifiers revealing how neural networks process hierarchical structure.\\nIn Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings.\\nCEUR-WS.org.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Elena Voita and Ivan Titov.\\n2020.\\nInformation-theoretic probing with minimum description length.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183–196, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Andreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024a.\\nDive into the chasm: Probing the gap between in- and cross-topic generalization.\\nIn Findings of the Association for Computational Linguistics: EACL 2024, pages 2197–2214, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Andreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024b.\\nHow to handle different types of out-ofdistribution scenarios in computational argumentation?\\na comprehensive and fine-grained field study.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'CoRR, abs/2309.08316.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\n2019a.\\nSuperglue: A stickier benchmark for general-purpose language understanding systems.\\nIn Advances in Neural Information Processing Systems, volume 32.\\nCurran Associates, Inc. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\n2019b.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'OpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li.\\n2021.\\nAdversarial GLUE: A multitask benchmark for robustness evaluation of language models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\\n2023.\\nHow far can camels go?\\nexploring the state of instruction tuning on open resources.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen.\\n2022.\\nSuper-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085–5109, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman.\\n2020.\\nBLiMP: The benchmark of linguistic minimal pairs for English.\\nTransactions of the Association for Computational Linguistics, 8:377– 392.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Bonnie Webber, Rashmi Prasad, Alan Lee, and Aravind Joshi.\\n2019.\\nThe penn discourse treebank 3.0 annotation manual.\\nPhiladelphia, University of Pennsylvania.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al.\\n2013.\\nOntonotes release 5.0.\\nLinguistic Data Consortium, Philadelphia, PA, 23:170.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Aaron Steven White, Drew Reisinger, Keisuke Sakaguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme.\\n2016.\\nUniversal decompositional semantics on Universal Dependencies.\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1713–1723, Austin, Texas.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu.\\n2020.\\nPerturbed masking: Parameter-free probing for analyzing and interpreting BERT.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4166–4176, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.\\n2023.\\nWizardlm: Empowering large language models to follow complex instructions.\\nCoRR, abs/2304.12244.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Prateek Yadav, Leshem Choshen, Colin Raffel, and Mohit Bansal.\\n2023.\\nCompeft: Compression for communicating parameter efficient updates via sparsification and quantization.\\nCoRR, abs/2311.13171.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang.\\n2023.\\nGLUE-X: Evaluating natural language understanding models from an out-ofdistribution generalization perspective.\\nIn Findings of the Association for Computational Linguistics:',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'ACL 2023, pages 12731–12750, Toronto, Canada.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Amir Zeldes.\\n2017.\\nThe GUM corpus: Creating multilayer resources in the classroom.\\nLanguage Resources and Evaluation, 51(3):581–612.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth.\\n2020. Do language embeddings capture scales?\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 292–299, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Yian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman.\\n2021.\\nWhen do you need billions of words of pretraining data?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1112–1125, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\n2023.\\nJudging llm-as-a-judge with mt-bench and chatbot arena.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\\n2023.\\nLIMA: less is more for alignment.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Zining Zhu, Soroosh Shahtalebi, and Frank Rudzicz.\\n2022a.\\nPredicting fine-tuning performance with probing.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11534–11547, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Zining Zhu, Jixuan Wang, Bai Li, and Frank Rudzicz.\\n2022b.\\nOn the data requirements of probing.\\nIn Findings of the Association for Computational Linguistics: ACL 2022, pages 4132–4147, Dublin, Ireland.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'References'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 1944–1953, Berlin, Germany.\\nAssociation for Computational Linguistics.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\\n2020.\\nLanguage models are few-shot learners.\\nIn Advances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nLynn Carlson, Daniel Marcu, and Mary Ellen Okurovsky.\\n2001.\\nBuilding a discourse-tagged corpus in the framework of rhetorical structure theory.\\nIn Proceedings of the SIGDIAL 2001 Workshop, The 2nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, Saturday, September 1, 2001 to Sunday, September 2, 2001, Aalborg, Denmark.\\nThe Association for Computer Linguistics.\\nNoam Chomsky.\\n1965.\\nAspects of the Theory of Syntax.\\nThe MIT Press, Cambridge.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\\n2022.\\nScaling instruction-finetuned language models.\\nCoRR, abs/2210.11416.\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.\\n2020.\\nELECTRA: pretraining text encoders as discriminators rather than generators.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\\n2021.\\nTraining verifiers to solve math word problems.\\nCoRR, abs/2110.14168.\\nAlexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni.\\n2018.\\nWhat you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.\\nIn',\n",
       "   'title': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "   'chunks': [{'text': 'Long Papers), pages 1944–1953, Berlin, Germany.\\nAssociation for Computational Linguistics.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\\n2020.\\nLanguage models are few-shot learners.\\nIn Advances in Neural Information Processing Systems 33:',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 10,\n",
       "     'source_doc': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Lynn Carlson, Daniel Marcu, and Mary Ellen Okurovsky.\\n2001.\\nBuilding a discourse-tagged corpus in the framework of rhetorical structure theory.\\nIn Proceedings of the SIGDIAL 2001 Workshop, The 2nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, Saturday, September 1, 2001 to Sunday, September 2, 2001, Aalborg, Denmark.\\nThe Association for Computer Linguistics.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Noam Chomsky.\\n1965.\\nAspects of the Theory of Syntax.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'The MIT Press, Cambridge.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\\n2022.\\nScaling instruction-finetuned language models.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'CoRR, abs/2210.11416.\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.\\n2020.\\nELECTRA: pretraining text encoders as discriminators rather than generators.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\\n2021.\\nTraining verifiers to solve math word problems.\\nCoRR, abs/2110.14168.',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni.\\n2018.\\nWhat you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.\\nIn',\n",
       "     'title': 'References > Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 2126–2136, Melbourne, Australia.\\nAssociation for Computational Linguistics.\\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.\\n2023.\\nFree dolly: Introducing the world’s first truly open instructiontuned llm.\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei.\\n2009.\\nImagenet: A large-scale hierarchical image database.\\n2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\n2019.\\nBERT: Pre-training of deep bidirectional transformers for language under- standing.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.\\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg.\\n2021.\\nAmnesic probing: Behavioral explanation with amnesic counterfactuals.\\nTransactions of the Association for Computational Linguistics, 9:160– 175.\\nRudolf Franz Flesch.\\n1948.\\nA new readability yardstick.\\nThe Journal of applied psychology, 32(3):221–233.\\nWilliam Gantt, Lelia Glass, and Aaron Steven White.\\n2022.\\nDecomposing and recomposing event structure.\\nTransactions of the Association for Computational Linguistics, 10:17–34.\\nVagrant Gautam, Eileen Bingert, D. Zhu, Anne Lauscher, and Dietrich Klakow.\\n2024.\\nRobust pronoun use fidelity with english llms: Are they reasoning, repeating, or just biased?\\nCoRR, abs/2404.03134.\\nMario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema.\\n2018.\\nUnder the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 240–248, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nVenkata Govindarajan, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nDecomposing generalization: Models of generic, habitual, and episodic statements.\\nTransactions of the Association for Computational Linguistics, 7:501–517.\\nZellig S Harris.\\n1954.\\nDistributional structure.\\nWord, 10(2-3):146–162.\\nPengcheng He, Jianfeng Gao, and Weizhu Chen.\\n2023.\\nDebertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.\\n2021.\\nDeberta: decoding-enhanced bert with disentangled attention.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.\\n2010.\\nSemEval-2010 task 8: Multiway classification of semantic relations between pairs of nominals.\\nIn Proceedings of the 5th International\\nWorkshop on Semantic Evaluation, pages 33–38, Uppsala, Sweden.\\nAssociation for Computational Linguistics.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021.\\nMeasuring massive multitask language understanding.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nMoshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, Swaminathan Sundararaman, and Danny Harnik.\\n2024.\\nLossless and nearlossless compression for foundation models.\\nCoRR, abs/2404.15198.\\nJohn Hewitt and Percy Liang.\\n2019.\\nDesigning and interpreting probes with control tasks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–2743, Hong Kong, China.\\nAssociation for Computational Linguistics.\\nJohn Hewitt and Christopher D. Manning.\\n2019.\\nA structural probe for finding syntax in word representations.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.\\nYufang Hou.\\n2018.\\nEnhanced word representations for bridging anaphora resolution.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 1–7, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.\\nYufang Hou.\\n2020.\\nBridging anaphora resolution as question answering.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1428–1438, Online.\\nAssociation for Computational Linguistics.\\nJennifer Hu and Roger Levy.\\n2023.\\nPrompting is not a substitute for probability measurements in large language models.\\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5040–5060, Singapore.\\nAssociation for Computational Linguistics.\\nPhilip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth.\\n2021.\\nBabyBERTa: Learning more grammar with small-scale child-directed language.\\nIn Proceedings of the 25th Conference on Computational Natural Language Learning, pages 624–646, Online.\\nAssociation for Computational Linguistics.\\nAlexander Immer, Lucas Torroba Hennigen, Vincent Fortuin, and Ryan Cotterell.\\n2022.\\nProbing as quantifying inductive bias.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1839– 1851, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2023.\\nMistral 7b.\\nCoRR, abs/2310.06825.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2024.\\nMixtral of experts.\\nCoRR, abs/2401.04088.\\nJosef Klafka and Allyson Ettinger.\\n2020.\\nSpying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4801–4811, Online.\\nAssociation for Computational Linguistics.\\nArne Köhn.\\n2015.\\nWhat’s in an embedding?\\nanalyzing word embeddings through multilingual evaluation.\\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073, Lisbon, Portugal.\\nAssociation for Computational Linguistics.\\nNatalia Konstantinova, Sheila C.M.\\nde Sousa, Noa P. Cruz, Manuel J. Maña, Maite Taboada, and Ruslan Mitkov.\\n2012.\\nA review corpus annotated for negation, speculation and their scope.\\nIn Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 3190–3195, Istanbul, Turkey.\\nEuropean Language Resources Association (ELRA).\\nFajri Koto, Jey Han Lau, and Timothy Baldwin.\\n2021.\\nDiscourse probing of pretrained language models.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3849–3864, Online.\\nAssociation for Computational Linguistics.\\nKatarzyna Krasnowska-Kieraś and Alina Wróblewska.\\n2019.\\nEmpirical linguistic study of sentence embeddings.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5729–5739, Florence, Italy.\\nAssociation for Computational Linguistics.\\nMurathan Kurfalı and Robert Östling.\\n2021.\\nProbing multilingual language models for discourse.\\nIn\\nProceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 8–19, Online.\\nAssociation for Computational Linguistics.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\\n2020.\\nALBERT: A lite BERT for self-supervised learning of language representations.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.\\nAnne Lauscher, Debora Nozza, Ehm Miltersen, Archie Crowley, and Dirk Hovy.\\n2023.\\nWhat about “em”?\\nhow commercial machine translation fails to handle (neo-)pronouns.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 377–392, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020.\\nBART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online.\\nAssociation for Computational Linguistics.\\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana AcostaNavas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.\\n2023.\\nHolistic evaluation of language models.\\nTransactions on Machine Learning Research.\\nFeatured Certification, Expert Certification.\\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\\n2016.\\nAssessing the ability of LSTMs to learn syntaxsensitive dependencies.\\nTransactions of the Association for Computational Linguistics, 4:521–535.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019.\\nRoberta: A robustly optimized BERT pretraining approach.\\nCoRR, abs/1907.11692.\\nIlya Loshchilov and Frank Hutter.\\n2019.\\nDecoupled weight decay regularization.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenRe- view.net.\\nSheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych.\\n2023.\\nAre emergent abilities in large language models just in-context learning?\\nCoRR, abs/2309.01809.\\nKyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko.\\n2024.\\nDissociating language and thought in large language models.\\nTrends in Cognitive Sciences.\\nGeorge A. Miller.\\n1995.\\nWordnet: A lexical database for english.\\nCommunications of the ACM, 38(11):39– 41.\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.\\n2022.\\nRethinking the role of demonstrations:\\nWhat makes in-context learning work?\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048–11064, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nArindam Mitra, Luciano Del Corro, Shweti Mahajan, Andrés Codas, Clarisse Simões, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah.\\n2023.\\nOrca 2: Teaching small language models how to reason.\\nCoRR, abs/2311.11045.\\nMoran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky.\\n2024.\\nState of what art? A call for multi-prompt LLM evaluation.\\nCoRR, abs/2401.00595.\\nMichael Mohler, Mary Brunson, Bryan Rink, and Marc Tomlinson.\\n2016.\\nIntroducing the LCC metaphor datasets.\\nIn Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 4221–4227, Portorož, Slovenia.\\nEuropean Language Resources Association (ELRA).\\nRoser Morante and Eduardo Blanco.\\n2012.\\n*SEM 2012 shared task: Resolving the scope and focus of negation.\\nIn *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 265–274, Montréal, Canada.\\nAssociation for Computational Linguistics.\\nMarius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020a.\\nOn the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers.\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 68–82, Online.\\nAssociation for Computational Linguistics.\\nMarius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020b.\\nOn the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2502–2516, Online.\\nAssociation for Computational Linguistics.\\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers.\\n2023.\\nMTEB: Massive text embedding benchmark.\\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014–2037, Dubrovnik, Croatia.\\nAssociation for Computational Linguistics.\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\\n2018.\\nDon’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for extreme summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nAllen Nie, Erin Bennett, and Noah Goodman.\\n2019.\\nDisSent: Learning sentence representations from explicit discourse relations.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4497–4510, Florence, Italy.\\nAssociation for Computational Linguistics.\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\\n2020.\\nAdversarial NLI: A new benchmark for natural language understanding.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885–4901, Online.\\nAssociation for Computational Linguistics.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022.\\nTraining language models to follow instructions with human feedback.\\nIn Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.\\nGustavo Paetzold and Lucia Specia.\\n2016.\\nSemEval 2016 task 11: Complex word identification.\\nIn Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 560–569, San Diego, California.\\nAssociation for Computational Linguistics.\\nOnkar Pandit and Yufang Hou.\\n2021.\\nProbing for bridging inference in transformer language models.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4153–4163, Online.\\nAssociation for Computational Linguistics.\\nJeffrey Pennington, Richard Socher, and Christopher Manning.\\n2014.\\nGloVe: Global vectors for word representation.\\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar.\\nAssociation for Computational Linguistics.\\nYotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen.\\n2023.\\nEfficient benchmarking (of language models).\\nCoRR, abs/2308.11696.\\nFabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel.\\n2020.\\nHow context affects language models’ factual predictions.\\nIn Conference on Automated Knowledge Base Construction, AKBC 2020, Virtual, June 22-24, 2020.\\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller.\\n2019.\\nLanguage models as knowledge bases?\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2463–2473.\\nAssociation for Computational Linguistics.\\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell.\\n2020.\\nInformation-theoretic probing for linguistic structure.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4609–4622, Online.\\nAssociation for Computational Linguistics.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\n2019.\\nLanguage models are unsupervised multitask learners.\\nOpenAI blog, 1(8):9.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\\n2020.\\nExploring the limits of transfer learning with a unified text-to-text transformer.\\nJournal of Machine Learning Research, 21(140):1–67.\\nPedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, and Jordan BoydGraber.\\n2021.\\nEvaluation examples are not equally informative: How should that change NLP leaderboards?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4486–4503, Online.\\nAssociation for Computational Linguistics.\\nRachel Rudinger, Adam Teichert, Ryan Culkin, Sheng Zhang, and Benjamin Van Durme.\\n2018a.\\nNeuralDavidsonian semantic proto-role labeling.\\nIn Pro-\\nceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 944–955, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nRachel Rudinger, Aaron Steven White, and Benjamin Van Durme.\\n2018b.\\nNeural models of factuality.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 731–744, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.\\nNaomi Shapiro, Amandalynne Paullada, and Shane Steinert-Threlkeld.\\n2021.\\nA multilabel approach to morphosyntactic probing.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4486–4524, Punta Cana, Dominican Republic.\\nAssociation for Computational Linguistics.\\nPratyusha Sharma, Jordan T. Ash, and Dipendra Misra.\\n2023.\\nThe truth is in there: Improving reasoning in language models with layer-selective rank reduction.\\nCoRR, abs/2312.13558.\\nXing Shi, Inkit Padhi, and Kevin Knight.\\n2016.\\nDoes string-based neural MT learn source syntax?\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526– 1534, Austin, Texas.\\nAssociation for Computational Linguistics.\\nNatalia Silveira, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning.\\n2014.\\nA gold standard dependency corpus for English.\\nIn Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2897– 2904, Reykjavik, Iceland.\\nEuropean Language Resources Association (ELRA).\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts.\\n2013.\\nRecursive deep models for semantic compositionality over a sentiment treebank.\\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA.\\nAssociation for Computational Linguistics.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al.\\n2022.\\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.\\nCoRR, abs/2206.04615.\\nGerard J Steen, Aletta G Dorst, J Berenike Herrmann, Anna A Kaal, Tina Krennmayr, Tryntje Pasma, et al.\\n2010.\\nA method for linguistic metaphor identification.\\nConverging evidence in language and communication research.\\nJohn Benjamins Publishing Company Amsterdam.\\nShivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, and Akash Srivastava.\\n2024.\\nLAB: large-scale alignment for chatbots.\\nCoRR, abs/2403.01081.\\nGyörgy Szarvas, Veronika Vincze, Richárd Farkas, and János Csirik.\\n2008.\\nThe BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts.\\nIn Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 38–45, Columbus, Ohio.\\nAssociation for Computational Linguistics.\\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant.\\n2020. oLMpics-On What Language Model Pre-training Captures.\\nTransactions of the Association for Computational Linguistics, 8:743–758.\\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler.\\n2023.\\nUL2: unifying language learning paradigms.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.\\nIan Tenney, Dipanjan Das, and Ellie Pavlick.\\n2019a.\\nBERT rediscovers the classical NLP pipeline.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy.\\nAssociation for Computational Linguistics.\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick.\\n2019b.\\nWhat do you learn from context?\\nprobing for sentence structure in contextualized word representations.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych.\\n2021.\\nBEIR:\\nA heterogeneous benchmark for zero-shot evaluation of information retrieval models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\nLucas Torroba Hennigen, Adina Williams, and Ryan Cotterell.\\n2020.\\nIntrinsic probing through dimension selection.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 197–216, Online.\\nAssociation for Computational Linguistics.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\n2023.\\nLlama 2: Open foundation and finetuned chat models.\\nCoRR, abs/2307.09288.\\nTeemu Vahtola, Mathias Creutz, and Jörg Tiedemann.\\n2022.\\nIt is not easy to detect paraphrases: Analysing semantic similarity with antonyms and negation using the new SemAntoNeg benchmark.\\nIn Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 249–262, Abu Dhabi, United Arab Emirates (Hybrid).\\nAssociation for Computational Linguistics.\\nSiddharth Vashishtha, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nFine-grained temporal relation extraction.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2906–2919, Florence, Italy.\\nAssociation for Computational Linguistics.\\nSara Veldhoen, Dieuwke Hupkes, and Willem H. Zuidema.\\n2016.\\nDiagnostic classifiers revealing how neural networks process hierarchical structure.\\nIn Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings.\\nCEUR-WS.org.\\nElena Voita and Ivan Titov.\\n2020.\\nInformation-theoretic probing with minimum description length.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183–196, Online.\\nAssociation for Computational Linguistics.\\nAndreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024a.\\nDive into the chasm: Probing the gap between in- and cross-topic generalization.\\nIn Findings of the Association for Computational Linguistics: EACL 2024, pages 2197–2214, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.\\nAndreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024b.\\nHow to handle different types of out-ofdistribution scenarios in computational argumentation?\\na comprehensive and fine-grained field study.\\nCoRR, abs/2309.08316.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\n2019a.\\nSuperglue: A stickier benchmark for general-purpose language understanding systems.\\nIn Advances in Neural Information Processing Systems, volume 32.\\nCurran Associates, Inc. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\n2019b.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nBoxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li.\\n2021.\\nAdversarial GLUE: A multitask benchmark for robustness evaluation of language models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\\n2023.\\nHow far can camels go?\\nexploring the state of instruction tuning on open resources.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen.\\n2022.\\nSuper-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085–5109, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman.\\n2020.\\nBLiMP: The benchmark of linguistic minimal pairs for English.\\nTransactions of the Association for Computational Linguistics, 8:377– 392.\\nBonnie Webber, Rashmi Prasad, Alan Lee, and Aravind Joshi.\\n2019.\\nThe penn discourse treebank 3.0 annotation manual.\\nPhiladelphia, University of Pennsylvania.\\nRalph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al.\\n2013.\\nOntonotes release 5.0.\\nLinguistic Data Consortium, Philadelphia, PA, 23:170.\\nAaron Steven White, Drew Reisinger, Keisuke Sakaguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme.\\n2016.\\nUniversal decompositional semantics on Universal Dependencies.\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1713–1723, Austin, Texas.\\nAssociation for Computational Linguistics.\\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu.\\n2020.\\nPerturbed masking: Parameter-free probing for analyzing and interpreting BERT.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4166–4176, Online.\\nAssociation for Computational Linguistics.\\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.\\n2023.\\nWizardlm: Empowering large language models to follow complex instructions.\\nCoRR, abs/2304.12244.\\nPrateek Yadav, Leshem Choshen, Colin Raffel, and Mohit Bansal.\\n2023.\\nCompeft: Compression for communicating parameter efficient updates via sparsification and quantization.\\nCoRR, abs/2311.13171.\\nLinyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang.\\n2023.\\nGLUE-X: Evaluating natural language understanding models from an out-ofdistribution generalization perspective.\\nIn Findings of the Association for Computational Linguistics:\\nACL 2023, pages 12731–12750, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nAmir Zeldes.\\n2017.\\nThe GUM corpus: Creating multilayer resources in the classroom.\\nLanguage Resources and Evaluation, 51(3):581–612.\\nXikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth.\\n2020. Do language embeddings capture scales?\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 292–299, Online.\\nAssociation for Computational Linguistics.\\nYian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman.\\n2021.\\nWhen do you need billions of words of pretraining data?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1112–1125, Online.\\nAssociation for Computational Linguistics.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\n2023.\\nJudging llm-as-a-judge with mt-bench and chatbot arena.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\\n2023.\\nLIMA: less is more for alignment.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nZining Zhu, Soroosh Shahtalebi, and Frank Rudzicz.\\n2022a.\\nPredicting fine-tuning performance with probing.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11534–11547, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nZining Zhu, Jixuan Wang, Bai Li, and Frank Rudzicz.\\n2022b.\\nOn the data requirements of probing.\\nIn Findings of the Association for Computational Linguistics: ACL 2022, pages 4132–4147, Dublin, Ireland.\\nAssociation for Computational Linguistics.',\n",
       "   'title': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "   'chunks': [{'text': 'Long Papers), pages 2126–2136, Melbourne, Australia.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.\\n2023.\\nFree dolly: Introducing the world’s first truly open instructiontuned llm.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei.\\n2009.\\nImagenet: A large-scale hierarchical image database.\\n2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\n2019.\\nBERT: Pre-training of deep bidirectional transformers for language under- standing.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg.\\n2021.\\nAmnesic probing: Behavioral explanation with amnesic counterfactuals.\\nTransactions of the Association for Computational Linguistics, 9:160– 175.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Rudolf Franz Flesch.\\n1948.\\nA new readability yardstick.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'The Journal of applied psychology, 32(3):221–233.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'William Gantt, Lelia Glass, and Aaron Steven White.\\n2022.\\nDecomposing and recomposing event structure.\\nTransactions of the Association for Computational Linguistics, 10:17–34.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Vagrant Gautam, Eileen Bingert, D. Zhu, Anne Lauscher, and Dietrich Klakow.\\n2024.\\nRobust pronoun use fidelity with english llms: Are they reasoning, repeating, or just biased?\\nCoRR, abs/2404.03134.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema.\\n2018.\\nUnder the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 240–248, Brussels, Belgium.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Venkata Govindarajan, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nDecomposing generalization: Models of generic, habitual, and episodic statements.\\nTransactions of the Association for Computational Linguistics, 7:501–517.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Zellig S Harris.\\n1954.\\nDistributional structure.\\nWord, 10(2-3):146–162.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Pengcheng He, Jianfeng Gao, and Weizhu Chen.\\n2023.\\nDebertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.\\n2021.\\nDeberta: decoding-enhanced bert with disentangled attention.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.\\n2010.\\nSemEval-2010 task 8: Multiway classification of semantic relations between pairs of nominals.\\nIn Proceedings of the 5th International',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Workshop on Semantic Evaluation, pages 33–38, Uppsala, Sweden.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021.\\nMeasuring massive multitask language understanding.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Moshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, Swaminathan Sundararaman, and Danny Harnik.\\n2024.\\nLossless and nearlossless compression for foundation models.\\nCoRR, abs/2404.15198.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'John Hewitt and Percy Liang.\\n2019.\\nDesigning and interpreting probes with control tasks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–2743, Hong Kong, China.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'John Hewitt and Christopher D. Manning.\\n2019.\\nA structural probe for finding syntax in word representations.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Yufang Hou.\\n2018.\\nEnhanced word representations for bridging anaphora resolution.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 1–7, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Yufang Hou.\\n2020.\\nBridging anaphora resolution as question answering.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1428–1438, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Jennifer Hu and Roger Levy.\\n2023.\\nPrompting is not a substitute for probability measurements in large language models.\\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5040–5060, Singapore.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Philip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth.\\n2021.\\nBabyBERTa: Learning more grammar with small-scale child-directed language.\\nIn Proceedings of the 25th Conference on Computational Natural Language Learning, pages 624–646, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Alexander Immer, Lucas Torroba Hennigen, Vincent Fortuin, and Ryan Cotterell.\\n2022.\\nProbing as quantifying inductive bias.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1839– 1851, Dublin, Ireland.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2023.\\nMistral 7b.\\nCoRR, abs/2310.06825.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2024.\\nMixtral of experts.\\nCoRR, abs/2401.04088.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Josef Klafka and Allyson Ettinger.\\n2020.\\nSpying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4801–4811, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Arne Köhn.\\n2015.\\nWhat’s in an embedding?\\nanalyzing word embeddings through multilingual evaluation.\\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073, Lisbon, Portugal.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Natalia Konstantinova, Sheila C.M.\\nde Sousa, Noa P. Cruz, Manuel J. Maña, Maite Taboada, and Ruslan Mitkov.\\n2012.\\nA review corpus annotated for negation, speculation and their scope.\\nIn Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 3190–3195, Istanbul, Turkey.\\nEuropean Language Resources Association (ELRA).',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Fajri Koto, Jey Han Lau, and Timothy Baldwin.\\n2021.\\nDiscourse probing of pretrained language models.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3849–3864, Online.\\nAssociation for Computational Linguistics.\\nKatarzyna Krasnowska-Kieraś and Alina Wróblewska.\\n2019.\\nEmpirical linguistic study of sentence embeddings.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5729–5739, Florence, Italy.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Murathan Kurfalı and Robert Östling.\\n2021.\\nProbing multilingual language models for discourse.\\nIn',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 8–19, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\\n2020.\\nALBERT: A lite BERT for self-supervised learning of language representations.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Anne Lauscher, Debora Nozza, Ehm Miltersen, Archie Crowley, and Dirk Hovy.\\n2023.\\nWhat about “em”?\\nhow commercial machine translation fails to handle (neo-)pronouns.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 377–392, Toronto, Canada.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020.\\nBART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana AcostaNavas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.\\n2023.\\nHolistic evaluation of language models.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Transactions on Machine Learning Research.\\nFeatured Certification, Expert Certification.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\\n2016.\\nAssessing the ability of LSTMs to learn syntaxsensitive dependencies.\\nTransactions of the Association for Computational Linguistics, 4:521–535.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019.\\nRoberta: A robustly optimized BERT pretraining approach.\\nCoRR, abs/1907.11692.\\nIlya Loshchilov and Frank Hutter.\\n2019.\\nDecoupled weight decay regularization.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenRe- view.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych.\\n2023.\\nAre emergent abilities in large language models just in-context learning?\\nCoRR, abs/2309.01809.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko.\\n2024.\\nDissociating language and thought in large language models.\\nTrends in Cognitive Sciences.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'George A. Miller.\\n1995.\\nWordnet: A lexical database for english.\\nCommunications of the ACM, 38(11):39– 41.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.\\n2022.\\nRethinking the role of demonstrations:',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'What makes in-context learning work?\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048–11064, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andrés Codas, Clarisse Simões, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah.\\n2023.\\nOrca 2: Teaching small language models how to reason.\\nCoRR, abs/2311.11045.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky.\\n2024.\\nState of what art? A call for multi-prompt LLM evaluation.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'CoRR, abs/2401.00595.\\nMichael Mohler, Mary Brunson, Bryan Rink, and Marc Tomlinson.\\n2016.\\nIntroducing the LCC metaphor datasets.\\nIn Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 4221–4227, Portorož, Slovenia.\\nEuropean Language Resources Association (ELRA).',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Roser Morante and Eduardo Blanco.\\n2012.\\n*SEM 2012 shared task: Resolving the scope and focus of negation.\\nIn *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 265–274, Montréal, Canada.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Marius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020a.\\nOn the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers.\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 68–82, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Marius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020b.\\nOn the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2502–2516, Online.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Association for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers.\\n2023.\\nMTEB: Massive text embedding benchmark.\\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014–2037, Dubrovnik, Croatia.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Shashi Narayan, Shay B. Cohen, and Mirella Lapata.\\n2018.\\nDon’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for extreme summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Allen Nie, Erin Bennett, and Noah Goodman.\\n2019.\\nDisSent: Learning sentence representations from explicit discourse relations.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4497–4510, Florence, Italy.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\\n2020.\\nAdversarial NLI: A new benchmark for natural language understanding.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885–4901, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022.\\nTraining language models to follow instructions with human feedback.\\nIn Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Gustavo Paetzold and Lucia Specia.\\n2016.\\nSemEval 2016 task 11: Complex word identification.\\nIn Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 560–569, San Diego, California.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Onkar Pandit and Yufang Hou.\\n2021.\\nProbing for bridging inference in transformer language models.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4153–4163, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Jeffrey Pennington, Richard Socher, and Christopher Manning.\\n2014.\\nGloVe: Global vectors for word representation.\\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen.\\n2023.\\nEfficient benchmarking (of language models).\\nCoRR, abs/2308.11696.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Fabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel.\\n2020.\\nHow context affects language models’ factual predictions.\\nIn Conference on Automated Knowledge Base Construction, AKBC 2020, Virtual, June 22-24, 2020.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller.\\n2019.\\nLanguage models as knowledge bases?\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2463–2473.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell.\\n2020.\\nInformation-theoretic probing for linguistic structure.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4609–4622, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\n2019.\\nLanguage models are unsupervised multitask learners.\\nOpenAI blog, 1(8):9.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\\n2020.\\nExploring the limits of transfer learning with a unified text-to-text transformer.\\nJournal of Machine Learning Research, 21(140):1–67.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, and Jordan BoydGraber.\\n2021.\\nEvaluation examples are not equally informative: How should that change NLP leaderboards?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4486–4503, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Rachel Rudinger, Adam Teichert, Ryan Culkin, Sheng Zhang, and Benjamin Van Durme.\\n2018a.\\nNeuralDavidsonian semantic proto-role labeling.\\nIn Pro-',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 944–955, Brussels, Belgium.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Rachel Rudinger, Aaron Steven White, and Benjamin Van Durme.\\n2018b.\\nNeural models of factuality.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 731–744, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Naomi Shapiro, Amandalynne Paullada, and Shane Steinert-Threlkeld.\\n2021.\\nA multilabel approach to morphosyntactic probing.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4486–4524, Punta Cana, Dominican Republic.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Pratyusha Sharma, Jordan T. Ash, and Dipendra Misra.\\n2023.\\nThe truth is in there: Improving reasoning in language models with layer-selective rank reduction.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'CoRR, abs/2312.13558.\\nXing Shi, Inkit Padhi, and Kevin Knight.\\n2016.\\nDoes string-based neural MT learn source syntax?\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526– 1534, Austin, Texas.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Natalia Silveira, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning.\\n2014.\\nA gold standard dependency corpus for English.\\nIn Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2897– 2904, Reykjavik, Iceland.\\nEuropean Language Resources Association (ELRA).',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts.\\n2013.\\nRecursive deep models for semantic compositionality over a sentiment treebank.\\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al.\\n2022.\\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'CoRR, abs/2206.04615.\\nGerard J Steen, Aletta G Dorst, J Berenike Herrmann, Anna A Kaal, Tina Krennmayr, Tryntje Pasma, et al.\\n2010.\\nA method for linguistic metaphor identification.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Converging evidence in language and communication research.\\nJohn Benjamins Publishing Company Amsterdam.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, and Akash Srivastava.\\n2024.\\nLAB: large-scale alignment for chatbots.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'CoRR, abs/2403.01081.\\nGyörgy Szarvas, Veronika Vincze, Richárd Farkas, and János Csirik.\\n2008.\\nThe BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts.\\nIn Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 38–45, Columbus, Ohio.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant.\\n2020. oLMpics-On What Language Model Pre-training Captures.\\nTransactions of the Association for Computational Linguistics, 8:743–758.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler.\\n2023.\\nUL2: unifying language learning paradigms.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Ian Tenney, Dipanjan Das, and Ellie Pavlick.\\n2019a.\\nBERT rediscovers the classical NLP pipeline.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick.\\n2019b.\\nWhat do you learn from context?\\nprobing for sentence structure in contextualized word representations.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych.\\n2021.\\nBEIR:',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'A heterogeneous benchmark for zero-shot evaluation of information retrieval models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Lucas Torroba Hennigen, Adina Williams, and Ryan Cotterell.\\n2020.\\nIntrinsic probing through dimension selection.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 197–216, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\n2023.\\nLlama 2: Open foundation and finetuned chat models.\\nCoRR, abs/2307.09288.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Teemu Vahtola, Mathias Creutz, and Jörg Tiedemann.\\n2022.\\nIt is not easy to detect paraphrases: Analysing semantic similarity with antonyms and negation using the new SemAntoNeg benchmark.\\nIn Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 249–262, Abu Dhabi, United Arab Emirates (Hybrid).\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Siddharth Vashishtha, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nFine-grained temporal relation extraction.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2906–2919, Florence, Italy.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Sara Veldhoen, Dieuwke Hupkes, and Willem H. Zuidema.\\n2016.\\nDiagnostic classifiers revealing how neural networks process hierarchical structure.\\nIn Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings.\\nCEUR-WS.org.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Elena Voita and Ivan Titov.\\n2020.\\nInformation-theoretic probing with minimum description length.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183–196, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Andreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024a.\\nDive into the chasm: Probing the gap between in- and cross-topic generalization.\\nIn Findings of the Association for Computational Linguistics: EACL 2024, pages 2197–2214, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Andreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024b.\\nHow to handle different types of out-ofdistribution scenarios in computational argumentation?\\na comprehensive and fine-grained field study.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'CoRR, abs/2309.08316.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\n2019a.\\nSuperglue: A stickier benchmark for general-purpose language understanding systems.\\nIn Advances in Neural Information Processing Systems, volume 32.\\nCurran Associates, Inc. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\n2019b.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'OpenReview.net.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li.\\n2021.\\nAdversarial GLUE: A multitask benchmark for robustness evaluation of language models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\\n2023.\\nHow far can camels go?\\nexploring the state of instruction tuning on open resources.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen.\\n2022.\\nSuper-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085–5109, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 16,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman.\\n2020.\\nBLiMP: The benchmark of linguistic minimal pairs for English.\\nTransactions of the Association for Computational Linguistics, 8:377– 392.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Bonnie Webber, Rashmi Prasad, Alan Lee, and Aravind Joshi.\\n2019.\\nThe penn discourse treebank 3.0 annotation manual.\\nPhiladelphia, University of Pennsylvania.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al.\\n2013.\\nOntonotes release 5.0.\\nLinguistic Data Consortium, Philadelphia, PA, 23:170.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Aaron Steven White, Drew Reisinger, Keisuke Sakaguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme.\\n2016.\\nUniversal decompositional semantics on Universal Dependencies.\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1713–1723, Austin, Texas.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu.\\n2020.\\nPerturbed masking: Parameter-free probing for analyzing and interpreting BERT.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4166–4176, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.\\n2023.\\nWizardlm: Empowering large language models to follow complex instructions.\\nCoRR, abs/2304.12244.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Prateek Yadav, Leshem Choshen, Colin Raffel, and Mohit Bansal.\\n2023.\\nCompeft: Compression for communicating parameter efficient updates via sparsification and quantization.\\nCoRR, abs/2311.13171.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang.\\n2023.\\nGLUE-X: Evaluating natural language understanding models from an out-ofdistribution generalization perspective.\\nIn Findings of the Association for Computational Linguistics:',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'ACL 2023, pages 12731–12750, Toronto, Canada.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Amir Zeldes.\\n2017.\\nThe GUM corpus: Creating multilayer resources in the classroom.\\nLanguage Resources and Evaluation, 51(3):581–612.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth.\\n2020. Do language embeddings capture scales?\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 292–299, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Yian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman.\\n2021.\\nWhen do you need billions of words of pretraining data?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1112–1125, Online.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\n2023.\\nJudging llm-as-a-judge with mt-bench and chatbot arena.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\\n2023.\\nLIMA: less is more for alignment.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Zining Zhu, Soroosh Shahtalebi, and Frank Rudzicz.\\n2022a.\\nPredicting fine-tuning performance with probing.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11534–11547, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'},\n",
       "    {'text': 'Zining Zhu, Jixuan Wang, Bai Li, and Frank Rudzicz.\\n2022b.\\nOn the data requirements of probing.\\nIn Findings of the Association for Computational Linguistics: ACL 2022, pages 4132–4147, Dublin, Ireland.\\nAssociation for Computational Linguistics.',\n",
       "     'title': 'References > Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:',\n",
       "     'page': 17,\n",
       "     'source_doc': 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:'}],\n",
       "   'tables': []},\n",
       "  {'text': 'A Additional Details of Holmes\\nA.1 Additional Details on the Evolution of Probing Literature\\nWe analyze publication trends by year and venue as shown in Table 2.\\nLess work was published between 2015-2018 (earlier) focusing on LSTMbased (Linzen et al., 2016; Conneau et al., 2018) and static LMs (Köhn, 2015; Linzen et al., 2016;\\nBelinkov et al., 2017; Conneau et al., 2018).\\nWith the release of BERT (Devlin et al., 2019) in 2019, we note increasing attention to analyzing linguistic abilities within LMs, with a peak of 90 papers in 2022.4 Considering the venue, more than half of the relevant work (149 papers) was published at major conferences (ACL and EMNLP), and 68 papers were published at AACL, EACL, NAACL, and COLING.5 In addition, we observe a constant contribution of TACL, various workshops, such as Analyzing and Interpreting Neural Networks for NLP or Representation Learning for NLP.\\nA.2 Experimental Details\\nProbing Hyperparameters Following previous work (Hewitt and Liang, 2019; Voita and Titov, 2020), we use fixed hyperparameters for training the probes: 20 epochs, where we find the best one using dev instances; AdamW (Loshchilov and Hutter, 2019) as optimizer; a batch size of 64; a learning rate of 0.0005; a dropout rate of 0.2; a warmup rate of 10% of the steps; random seeds: [0, 1, 2, 3, 4] Hardware We run all of our experiments using 12 Nvidia RTX A6000 GPUs.\\nEvery GPU provides 48GB of memory and 10752 CUDA Cores.\\nConsidered LMs Table 8 outlines the details of the LMs we evaluate on Holmes in this work.\\nA.3 Linguistic Task Categorization\\nWe show in Table 3, Table 4, Table 7, Table 5, and Table 6 which resources Holmes use to cover morphology, syntax, semantics, reasoning, and discourse phenomena.\\nThis includes 33 works providing the data, the specific linguistic phenomena, or both.\\nFor example, for readability we use the data of Weischedel et al.\\n(2013) and calculated the flesch score (Flesch, 1948).\\n4Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-analysis.\\n5Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.\\n |  | earlier 2019 | 2020 | 2021 | 2022 | 2023 | Total\\n | --- | --- | --- | --- | --- | --- | ---\\n | ACL | 2 | 10 | 12 | 9 | 34 | 25 | 92\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | AACL | - | - | - | - | 1 | - | 1\\n | COLING | - | - | 10 | - | 9 | - | 19\\n | EACL | - | - | - | 7 | - | 15 | 22\\n | EMNLP | 2 | 4 | 13 | 17 | 21 | - | 57\\n | NAACL | - | 3 | - | 9 | 14 | - | 26\\n | TACL | 1 | 1 | 2 | 3 | 3 | 1 | 11\\n | Workshops | 4 | 4 | 10 | 10 | 7 | 1 | 36\\n | Other | 1 | 2 | 1 | 1 | 1 | 4 | 10\\n | Probing | 10 | 24 | 48 | 56 | 90 | 46 | 274\\n\\nAll Papers 8,056 3,111 3,822 4,294 5,133 3,647 28,063\\nTable 2: Evolution of probing studies.\\nNote that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.\\nMorphology First, we feature 19 tasks verifying morphology phenomena: Anaphor agreement, determiner noun agreement, subject-verb agreement and irregular forms (Warstadt et al., 2020; Huebner et al., 2021).\\nSyntax The second group of 75 tasks verifies the following syntax phenomena: Part-of-speech tagging and constituent labeling (Weischedel et al., 2013); dependency labeling (Silveira et al., 2014); bigram-shift, tree-depth, top-constituent-task, and sentence-length (Conneau et al., 2018); subject- & object-number, and deoncausative-inchoative alternation based on Klafka and Ettinger (2020); binding, control/raising, negative polarity item licensing, island-effects, argument-structure, ellipsis, and filler-gap (Warstadt et al., 2020; Huebner et al., 2021).\\nSemantics Third, consider 67 tasks covering semantics phenomena: Named-entity labeling and semantic-role labeling (Weischedel et al., 2013); subject- and object-number, tense, semantic odd man out, word content, and coordination inversion (Conneau et al., 2018); semantic relation classification (Hendrickx et al., 2010); semantic proto-roles (Rudinger et al., 2018a); factuality (Rudinger et al., 2018b); genericity (Govindarajan et al., 2019); event structure (Gantt et al., 2022); time (Vashishtha et al., 2019); word sense (White et al., 2016); sentiment analysis (Socher et al., 2013); object- and subject-animacy, objectand subject-gender, verb-tense, and verb-dynamic Klafka and Ettinger (2020); metaphor (Mohler et al., 2016; Birke and Sarkar, 2006; Steen et al., 2010); complex word identification (Paetzold and Specia, 2016); and passive (Krasnowska-Kieraś and Wróblewska, 2019).\\nIn addition, we derive synonym-/antonym-detection task using WordNet (Miller, 1995) and the texts from OntoNotes v5 Weischedel et al.\\n(2013).\\nReasoning Forth, 19 tasks cover reasoning phenomena: Paraphrasticity with negation and antonyms (Vahtola et al., 2022); negation detection (Szarvas et al., 2008; Konstantinova et al., 2012; Morante and Blanco, 2012); negation-span classification (Szarvas et al., 2008; Konstantinova et al., 2012); negation-correspondence (Szarvas et al., 2008; Konstantinova et al., 2012); speculation detection, speculation-span classification, and\\nspeculation-correspondence (Szarvas et al., 2008); and always-never, age comparison, objects comparison, antonym negation, property conjunction, taxonomy connection, encyclopedic composition, and multi-hop composition (Talmor et al., 2020).\\nDiscourse Finally, Holmes embodies 28 task addressing discourse phenomena: Co-reference resolution Weischedel et al.\\n(2013); bridging (Hou, 2018, 2020; Pandit and Hou, 2021); discourse connective (Nie et al., 2019); sentence order and next-sentence prediction (Narayan et al., 2018); discourse correspondence, discourse order, discourse relation, discourse distance, discourse explicit classes, discourse implicit classes (Webber et al., 2019; Kurfalı and Östling, 2021); and rst-count/-depth/-distance/-relation/-relationgroup/-successively/-type (Carlson et al., 2001; Koto et al., 2021; Kurfalı and Östling, 2021; Zeldes, 2017).\\nA.4 Details of Probing Dataset Composition\\nWhenever possible, we rely on established probing datasets and transform instances into a unified format: 1) an input x which is either one or a pair of span(s) or sentence(s), including the string and an optional starting and ending index in the context c when task type is either a span or span-pair classification; 2) an optional textual context c to encode x, for example the sentence in which a span occurs; and 3) a corresponding label y. If given, we use the original train/dev/test splits.\\nHowever, if this division does not exist, we use a 70/10/20 ratio to form these splits.\\nFurthermore, we adapt the design of some tasks to map to our task format.\\nExemplary, for the oLMmpics (Talmor et al., 2020) dataset, we transform the mask-filling tasks into a binary classification where the correct label corresponds to a sentence with a correctly filled mask incorrect to a sentence where the mask was filled wrongly.\\nOnToNotes Following Tenney et al.\\n(2019b,a), we use the OntoNotes (Weischedel et al., 2013) dataset to derive part-of-speech tagging, constituent labeling, named-entity labeling, semantic role, and co-reference resolution probing datasets.\\nFurther, we consider with constituent maximum depth and constituent node length further properties of the constituent tree this dataset OntoNotes.\\nDependency Corpus As in Tenney et al.\\n(2019b,a), we use Universal Dependencies annotations of the English Web Treebank to form a dependency labeling datasets.\\nContext Probes Presented in Klafka and Ettinger (2020), we compose nine datasets to verify information about context words.\\nBLiMP Dataset Using the data presented in the BLiMP benchmark (Warstadt et al., 2020), we derive 67 probing datasets verifying specific phenomena, like island effect, covering morphology, syntax, and semantics.\\nUnlike the original version, we compose a binary classification task for every phenomenon.\\nPrecisely, whether to accept or reject a given sentence, where rejecting means that the given linguistic phenomena is violated.\\nZorro Dataset As for the BLiMP tasks, we convert the 21 distinct Zorro tasks into a binary classification task on whether a sentence accepts or rejects the given linguistic phenomena is violated.\\nSemEval-2010 Task 8 For semantic relation classification we rely on the dataset of Hendrickx et al.\\n(2010).\\nDecompositional Semantics Initiative The Decompositional Semantics Initiative6 provides a large number of datasets to verify semantic phenomena.\\nApart of the common use semantic protoroles (Rudinger et al., 2018a), we use their collection of works to compose probing datasets for factuality (Rudinger et al., 2018b), genericity (Govindarajan et al., 2019), event structure (Vashishtha et al., 2019), time (Vashishtha et al., 2019), and word sense (White et al., 2016).\\nSentiment Analysis We use the commonly used work of Socher et al.\\n(2013) and form a probing dataset targeting sentiment.\\nMetaphor As in Aghazadeh et al.\\n(2022), we use the data from Mohler et al.\\n(2016); Birke and Sarkar (2006); Steen et al.\\n(2010) to form three metaphor datasets.\\nComplex Word Identification We consider word complexity for the first time and use the data presented in Paetzold and Specia (2016).\\nIt provides annotations for different complexity levels of words.\\nPassive We use data from Krasnowska-Kieraś and Wróblewska (2019) to form a probing dataset assessing knowledge about passive language.\\nSynonym / Antonym Replacement Using the text of the OntoNotes (Weischedel et al., 2013) and Wordnet (Miller, 1995), we form a probing dataset to detect synonym and antonym replacement.\\nSpecifically, the binary classification task is: given two texts (the original and an updated one), was the updated one changed by replacing a word with its synonym or antonym?\\nNegation With this work, we verify for the first time negation based on human annotated datasets (Vahtola et al., 2022; Szarvas et al., 2008; Konstantinova et al., 2012).\\nSpecifically, we form different probing datasets.\\n• Is a text negated or not?\\n• Given two text spans, does the negation within the first one correspond to the second one?\\n• Given a text span, is it the cue or the scope of the negation?\\noLMmpics We form probing datasets addressing different lexical reasoning using the data presented in Talmor et al.\\n(2020).\\nAs they provide multiple choices, we form correct instances by filling the gap with the correct option and wrong ones by filling in the other options.\\nSpecifically, we form dataset for always-never, age comparison, objects comparison, antonym-negation, multi-hop composition property conjunction, taxonomy conjunction, and encyclopedic composition.\\nBridging We rely on the data presented in Pandit and Hou (2021) and form two probing datasets.\\nOne is to verify whether a text is linguistically applicable, considering bridging (antecedent matches anaphora).\\nAnd a second one to verify whether an antecedent and anaphora match.\\nDiscourse Connective Using data from Nie et al.\\n(2019), we form a probing dataset to assess whether a given connective marker matches the discourse of the given text.\\nSentence Order and Next Sentence Prediction Following Narayan et al.\\n(2018), we form two datasets to verify the order of good or badness of a given sentence and whether two sentences occur after each other.\\nDiscourse Representation Theory We use data from Webber et al.\\n(2019) to compose eight probing datasets addressing discourse representation theory:\\n• Four probing dataset predicting the class of a given span.\\nWe distinguish between implicit, explicit, implicit-coarse, and explicit-coarse.\\n• The absolute distance, number of words, between two spans in the text.\\n• Whether the order of two spans is correct or not.\\n• Whether two spans have discourse relation or not.\\n• The specific discourse relation of two spans.\\nRhetorical Structure Theory Using annotations from Carlson et al.\\n(2001); Zeldes (2017), we compose 14 probing datasets addressing rhetorical theory.\\nSpecifically, we compose the following seven types of datasets for both works:\\n• The rhetorical type of a text span, either nucleus or satellite.\\n• The number of children of a text span within the rhetorical tree of the text.\\n• The depth of a text span within the rhetorical tree of the text.\\n• The number of edges between two text spans within the rhetorical tree.\\n• The specific rhetorical relation between two text spans like conclusion.\\n• The relation group of a specific rhetorical relation between two text spans like evaluation for the relation conclusion.\\n• Whether two text spans occur after each other in the rhetorical tree.\\nPhenomena\\n | anaphor agreement | 3 | ✓ | ✓\\n | --- | --- | --- | ---\\n | determiner noun agreement | 10 | ✓ | ✓\\n | irregular forms | 3 | ✓ | ✓\\n | subject-verb agreement | 10 | ✓ | ✓\\n\\nTable 3: Overview of resources and linguistic phenomena mapping for morphology.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n\\nTable 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n\\nTable 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n\\nTable 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n\\nTable 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.\\n | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n\\nTable 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       "   'title': 'A Additional Details of Holmes',\n",
       "   'chunks': [{'text': 'We analyze publication trends by year and venue as shown in Table 2.\\nLess work was published between 2015-2018 (earlier) focusing on LSTMbased (Linzen et al., 2016; Conneau et al., 2018) and static LMs (Köhn, 2015; Linzen et al., 2016;',\n",
       "     'title': 'A Additional Details of Holmes > A.1 Additional Details on the Evolution of Probing Literature',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Belinkov et al., 2017; Conneau et al., 2018).\\nWith the release of BERT (Devlin et al., 2019) in 2019, we note increasing attention to analyzing linguistic abilities within LMs, with a peak of 90 papers in 2022.4 Considering the venue, more than half of the relevant work (149 papers) was published at major conferences (ACL and EMNLP), and 68 papers were published at AACL, EACL, NAACL, and COLING.5 In addition, we observe a constant contribution of TACL, various workshops, such as Analyzing and Interpreting Neural Networks for NLP or Representation Learning for NLP.',\n",
       "     'title': 'A Additional Details of Holmes > A.1 Additional Details on the Evolution of Probing Literature',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Probing Hyperparameters Following previous work (Hewitt and Liang, 2019; Voita and Titov, 2020), we use fixed hyperparameters for training the probes: 20 epochs, where we find the best one using dev instances; AdamW (Loshchilov and Hutter, 2019) as optimizer; a batch size of 64; a learning rate of 0.0005; a dropout rate of 0.2; a warmup rate of 10% of the steps; random seeds: [0, 1, 2, 3, 4] Hardware We run all of our experiments using 12 Nvidia RTX A6000 GPUs.\\nEvery GPU provides 48GB of memory and 10752 CUDA Cores.',\n",
       "     'title': 'A Additional Details of Holmes > A.2 Experimental Details',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Considered LMs Table 8 outlines the details of the LMs we evaluate on Holmes in this work.',\n",
       "     'title': 'A Additional Details of Holmes > A.2 Experimental Details',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'We show in Table 3, Table 4, Table 7, Table 5, and Table 6 which resources Holmes use to cover morphology, syntax, semantics, reasoning, and discourse phenomena.\\nThis includes 33 works providing the data, the specific linguistic phenomena, or both.\\nFor example, for readability we use the data of Weischedel et al.\\n(2013) and calculated the flesch score (Flesch, 1948).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '4Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-analysis.',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '5Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': ' |  | earlier 2019 | 2020 | 2021 | 2022 | 2023 | Total\\n | --- | --- | --- | --- | --- | --- | ---\\n | ACL | 2 | 10 | 12 | 9 | 34 | 25 | 92\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | AACL | - | - | - | - | 1 | - | 1\\n | COLING | - | - | 10 | - | 9 | - | 19\\n | EACL | - | - | - | 7 | - | 15 | 22\\n | EMNLP | 2 | 4 | 13 | 17 | 21 | - | 57\\n | NAACL | - | 3 | - | 9 | 14 | - | 26\\n | TACL | 1 | 1 | 2 | 3 | 3 | 1 | 11\\n | Workshops | 4 | 4 | 10 | 10 | 7 | 1 | 36\\n | Other | 1 | 2 | 1 | 1 | 1 | 4 | 10\\n | Probing | 10 | 24 | 48 | 56 | 90 | 46 | 274\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'All Papers 8,056 3,111 3,822 4,294 5,133 3,647 28,063',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Table 2: Evolution of probing studies.\\nNote that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Morphology First, we feature 19 tasks verifying morphology phenomena: Anaphor agreement, determiner noun agreement, subject-verb agreement and irregular forms (Warstadt et al., 2020; Huebner et al., 2021).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Syntax The second group of 75 tasks verifies the following syntax phenomena: Part-of-speech tagging and constituent labeling (Weischedel et al., 2013); dependency labeling (Silveira et al., 2014); bigram-shift, tree-depth, top-constituent-task, and sentence-length (Conneau et al., 2018); subject- & object-number, and deoncausative-inchoative alternation based on Klafka and Ettinger (2020); binding, control/raising, negative polarity item licensing, island-effects, argument-structure, ellipsis, and filler-gap (Warstadt et al., 2020; Huebner et al., 2021).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Semantics Third, consider 67 tasks covering semantics phenomena: Named-entity labeling and semantic-role labeling (Weischedel et al., 2013); subject- and object-number, tense, semantic odd man out, word content, and coordination inversion (Conneau et al., 2018); semantic relation classification (Hendrickx et al., 2010); semantic proto-roles (Rudinger et al., 2018a); factuality (Rudinger et al., 2018b); genericity (Govindarajan et al., 2019); event structure (Gantt et al., 2022); time (Vashishtha et al., 2019); word sense (White et al., 2016); sentiment analysis (Socher et al., 2013); object- and subject-animacy, objectand subject-gender, verb-tense, and verb-dynamic Klafka and Ettinger (2020); metaphor (Mohler et al., 2016; Birke and Sarkar, 2006; Steen et al., 2010); complex word identification (Paetzold and Specia, 2016); and passive (Krasnowska-Kieraś and Wróblewska, 2019).\\nIn addition, we derive synonym-/antonym-detection task using WordNet (Miller, 1995) and the texts from OntoNotes v5 Weischedel et al.\\n(2013).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Reasoning Forth, 19 tasks cover reasoning phenomena: Paraphrasticity with negation and antonyms (Vahtola et al., 2022); negation detection (Szarvas et al., 2008; Konstantinova et al., 2012; Morante and Blanco, 2012); negation-span classification (Szarvas et al., 2008; Konstantinova et al., 2012); negation-correspondence (Szarvas et al., 2008; Konstantinova et al., 2012); speculation detection, speculation-span classification, and',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'speculation-correspondence (Szarvas et al., 2008); and always-never, age comparison, objects comparison, antonym negation, property conjunction, taxonomy connection, encyclopedic composition, and multi-hop composition (Talmor et al., 2020).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Discourse Finally, Holmes embodies 28 task addressing discourse phenomena: Co-reference resolution Weischedel et al.\\n(2013); bridging (Hou, 2018, 2020; Pandit and Hou, 2021); discourse connective (Nie et al., 2019); sentence order and next-sentence prediction (Narayan et al., 2018); discourse correspondence, discourse order, discourse relation, discourse distance, discourse explicit classes, discourse implicit classes (Webber et al., 2019; Kurfalı and Östling, 2021); and rst-count/-depth/-distance/-relation/-relationgroup/-successively/-type (Carlson et al., 2001; Koto et al., 2021; Kurfalı and Östling, 2021; Zeldes, 2017).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Whenever possible, we rely on established probing datasets and transform instances into a unified format: 1) an input x which is either one or a pair of span(s) or sentence(s), including the string and an optional starting and ending index in the context c when task type is either a span or span-pair classification; 2) an optional textual context c to encode x, for example the sentence in which a span occurs; and 3) a corresponding label y. If given, we use the original train/dev/test splits.\\nHowever, if this division does not exist, we use a 70/10/20 ratio to form these splits.\\nFurthermore, we adapt the design of some tasks to map to our task format.\\nExemplary, for the oLMmpics (Talmor et al., 2020) dataset, we transform the mask-filling tasks into a binary classification where the correct label corresponds to a sentence with a correctly filled mask incorrect to a sentence where the mask was filled wrongly.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'OnToNotes Following Tenney et al.\\n(2019b,a), we use the OntoNotes (Weischedel et al., 2013) dataset to derive part-of-speech tagging, constituent labeling, named-entity labeling, semantic role, and co-reference resolution probing datasets.\\nFurther, we consider with constituent maximum depth and constituent node length further properties of the constituent tree this dataset OntoNotes.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Dependency Corpus As in Tenney et al.\\n(2019b,a), we use Universal Dependencies annotations of the English Web Treebank to form a dependency labeling datasets.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Context Probes Presented in Klafka and Ettinger (2020), we compose nine datasets to verify information about context words.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'BLiMP Dataset Using the data presented in the BLiMP benchmark (Warstadt et al., 2020), we derive 67 probing datasets verifying specific phenomena, like island effect, covering morphology, syntax, and semantics.\\nUnlike the original version, we compose a binary classification task for every phenomenon.\\nPrecisely, whether to accept or reject a given sentence, where rejecting means that the given linguistic phenomena is violated.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Zorro Dataset As for the BLiMP tasks, we convert the 21 distinct Zorro tasks into a binary classification task on whether a sentence accepts or rejects the given linguistic phenomena is violated.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'SemEval-2010 Task 8 For semantic relation classification we rely on the dataset of Hendrickx et al.\\n(2010).',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Decompositional Semantics Initiative The Decompositional Semantics Initiative6 provides a large number of datasets to verify semantic phenomena.\\nApart of the common use semantic protoroles (Rudinger et al., 2018a), we use their collection of works to compose probing datasets for factuality (Rudinger et al., 2018b), genericity (Govindarajan et al., 2019), event structure (Vashishtha et al., 2019), time (Vashishtha et al., 2019), and word sense (White et al., 2016).',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Sentiment Analysis We use the commonly used work of Socher et al.\\n(2013) and form a probing dataset targeting sentiment.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Metaphor As in Aghazadeh et al.\\n(2022), we use the data from Mohler et al.\\n(2016); Birke and Sarkar (2006); Steen et al.\\n(2010) to form three metaphor datasets.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Complex Word Identification We consider word complexity for the first time and use the data presented in Paetzold and Specia (2016).\\nIt provides annotations for different complexity levels of words.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Passive We use data from Krasnowska-Kieraś and Wróblewska (2019) to form a probing dataset assessing knowledge about passive language.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Synonym / Antonym Replacement Using the text of the OntoNotes (Weischedel et al., 2013) and Wordnet (Miller, 1995), we form a probing dataset to detect synonym and antonym replacement.\\nSpecifically, the binary classification task is: given two texts (the original and an updated one), was the updated one changed by replacing a word with its synonym or antonym?\\nNegation With this work, we verify for the first time negation based on human annotated datasets (Vahtola et al., 2022; Szarvas et al., 2008; Konstantinova et al., 2012).\\nSpecifically, we form different probing datasets.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• Is a text negated or not?',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• Given two text spans, does the negation within the first one correspond to the second one?',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• Given a text span, is it the cue or the scope of the negation?',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'oLMmpics We form probing datasets addressing different lexical reasoning using the data presented in Talmor et al.\\n(2020).\\nAs they provide multiple choices, we form correct instances by filling the gap with the correct option and wrong ones by filling in the other options.\\nSpecifically, we form dataset for always-never, age comparison, objects comparison, antonym-negation, multi-hop composition property conjunction, taxonomy conjunction, and encyclopedic composition.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Bridging We rely on the data presented in Pandit and Hou (2021) and form two probing datasets.\\nOne is to verify whether a text is linguistically applicable, considering bridging (antecedent matches anaphora).\\nAnd a second one to verify whether an antecedent and anaphora match.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Discourse Connective Using data from Nie et al.\\n(2019), we form a probing dataset to assess whether a given connective marker matches the discourse of the given text.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Sentence Order and Next Sentence Prediction Following Narayan et al.\\n(2018), we form two datasets to verify the order of good or badness of a given sentence and whether two sentences occur after each other.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Discourse Representation Theory We use data from Webber et al.\\n(2019) to compose eight probing datasets addressing discourse representation theory:',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• Four probing dataset predicting the class of a given span.\\nWe distinguish between implicit, explicit, implicit-coarse, and explicit-coarse.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• The absolute distance, number of words, between two spans in the text.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• Whether the order of two spans is correct or not.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• Whether two spans have discourse relation or not.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• The specific discourse relation of two spans.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Rhetorical Structure Theory Using annotations from Carlson et al.\\n(2001); Zeldes (2017), we compose 14 probing datasets addressing rhetorical theory.\\nSpecifically, we compose the following seven types of datasets for both works:',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• The rhetorical type of a text span, either nucleus or satellite.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• The number of children of a text span within the rhetorical tree of the text.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• The depth of a text span within the rhetorical tree of the text.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• The number of edges between two text spans within the rhetorical tree.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• The specific rhetorical relation between two text spans like conclusion.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• The relation group of a specific rhetorical relation between two text spans like evaluation for the relation conclusion.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': '• Whether two text spans occur after each other in the rhetorical tree.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': ' | anaphor agreement | 3 | ✓ | ✓\\n | --- | --- | --- | ---\\n | determiner noun agreement | 10 | ✓ | ✓\\n | irregular forms | 3 | ✓ | ✓\\n | subject-verb agreement | 10 | ✓ | ✓\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Table 3: Overview of resources and linguistic phenomena mapping for morphology.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': ' | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Table 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': ' | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Table 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': ' | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Table 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': ' | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 22,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Table 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 22,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': ' | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 23,\n",
       "     'source_doc': 'A Additional Details of Holmes'},\n",
       "    {'text': 'Table 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 23,\n",
       "     'source_doc': 'A Additional Details of Holmes'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316c99c90>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316c9b910>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316c9bdf0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cbdb10>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cbe890>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cbf4c0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cdfd90>]},\n",
       "  {'text': 'A.1 Additional Details on the Evolution of Probing Literature\\nWe analyze publication trends by year and venue as shown in Table 2.\\nLess work was published between 2015-2018 (earlier) focusing on LSTMbased (Linzen et al., 2016; Conneau et al., 2018) and static LMs (Köhn, 2015; Linzen et al., 2016;\\nBelinkov et al., 2017; Conneau et al., 2018).\\nWith the release of BERT (Devlin et al., 2019) in 2019, we note increasing attention to analyzing linguistic abilities within LMs, with a peak of 90 papers in 2022.4 Considering the venue, more than half of the relevant work (149 papers) was published at major conferences (ACL and EMNLP), and 68 papers were published at AACL, EACL, NAACL, and COLING.5 In addition, we observe a constant contribution of TACL, various workshops, such as Analyzing and Interpreting Neural Networks for NLP or Representation Learning for NLP.',\n",
       "   'title': 'A.1 Additional Details on the Evolution of Probing Literature',\n",
       "   'chunks': [{'text': 'We analyze publication trends by year and venue as shown in Table 2.\\nLess work was published between 2015-2018 (earlier) focusing on LSTMbased (Linzen et al., 2016; Conneau et al., 2018) and static LMs (Köhn, 2015; Linzen et al., 2016;',\n",
       "     'title': 'A Additional Details of Holmes > A.1 Additional Details on the Evolution of Probing Literature',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.1 Additional Details on the Evolution of Probing Literature'},\n",
       "    {'text': 'Belinkov et al., 2017; Conneau et al., 2018).\\nWith the release of BERT (Devlin et al., 2019) in 2019, we note increasing attention to analyzing linguistic abilities within LMs, with a peak of 90 papers in 2022.4 Considering the venue, more than half of the relevant work (149 papers) was published at major conferences (ACL and EMNLP), and 68 papers were published at AACL, EACL, NAACL, and COLING.5 In addition, we observe a constant contribution of TACL, various workshops, such as Analyzing and Interpreting Neural Networks for NLP or Representation Learning for NLP.',\n",
       "     'title': 'A Additional Details of Holmes > A.1 Additional Details on the Evolution of Probing Literature',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.1 Additional Details on the Evolution of Probing Literature'}],\n",
       "   'tables': []},\n",
       "  {'text': 'A.2 Experimental Details\\nProbing Hyperparameters Following previous work (Hewitt and Liang, 2019; Voita and Titov, 2020), we use fixed hyperparameters for training the probes: 20 epochs, where we find the best one using dev instances; AdamW (Loshchilov and Hutter, 2019) as optimizer; a batch size of 64; a learning rate of 0.0005; a dropout rate of 0.2; a warmup rate of 10% of the steps; random seeds: [0, 1, 2, 3, 4] Hardware We run all of our experiments using 12 Nvidia RTX A6000 GPUs.\\nEvery GPU provides 48GB of memory and 10752 CUDA Cores.\\nConsidered LMs Table 8 outlines the details of the LMs we evaluate on Holmes in this work.',\n",
       "   'title': 'A.2 Experimental Details',\n",
       "   'chunks': [{'text': 'Probing Hyperparameters Following previous work (Hewitt and Liang, 2019; Voita and Titov, 2020), we use fixed hyperparameters for training the probes: 20 epochs, where we find the best one using dev instances; AdamW (Loshchilov and Hutter, 2019) as optimizer; a batch size of 64; a learning rate of 0.0005; a dropout rate of 0.2; a warmup rate of 10% of the steps; random seeds: [0, 1, 2, 3, 4] Hardware We run all of our experiments using 12 Nvidia RTX A6000 GPUs.\\nEvery GPU provides 48GB of memory and 10752 CUDA Cores.',\n",
       "     'title': 'A Additional Details of Holmes > A.2 Experimental Details',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.2 Experimental Details'},\n",
       "    {'text': 'Considered LMs Table 8 outlines the details of the LMs we evaluate on Holmes in this work.',\n",
       "     'title': 'A Additional Details of Holmes > A.2 Experimental Details',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.2 Experimental Details'}],\n",
       "   'tables': []},\n",
       "  {'text': 'A.3 Linguistic Task Categorization\\nWe show in Table 3, Table 4, Table 7, Table 5, and Table 6 which resources Holmes use to cover morphology, syntax, semantics, reasoning, and discourse phenomena.\\nThis includes 33 works providing the data, the specific linguistic phenomena, or both.\\nFor example, for readability we use the data of Weischedel et al.\\n(2013) and calculated the flesch score (Flesch, 1948).\\n4Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-analysis.\\n5Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.\\n |  | earlier 2019 | 2020 | 2021 | 2022 | 2023 | Total\\n | --- | --- | --- | --- | --- | --- | ---\\n | ACL | 2 | 10 | 12 | 9 | 34 | 25 | 92\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | AACL | - | - | - | - | 1 | - | 1\\n | COLING | - | - | 10 | - | 9 | - | 19\\n | EACL | - | - | - | 7 | - | 15 | 22\\n | EMNLP | 2 | 4 | 13 | 17 | 21 | - | 57\\n | NAACL | - | 3 | - | 9 | 14 | - | 26\\n | TACL | 1 | 1 | 2 | 3 | 3 | 1 | 11\\n | Workshops | 4 | 4 | 10 | 10 | 7 | 1 | 36\\n | Other | 1 | 2 | 1 | 1 | 1 | 4 | 10\\n | Probing | 10 | 24 | 48 | 56 | 90 | 46 | 274\\n\\nAll Papers 8,056 3,111 3,822 4,294 5,133 3,647 28,063\\nTable 2: Evolution of probing studies.\\nNote that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.\\nMorphology First, we feature 19 tasks verifying morphology phenomena: Anaphor agreement, determiner noun agreement, subject-verb agreement and irregular forms (Warstadt et al., 2020; Huebner et al., 2021).\\nSyntax The second group of 75 tasks verifies the following syntax phenomena: Part-of-speech tagging and constituent labeling (Weischedel et al., 2013); dependency labeling (Silveira et al., 2014); bigram-shift, tree-depth, top-constituent-task, and sentence-length (Conneau et al., 2018); subject- & object-number, and deoncausative-inchoative alternation based on Klafka and Ettinger (2020); binding, control/raising, negative polarity item licensing, island-effects, argument-structure, ellipsis, and filler-gap (Warstadt et al., 2020; Huebner et al., 2021).\\nSemantics Third, consider 67 tasks covering semantics phenomena: Named-entity labeling and semantic-role labeling (Weischedel et al., 2013); subject- and object-number, tense, semantic odd man out, word content, and coordination inversion (Conneau et al., 2018); semantic relation classification (Hendrickx et al., 2010); semantic proto-roles (Rudinger et al., 2018a); factuality (Rudinger et al., 2018b); genericity (Govindarajan et al., 2019); event structure (Gantt et al., 2022); time (Vashishtha et al., 2019); word sense (White et al., 2016); sentiment analysis (Socher et al., 2013); object- and subject-animacy, objectand subject-gender, verb-tense, and verb-dynamic Klafka and Ettinger (2020); metaphor (Mohler et al., 2016; Birke and Sarkar, 2006; Steen et al., 2010); complex word identification (Paetzold and Specia, 2016); and passive (Krasnowska-Kieraś and Wróblewska, 2019).\\nIn addition, we derive synonym-/antonym-detection task using WordNet (Miller, 1995) and the texts from OntoNotes v5 Weischedel et al.\\n(2013).\\nReasoning Forth, 19 tasks cover reasoning phenomena: Paraphrasticity with negation and antonyms (Vahtola et al., 2022); negation detection (Szarvas et al., 2008; Konstantinova et al., 2012; Morante and Blanco, 2012); negation-span classification (Szarvas et al., 2008; Konstantinova et al., 2012); negation-correspondence (Szarvas et al., 2008; Konstantinova et al., 2012); speculation detection, speculation-span classification, and\\nspeculation-correspondence (Szarvas et al., 2008); and always-never, age comparison, objects comparison, antonym negation, property conjunction, taxonomy connection, encyclopedic composition, and multi-hop composition (Talmor et al., 2020).\\nDiscourse Finally, Holmes embodies 28 task addressing discourse phenomena: Co-reference resolution Weischedel et al.\\n(2013); bridging (Hou, 2018, 2020; Pandit and Hou, 2021); discourse connective (Nie et al., 2019); sentence order and next-sentence prediction (Narayan et al., 2018); discourse correspondence, discourse order, discourse relation, discourse distance, discourse explicit classes, discourse implicit classes (Webber et al., 2019; Kurfalı and Östling, 2021); and rst-count/-depth/-distance/-relation/-relationgroup/-successively/-type (Carlson et al., 2001; Koto et al., 2021; Kurfalı and Östling, 2021; Zeldes, 2017).',\n",
       "   'title': 'A.3 Linguistic Task Categorization',\n",
       "   'chunks': [{'text': 'We show in Table 3, Table 4, Table 7, Table 5, and Table 6 which resources Holmes use to cover morphology, syntax, semantics, reasoning, and discourse phenomena.\\nThis includes 33 works providing the data, the specific linguistic phenomena, or both.\\nFor example, for readability we use the data of Weischedel et al.\\n(2013) and calculated the flesch score (Flesch, 1948).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': '4Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-analysis.',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': '5Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': ' |  | earlier 2019 | 2020 | 2021 | 2022 | 2023 | Total\\n | --- | --- | --- | --- | --- | --- | ---\\n | ACL | 2 | 10 | 12 | 9 | 34 | 25 | 92\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | AACL | - | - | - | - | 1 | - | 1\\n | COLING | - | - | 10 | - | 9 | - | 19\\n | EACL | - | - | - | 7 | - | 15 | 22\\n | EMNLP | 2 | 4 | 13 | 17 | 21 | - | 57\\n | NAACL | - | 3 | - | 9 | 14 | - | 26\\n | TACL | 1 | 1 | 2 | 3 | 3 | 1 | 11\\n | Workshops | 4 | 4 | 10 | 10 | 7 | 1 | 36\\n | Other | 1 | 2 | 1 | 1 | 1 | 4 | 10\\n | Probing | 10 | 24 | 48 | 56 | 90 | 46 | 274\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': 'All Papers 8,056 3,111 3,822 4,294 5,133 3,647 28,063',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': 'Table 2: Evolution of probing studies.\\nNote that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': 'Morphology First, we feature 19 tasks verifying morphology phenomena: Anaphor agreement, determiner noun agreement, subject-verb agreement and irregular forms (Warstadt et al., 2020; Huebner et al., 2021).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': 'Syntax The second group of 75 tasks verifies the following syntax phenomena: Part-of-speech tagging and constituent labeling (Weischedel et al., 2013); dependency labeling (Silveira et al., 2014); bigram-shift, tree-depth, top-constituent-task, and sentence-length (Conneau et al., 2018); subject- & object-number, and deoncausative-inchoative alternation based on Klafka and Ettinger (2020); binding, control/raising, negative polarity item licensing, island-effects, argument-structure, ellipsis, and filler-gap (Warstadt et al., 2020; Huebner et al., 2021).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': 'Semantics Third, consider 67 tasks covering semantics phenomena: Named-entity labeling and semantic-role labeling (Weischedel et al., 2013); subject- and object-number, tense, semantic odd man out, word content, and coordination inversion (Conneau et al., 2018); semantic relation classification (Hendrickx et al., 2010); semantic proto-roles (Rudinger et al., 2018a); factuality (Rudinger et al., 2018b); genericity (Govindarajan et al., 2019); event structure (Gantt et al., 2022); time (Vashishtha et al., 2019); word sense (White et al., 2016); sentiment analysis (Socher et al., 2013); object- and subject-animacy, objectand subject-gender, verb-tense, and verb-dynamic Klafka and Ettinger (2020); metaphor (Mohler et al., 2016; Birke and Sarkar, 2006; Steen et al., 2010); complex word identification (Paetzold and Specia, 2016); and passive (Krasnowska-Kieraś and Wróblewska, 2019).\\nIn addition, we derive synonym-/antonym-detection task using WordNet (Miller, 1995) and the texts from OntoNotes v5 Weischedel et al.\\n(2013).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 18,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': 'Reasoning Forth, 19 tasks cover reasoning phenomena: Paraphrasticity with negation and antonyms (Vahtola et al., 2022); negation detection (Szarvas et al., 2008; Konstantinova et al., 2012; Morante and Blanco, 2012); negation-span classification (Szarvas et al., 2008; Konstantinova et al., 2012); negation-correspondence (Szarvas et al., 2008; Konstantinova et al., 2012); speculation detection, speculation-span classification, and',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': 'speculation-correspondence (Szarvas et al., 2008); and always-never, age comparison, objects comparison, antonym negation, property conjunction, taxonomy connection, encyclopedic composition, and multi-hop composition (Talmor et al., 2020).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'},\n",
       "    {'text': 'Discourse Finally, Holmes embodies 28 task addressing discourse phenomena: Co-reference resolution Weischedel et al.\\n(2013); bridging (Hou, 2018, 2020; Pandit and Hou, 2021); discourse connective (Nie et al., 2019); sentence order and next-sentence prediction (Narayan et al., 2018); discourse correspondence, discourse order, discourse relation, discourse distance, discourse explicit classes, discourse implicit classes (Webber et al., 2019; Kurfalı and Östling, 2021); and rst-count/-depth/-distance/-relation/-relationgroup/-successively/-type (Carlson et al., 2001; Koto et al., 2021; Kurfalı and Östling, 2021; Zeldes, 2017).',\n",
       "     'title': 'A Additional Details of Holmes > A.3 Linguistic Task Categorization',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.3 Linguistic Task Categorization'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316c99c90>]},\n",
       "  {'text': 'A.4 Details of Probing Dataset Composition\\nWhenever possible, we rely on established probing datasets and transform instances into a unified format: 1) an input x which is either one or a pair of span(s) or sentence(s), including the string and an optional starting and ending index in the context c when task type is either a span or span-pair classification; 2) an optional textual context c to encode x, for example the sentence in which a span occurs; and 3) a corresponding label y. If given, we use the original train/dev/test splits.\\nHowever, if this division does not exist, we use a 70/10/20 ratio to form these splits.\\nFurthermore, we adapt the design of some tasks to map to our task format.\\nExemplary, for the oLMmpics (Talmor et al., 2020) dataset, we transform the mask-filling tasks into a binary classification where the correct label corresponds to a sentence with a correctly filled mask incorrect to a sentence where the mask was filled wrongly.\\nOnToNotes Following Tenney et al.\\n(2019b,a), we use the OntoNotes (Weischedel et al., 2013) dataset to derive part-of-speech tagging, constituent labeling, named-entity labeling, semantic role, and co-reference resolution probing datasets.\\nFurther, we consider with constituent maximum depth and constituent node length further properties of the constituent tree this dataset OntoNotes.\\nDependency Corpus As in Tenney et al.\\n(2019b,a), we use Universal Dependencies annotations of the English Web Treebank to form a dependency labeling datasets.\\nContext Probes Presented in Klafka and Ettinger (2020), we compose nine datasets to verify information about context words.\\nBLiMP Dataset Using the data presented in the BLiMP benchmark (Warstadt et al., 2020), we derive 67 probing datasets verifying specific phenomena, like island effect, covering morphology, syntax, and semantics.\\nUnlike the original version, we compose a binary classification task for every phenomenon.\\nPrecisely, whether to accept or reject a given sentence, where rejecting means that the given linguistic phenomena is violated.\\nZorro Dataset As for the BLiMP tasks, we convert the 21 distinct Zorro tasks into a binary classification task on whether a sentence accepts or rejects the given linguistic phenomena is violated.\\nSemEval-2010 Task 8 For semantic relation classification we rely on the dataset of Hendrickx et al.\\n(2010).\\nDecompositional Semantics Initiative The Decompositional Semantics Initiative6 provides a large number of datasets to verify semantic phenomena.\\nApart of the common use semantic protoroles (Rudinger et al., 2018a), we use their collection of works to compose probing datasets for factuality (Rudinger et al., 2018b), genericity (Govindarajan et al., 2019), event structure (Vashishtha et al., 2019), time (Vashishtha et al., 2019), and word sense (White et al., 2016).\\nSentiment Analysis We use the commonly used work of Socher et al.\\n(2013) and form a probing dataset targeting sentiment.\\nMetaphor As in Aghazadeh et al.\\n(2022), we use the data from Mohler et al.\\n(2016); Birke and Sarkar (2006); Steen et al.\\n(2010) to form three metaphor datasets.\\nComplex Word Identification We consider word complexity for the first time and use the data presented in Paetzold and Specia (2016).\\nIt provides annotations for different complexity levels of words.\\nPassive We use data from Krasnowska-Kieraś and Wróblewska (2019) to form a probing dataset assessing knowledge about passive language.\\nSynonym / Antonym Replacement Using the text of the OntoNotes (Weischedel et al., 2013) and Wordnet (Miller, 1995), we form a probing dataset to detect synonym and antonym replacement.\\nSpecifically, the binary classification task is: given two texts (the original and an updated one), was the updated one changed by replacing a word with its synonym or antonym?\\nNegation With this work, we verify for the first time negation based on human annotated datasets (Vahtola et al., 2022; Szarvas et al., 2008; Konstantinova et al., 2012).\\nSpecifically, we form different probing datasets.\\n• Is a text negated or not?\\n• Given two text spans, does the negation within the first one correspond to the second one?\\n• Given a text span, is it the cue or the scope of the negation?\\noLMmpics We form probing datasets addressing different lexical reasoning using the data presented in Talmor et al.\\n(2020).\\nAs they provide multiple choices, we form correct instances by filling the gap with the correct option and wrong ones by filling in the other options.\\nSpecifically, we form dataset for always-never, age comparison, objects comparison, antonym-negation, multi-hop composition property conjunction, taxonomy conjunction, and encyclopedic composition.\\nBridging We rely on the data presented in Pandit and Hou (2021) and form two probing datasets.\\nOne is to verify whether a text is linguistically applicable, considering bridging (antecedent matches anaphora).\\nAnd a second one to verify whether an antecedent and anaphora match.\\nDiscourse Connective Using data from Nie et al.\\n(2019), we form a probing dataset to assess whether a given connective marker matches the discourse of the given text.\\nSentence Order and Next Sentence Prediction Following Narayan et al.\\n(2018), we form two datasets to verify the order of good or badness of a given sentence and whether two sentences occur after each other.\\nDiscourse Representation Theory We use data from Webber et al.\\n(2019) to compose eight probing datasets addressing discourse representation theory:\\n• Four probing dataset predicting the class of a given span.\\nWe distinguish between implicit, explicit, implicit-coarse, and explicit-coarse.\\n• The absolute distance, number of words, between two spans in the text.\\n• Whether the order of two spans is correct or not.\\n• Whether two spans have discourse relation or not.\\n• The specific discourse relation of two spans.\\nRhetorical Structure Theory Using annotations from Carlson et al.\\n(2001); Zeldes (2017), we compose 14 probing datasets addressing rhetorical theory.\\nSpecifically, we compose the following seven types of datasets for both works:\\n• The rhetorical type of a text span, either nucleus or satellite.\\n• The number of children of a text span within the rhetorical tree of the text.\\n• The depth of a text span within the rhetorical tree of the text.\\n• The number of edges between two text spans within the rhetorical tree.\\n• The specific rhetorical relation between two text spans like conclusion.\\n• The relation group of a specific rhetorical relation between two text spans like evaluation for the relation conclusion.\\n• Whether two text spans occur after each other in the rhetorical tree.\\nPhenomena\\n | anaphor agreement | 3 | ✓ | ✓\\n | --- | --- | --- | ---\\n | determiner noun agreement | 10 | ✓ | ✓\\n | irregular forms | 3 | ✓ | ✓\\n | subject-verb agreement | 10 | ✓ | ✓\\n\\nTable 3: Overview of resources and linguistic phenomena mapping for morphology.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n\\nTable 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n\\nTable 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n\\nTable 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n\\nTable 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.\\n | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n\\nTable 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       "   'title': 'A.4 Details of Probing Dataset Composition',\n",
       "   'chunks': [{'text': 'Whenever possible, we rely on established probing datasets and transform instances into a unified format: 1) an input x which is either one or a pair of span(s) or sentence(s), including the string and an optional starting and ending index in the context c when task type is either a span or span-pair classification; 2) an optional textual context c to encode x, for example the sentence in which a span occurs; and 3) a corresponding label y. If given, we use the original train/dev/test splits.\\nHowever, if this division does not exist, we use a 70/10/20 ratio to form these splits.\\nFurthermore, we adapt the design of some tasks to map to our task format.\\nExemplary, for the oLMmpics (Talmor et al., 2020) dataset, we transform the mask-filling tasks into a binary classification where the correct label corresponds to a sentence with a correctly filled mask incorrect to a sentence where the mask was filled wrongly.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'OnToNotes Following Tenney et al.\\n(2019b,a), we use the OntoNotes (Weischedel et al., 2013) dataset to derive part-of-speech tagging, constituent labeling, named-entity labeling, semantic role, and co-reference resolution probing datasets.\\nFurther, we consider with constituent maximum depth and constituent node length further properties of the constituent tree this dataset OntoNotes.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Dependency Corpus As in Tenney et al.\\n(2019b,a), we use Universal Dependencies annotations of the English Web Treebank to form a dependency labeling datasets.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Context Probes Presented in Klafka and Ettinger (2020), we compose nine datasets to verify information about context words.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'BLiMP Dataset Using the data presented in the BLiMP benchmark (Warstadt et al., 2020), we derive 67 probing datasets verifying specific phenomena, like island effect, covering morphology, syntax, and semantics.\\nUnlike the original version, we compose a binary classification task for every phenomenon.\\nPrecisely, whether to accept or reject a given sentence, where rejecting means that the given linguistic phenomena is violated.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Zorro Dataset As for the BLiMP tasks, we convert the 21 distinct Zorro tasks into a binary classification task on whether a sentence accepts or rejects the given linguistic phenomena is violated.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'SemEval-2010 Task 8 For semantic relation classification we rely on the dataset of Hendrickx et al.\\n(2010).',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Decompositional Semantics Initiative The Decompositional Semantics Initiative6 provides a large number of datasets to verify semantic phenomena.\\nApart of the common use semantic protoroles (Rudinger et al., 2018a), we use their collection of works to compose probing datasets for factuality (Rudinger et al., 2018b), genericity (Govindarajan et al., 2019), event structure (Vashishtha et al., 2019), time (Vashishtha et al., 2019), and word sense (White et al., 2016).',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Sentiment Analysis We use the commonly used work of Socher et al.\\n(2013) and form a probing dataset targeting sentiment.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 19,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Metaphor As in Aghazadeh et al.\\n(2022), we use the data from Mohler et al.\\n(2016); Birke and Sarkar (2006); Steen et al.\\n(2010) to form three metaphor datasets.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Complex Word Identification We consider word complexity for the first time and use the data presented in Paetzold and Specia (2016).\\nIt provides annotations for different complexity levels of words.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Passive We use data from Krasnowska-Kieraś and Wróblewska (2019) to form a probing dataset assessing knowledge about passive language.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Synonym / Antonym Replacement Using the text of the OntoNotes (Weischedel et al., 2013) and Wordnet (Miller, 1995), we form a probing dataset to detect synonym and antonym replacement.\\nSpecifically, the binary classification task is: given two texts (the original and an updated one), was the updated one changed by replacing a word with its synonym or antonym?\\nNegation With this work, we verify for the first time negation based on human annotated datasets (Vahtola et al., 2022; Szarvas et al., 2008; Konstantinova et al., 2012).\\nSpecifically, we form different probing datasets.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• Is a text negated or not?',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• Given two text spans, does the negation within the first one correspond to the second one?',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• Given a text span, is it the cue or the scope of the negation?',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'oLMmpics We form probing datasets addressing different lexical reasoning using the data presented in Talmor et al.\\n(2020).\\nAs they provide multiple choices, we form correct instances by filling the gap with the correct option and wrong ones by filling in the other options.\\nSpecifically, we form dataset for always-never, age comparison, objects comparison, antonym-negation, multi-hop composition property conjunction, taxonomy conjunction, and encyclopedic composition.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Bridging We rely on the data presented in Pandit and Hou (2021) and form two probing datasets.\\nOne is to verify whether a text is linguistically applicable, considering bridging (antecedent matches anaphora).\\nAnd a second one to verify whether an antecedent and anaphora match.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Discourse Connective Using data from Nie et al.\\n(2019), we form a probing dataset to assess whether a given connective marker matches the discourse of the given text.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Sentence Order and Next Sentence Prediction Following Narayan et al.\\n(2018), we form two datasets to verify the order of good or badness of a given sentence and whether two sentences occur after each other.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Discourse Representation Theory We use data from Webber et al.\\n(2019) to compose eight probing datasets addressing discourse representation theory:',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• Four probing dataset predicting the class of a given span.\\nWe distinguish between implicit, explicit, implicit-coarse, and explicit-coarse.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• The absolute distance, number of words, between two spans in the text.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• Whether the order of two spans is correct or not.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• Whether two spans have discourse relation or not.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• The specific discourse relation of two spans.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Rhetorical Structure Theory Using annotations from Carlson et al.\\n(2001); Zeldes (2017), we compose 14 probing datasets addressing rhetorical theory.\\nSpecifically, we compose the following seven types of datasets for both works:',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• The rhetorical type of a text span, either nucleus or satellite.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• The number of children of a text span within the rhetorical tree of the text.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• The depth of a text span within the rhetorical tree of the text.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• The number of edges between two text spans within the rhetorical tree.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• The specific rhetorical relation between two text spans like conclusion.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• The relation group of a specific rhetorical relation between two text spans like evaluation for the relation conclusion.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': '• Whether two text spans occur after each other in the rhetorical tree.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition',\n",
       "     'page': 20,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': ' | anaphor agreement | 3 | ✓ | ✓\\n | --- | --- | --- | ---\\n | determiner noun agreement | 10 | ✓ | ✓\\n | irregular forms | 3 | ✓ | ✓\\n | subject-verb agreement | 10 | ✓ | ✓\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Table 3: Overview of resources and linguistic phenomena mapping for morphology.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': ' | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Table 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': ' | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Table 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': ' | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Table 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': ' | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 22,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Table 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 22,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': ' | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 23,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'},\n",
       "    {'text': 'Table 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 23,\n",
       "     'source_doc': 'A.4 Details of Probing Dataset Composition'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316c9b910>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316c9bdf0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cbdb10>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cbe890>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cbf4c0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cdfd90>]},\n",
       "  {'text': 'Phenomena\\n | anaphor agreement | 3 | ✓ | ✓\\n | --- | --- | --- | ---\\n | determiner noun agreement | 10 | ✓ | ✓\\n | irregular forms | 3 | ✓ | ✓\\n | subject-verb agreement | 10 | ✓ | ✓\\n\\nTable 3: Overview of resources and linguistic phenomena mapping for morphology.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n\\nTable 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n\\nTable 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n\\nTable 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n\\nTable 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.\\n | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n\\nTable 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       "   'title': 'Phenomena',\n",
       "   'chunks': [{'text': ' | anaphor agreement | 3 | ✓ | ✓\\n | --- | --- | --- | ---\\n | determiner noun agreement | 10 | ✓ | ✓\\n | irregular forms | 3 | ✓ | ✓\\n | subject-verb agreement | 10 | ✓ | ✓\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 3: Overview of resources and linguistic phenomena mapping for morphology.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': ' | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': ' | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': ' | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': ' | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 22,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 22,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': ' | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 23,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 23,\n",
       "     'source_doc': 'Phenomena'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316c9b910>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316c9bdf0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cbdb10>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cbe890>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cbf4c0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cdfd90>]},\n",
       "  {'text': 'Phenomena\\n | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n\\nTable 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "   'title': 'Phenomena',\n",
       "   'chunks': [{'text': ' | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316c9bdf0>]},\n",
       "  {'text': 'Phenomena\\n | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n\\nTable 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "   'title': 'Phenomena',\n",
       "   'chunks': [{'text': ' | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316cbdb10>]},\n",
       "  {'text': 'Phenomena\\n | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n\\nTable 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "   'title': 'Phenomena',\n",
       "   'chunks': [{'text': ' | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 21,\n",
       "     'source_doc': 'Phenomena'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316cbe890>]},\n",
       "  {'text': 'Phenomena\\n | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n\\nTable 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.\\n | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n\\nTable 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       "   'title': 'Phenomena',\n",
       "   'chunks': [{'text': ' | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 22,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 22,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': ' | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 23,\n",
       "     'source_doc': 'Phenomena'},\n",
       "    {'text': 'Table 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       "     'title': 'A Additional Details of Holmes > A.4 Details of Probing Dataset Composition > Phenomena > Phenomena',\n",
       "     'page': 23,\n",
       "     'source_doc': 'Phenomena'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316cbf4c0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316cdfd90>]}],\n",
       " [{'text': 'Stylus: Automatic Adapter Selection for Diffusion Models',\n",
       "   'title': 'Stylus: Automatic Adapter Selection for Diffusion Models',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Michael Luo UC Berkeley\\nmichael.luo@berkeley.edu jegonzal@berkeley.edu wong.justin@berkeley.edu zhifengc@google.com brandon@btrabucco.com rsalakhu@cs.cmu.edu huangyp@google.com istoica@berkeley.edu',\n",
       "   'title': 'Michael Luo UC Berkeley',\n",
       "   'chunks': [{'text': 'michael.luo@berkeley.edu jegonzal@berkeley.edu wong.justin@berkeley.edu zhifengc@google.com brandon@btrabucco.com rsalakhu@cs.cmu.edu huangyp@google.com istoica@berkeley.edu',\n",
       "     'title': 'Michael Luo UC Berkeley',\n",
       "     'page': 0,\n",
       "     'source_doc': 'Michael Luo UC Berkeley'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Joseph E. Gonzalez UC Berkeley',\n",
       "   'title': 'Joseph E. Gonzalez UC Berkeley',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Justin Wong UC Berkeley',\n",
       "   'title': 'Justin Wong UC Berkeley',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Zhifeng Chen Google Deepmind',\n",
       "   'title': 'Zhifeng Chen Google Deepmind',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Brandon Trabucco CMU MLD',\n",
       "   'title': 'Brandon Trabucco CMU MLD',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Ruslan Salakhutdinov CMU MLD',\n",
       "   'title': 'Ruslan Salakhutdinov CMU MLD',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Yanping Huang Google Deepmind',\n",
       "   'title': 'Yanping Huang Google Deepmind',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Ion Stoica UC Berkeley\\nStylus\\nSD\\n\\n | v1.5 |  | \\n | --- | --- | ---\\n | “This LoRA generates huskies…” |  |  |  | “This LoRA generates snowboards…”\\n | --- | --- | --- | --- | ---\\n |  | Wooden dish rack on a counter holding plates, saucers, a bowl, mugs and glasses. | \\n | A boy holding an umbrella on the edge of a cliff.\\n | Figure 1. Adapter Selection. Given a user-provided prompt, our method identifies highly relevant adapters (e.g. Low-Rank Adaptation, LoRA) that are closely aligned with the prompt’s context and at least one of the prompt’s keywords. Composing relevant adapters into Sta- ble Diffusion improves visual fidelity, image diversity, and textual alignment. Note that these prompts are sampled from MS-COCO [19].\\n | A open field with large elephants standing in it. greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.\\n | Abstract\\n | Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by opensource communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions. To generate high quality images, This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt’s keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts’ keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves\\n | 1. Introduction\\n | In the evolving field of generative image models, finetuned adapters [7, 9] have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different adapters and model checkpoints, fueling the proliferation of creative AI art [24, 43]. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA) [12] emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.\\n | In light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts\\n | Refiner Composer\\n | Below is an ordered list of adapter descriptions based on relevance:\\n | … “This LoRA generates [XYZ]” #0: This LoRA generates huskies… #842: This LoRA introduces the style of Christmas cards… #3: This LoRA generates snowboards… …\\n | #3 #2 #1 #0 LoRA\\n | … #1 #2 #3 #4#0\\n | and the prompt “Two dogs playing in the snow”.\\n | Vector DB\\n | Adapter DB\\n | First, segment the prompt into different keywords. Second, for each keyword, identify the relevant adapters which directly match the keyword and the prompt’s context.\\n | Retriever\\n | Two dogs playing in the snow.\\n | #0 |  | #242 … | #3 | \\n\\n“dogs” “snow” “This LoRA makes snow more powdery…” “This LoRA generates high-quality images of dogs” #72 #72 #1337 #0 #242 Figure 2.\\nStylus algorithm.\\nStylus consists of three stages.\\nThe refiner plugs an adapter’s model card through a VLM to generate textual descriptions of an adapter’s task and then through an encoder to produce the corresponding text embedding.\\nThe retriever fetches candidate adapters that are relevant to the entire user prompt.\\nFinally, the composer prunes and jointly categorizes the remaining adapters based on the prompt’s tasks, which correspond to a set of keywords.\\n(see Fig. 1).\\nHowever, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings [16].\\nSpecifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data—a common issue on open-source platforms.\\nFurthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks.\\nFor instance, the prompt “two dogs playing the snow” suggests that there are two tasks: generating images of “dogs” and “snow”.\\nThis necessitates segmenting the prompt into various tasks (i.e. keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems [8].\\nFinally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).\\n100000\\n75000\\n50000\\n | 25000\\n | ---\\n | SD 1.5 | SDXL 1.0 | SD 1.5 | SDXL 1.0 0\\n | --- | --- | --- | ---\\n | We propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the refiner plugs in an adapter’s model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods [16], the retriever scores the relevance of each embedding against the user’s entire prompt to retrieve a set of candidate adapters. Finally, the composer segments the prompt into disjoint tasks, further prunes irrelevant can- didate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that\\n | LoRA | Textual Inversion |  | \\n | Civit AI |  | Hugging Face | \\n\\nFigure 3.\\nNumber of Adapters.\\nCivit AI boasts 100K+ adapters for Stable Diffusion, outpacing that of Hugging Face.\\nLow-Rank Adaptation (LoRA) is the dominant approach for finetuning.\\nintroduce biases detrimental to image generation (§ 4.3).\\nFinally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.\\nTo evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs1, that contains pre-computed adapter documentations and embeddings from Stylus’s refiner.\\nOur results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints—shifting the CLIP-FID Pareto curve towards greater efficiency and achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators.\\nAs a system, Stylus is practical and does not present large overheads to the image generation process.\\nFinally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation.\\n1Sourced from https://civitai.com/ [24].',\n",
       "   'title': 'Ion Stoica UC Berkeley',\n",
       "   'chunks': [{'text': '',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 0,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'},\n",
       "    {'text': ' | v1.5 |  | \\n | --- | --- | ---\\n | “This LoRA generates huskies…” |  |  |  | “This LoRA generates snowboards…”\\n | --- | --- | --- | --- | ---\\n |  | Wooden dish rack on a counter holding plates, saucers, a bowl, mugs and glasses. | \\n | A boy holding an umbrella on the edge of a cliff.\\n | Figure 1. Adapter Selection. Given a user-provided prompt, our method identifies highly relevant adapters (e.g. Low-Rank Adaptation, LoRA) that are closely aligned with the prompt’s context and at least one of the prompt’s keywords. Composing relevant adapters into Sta- ble Diffusion improves visual fidelity, image diversity, and textual alignment. Note that these prompts are sampled from MS-COCO [19].\\n | A open field with large elephants standing in it. greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.\\n | Abstract\\n | Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by opensource communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions. To generate high quality images, This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt’s keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts’ keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves\\n | 1. Introduction\\n | In the evolving field of generative image models, finetuned adapters [7, 9] have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different adapters and model checkpoints, fueling the proliferation of creative AI art [24, 43]. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA) [12] emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.\\n | In light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts\\n | Refiner Composer\\n | Below is an ordered list of adapter descriptions based on relevance:\\n | … “This LoRA generates [XYZ]” #0: This LoRA generates huskies… #842: This LoRA introduces the style of Christmas cards… #3: This LoRA generates snowboards… …\\n | #3 #2 #1 #0 LoRA\\n | … #1 #2 #3 #4#0\\n | and the prompt “Two dogs playing in the snow”.\\n | Vector DB\\n | Adapter DB\\n | First, segment the prompt into different keywords. Second, for each keyword, identify the relevant adapters which directly match the keyword and the prompt’s context.\\n | Retriever\\n | Two dogs playing in the snow.\\n | #0 |  | #242 … | #3 | \\n',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'},\n",
       "    {'text': '“dogs” “snow” “This LoRA makes snow more powdery…” “This LoRA generates high-quality images of dogs” #72 #72 #1337 #0 #242 Figure 2.\\nStylus algorithm.\\nStylus consists of three stages.\\nThe refiner plugs an adapter’s model card through a VLM to generate textual descriptions of an adapter’s task and then through an encoder to produce the corresponding text embedding.\\nThe retriever fetches candidate adapters that are relevant to the entire user prompt.\\nFinally, the composer prunes and jointly categorizes the remaining adapters based on the prompt’s tasks, which correspond to a set of keywords.',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'},\n",
       "    {'text': '(see Fig. 1).\\nHowever, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings [16].\\nSpecifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data—a common issue on open-source platforms.\\nFurthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks.\\nFor instance, the prompt “two dogs playing the snow” suggests that there are two tasks: generating images of “dogs” and “snow”.\\nThis necessitates segmenting the prompt into various tasks (i.e. keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems [8].\\nFinally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'},\n",
       "    {'text': '100000',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'},\n",
       "    {'text': '75000',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'},\n",
       "    {'text': '50000',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'},\n",
       "    {'text': ' | 25000\\n | ---\\n | SD 1.5 | SDXL 1.0 | SD 1.5 | SDXL 1.0 0\\n | --- | --- | --- | ---\\n | We propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the refiner plugs in an adapter’s model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods [16], the retriever scores the relevance of each embedding against the user’s entire prompt to retrieve a set of candidate adapters. Finally, the composer segments the prompt into disjoint tasks, further prunes irrelevant can- didate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that\\n | LoRA | Textual Inversion |  | \\n | Civit AI |  | Hugging Face | \\n',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'},\n",
       "    {'text': 'Figure 3.\\nNumber of Adapters.\\nCivit AI boasts 100K+ adapters for Stable Diffusion, outpacing that of Hugging Face.\\nLow-Rank Adaptation (LoRA) is the dominant approach for finetuning.\\nintroduce biases detrimental to image generation (§ 4.3).\\nFinally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'},\n",
       "    {'text': 'To evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs1, that contains pre-computed adapter documentations and embeddings from Stylus’s refiner.\\nOur results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints—shifting the CLIP-FID Pareto curve towards greater efficiency and achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators.\\nAs a system, Stylus is practical and does not present large overheads to the image generation process.\\nFinally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation.',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'},\n",
       "    {'text': '1Sourced from https://civitai.com/ [24].',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Ion Stoica UC Berkeley'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316d2a050>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316d2a080>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316d2b070>]},\n",
       "  {'text': 'Stylus\\nSD\\n\\n | v1.5 |  | \\n | --- | --- | ---\\n | “This LoRA generates huskies…” |  |  |  | “This LoRA generates snowboards…”\\n | --- | --- | --- | --- | ---\\n |  | Wooden dish rack on a counter holding plates, saucers, a bowl, mugs and glasses. | \\n | A boy holding an umbrella on the edge of a cliff.\\n | Figure 1. Adapter Selection. Given a user-provided prompt, our method identifies highly relevant adapters (e.g. Low-Rank Adaptation, LoRA) that are closely aligned with the prompt’s context and at least one of the prompt’s keywords. Composing relevant adapters into Sta- ble Diffusion improves visual fidelity, image diversity, and textual alignment. Note that these prompts are sampled from MS-COCO [19].\\n | A open field with large elephants standing in it. greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.\\n | Abstract\\n | Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by opensource communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions. To generate high quality images, This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt’s keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts’ keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves\\n | 1. Introduction\\n | In the evolving field of generative image models, finetuned adapters [7, 9] have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different adapters and model checkpoints, fueling the proliferation of creative AI art [24, 43]. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA) [12] emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.\\n | In light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts\\n | Refiner Composer\\n | Below is an ordered list of adapter descriptions based on relevance:\\n | … “This LoRA generates [XYZ]” #0: This LoRA generates huskies… #842: This LoRA introduces the style of Christmas cards… #3: This LoRA generates snowboards… …\\n | #3 #2 #1 #0 LoRA\\n | … #1 #2 #3 #4#0\\n | and the prompt “Two dogs playing in the snow”.\\n | Vector DB\\n | Adapter DB\\n | First, segment the prompt into different keywords. Second, for each keyword, identify the relevant adapters which directly match the keyword and the prompt’s context.\\n | Retriever\\n | Two dogs playing in the snow.\\n | #0 |  | #242 … | #3 | \\n\\n“dogs” “snow” “This LoRA makes snow more powdery…” “This LoRA generates high-quality images of dogs” #72 #72 #1337 #0 #242 Figure 2.\\nStylus algorithm.\\nStylus consists of three stages.\\nThe refiner plugs an adapter’s model card through a VLM to generate textual descriptions of an adapter’s task and then through an encoder to produce the corresponding text embedding.\\nThe retriever fetches candidate adapters that are relevant to the entire user prompt.\\nFinally, the composer prunes and jointly categorizes the remaining adapters based on the prompt’s tasks, which correspond to a set of keywords.\\n(see Fig. 1).\\nHowever, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings [16].\\nSpecifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data—a common issue on open-source platforms.\\nFurthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks.\\nFor instance, the prompt “two dogs playing the snow” suggests that there are two tasks: generating images of “dogs” and “snow”.\\nThis necessitates segmenting the prompt into various tasks (i.e. keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems [8].\\nFinally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).\\n100000\\n75000\\n50000\\n | 25000\\n | ---\\n | SD 1.5 | SDXL 1.0 | SD 1.5 | SDXL 1.0 0\\n | --- | --- | --- | ---\\n | We propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the refiner plugs in an adapter’s model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods [16], the retriever scores the relevance of each embedding against the user’s entire prompt to retrieve a set of candidate adapters. Finally, the composer segments the prompt into disjoint tasks, further prunes irrelevant can- didate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that\\n | LoRA | Textual Inversion |  | \\n | Civit AI |  | Hugging Face | \\n\\nFigure 3.\\nNumber of Adapters.\\nCivit AI boasts 100K+ adapters for Stable Diffusion, outpacing that of Hugging Face.\\nLow-Rank Adaptation (LoRA) is the dominant approach for finetuning.\\nintroduce biases detrimental to image generation (§ 4.3).\\nFinally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.\\nTo evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs1, that contains pre-computed adapter documentations and embeddings from Stylus’s refiner.\\nOur results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints—shifting the CLIP-FID Pareto curve towards greater efficiency and achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators.\\nAs a system, Stylus is practical and does not present large overheads to the image generation process.\\nFinally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation.\\n1Sourced from https://civitai.com/ [24].',\n",
       "   'title': 'Stylus',\n",
       "   'chunks': [{'text': '',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 0,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': ' | v1.5 |  | \\n | --- | --- | ---\\n | “This LoRA generates huskies…” |  |  |  | “This LoRA generates snowboards…”\\n | --- | --- | --- | --- | ---\\n |  | Wooden dish rack on a counter holding plates, saucers, a bowl, mugs and glasses. | \\n | A boy holding an umbrella on the edge of a cliff.\\n | Figure 1. Adapter Selection. Given a user-provided prompt, our method identifies highly relevant adapters (e.g. Low-Rank Adaptation, LoRA) that are closely aligned with the prompt’s context and at least one of the prompt’s keywords. Composing relevant adapters into Sta- ble Diffusion improves visual fidelity, image diversity, and textual alignment. Note that these prompts are sampled from MS-COCO [19].\\n | A open field with large elephants standing in it. greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.\\n | Abstract\\n | Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by opensource communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions. To generate high quality images, This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt’s keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts’ keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves\\n | 1. Introduction\\n | In the evolving field of generative image models, finetuned adapters [7, 9] have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different adapters and model checkpoints, fueling the proliferation of creative AI art [24, 43]. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA) [12] emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.\\n | In light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts\\n | Refiner Composer\\n | Below is an ordered list of adapter descriptions based on relevance:\\n | … “This LoRA generates [XYZ]” #0: This LoRA generates huskies… #842: This LoRA introduces the style of Christmas cards… #3: This LoRA generates snowboards… …\\n | #3 #2 #1 #0 LoRA\\n | … #1 #2 #3 #4#0\\n | and the prompt “Two dogs playing in the snow”.\\n | Vector DB\\n | Adapter DB\\n | First, segment the prompt into different keywords. Second, for each keyword, identify the relevant adapters which directly match the keyword and the prompt’s context.\\n | Retriever\\n | Two dogs playing in the snow.\\n | #0 |  | #242 … | #3 | \\n',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': '“dogs” “snow” “This LoRA makes snow more powdery…” “This LoRA generates high-quality images of dogs” #72 #72 #1337 #0 #242 Figure 2.\\nStylus algorithm.\\nStylus consists of three stages.\\nThe refiner plugs an adapter’s model card through a VLM to generate textual descriptions of an adapter’s task and then through an encoder to produce the corresponding text embedding.\\nThe retriever fetches candidate adapters that are relevant to the entire user prompt.\\nFinally, the composer prunes and jointly categorizes the remaining adapters based on the prompt’s tasks, which correspond to a set of keywords.',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': '(see Fig. 1).\\nHowever, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings [16].\\nSpecifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data—a common issue on open-source platforms.\\nFurthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks.\\nFor instance, the prompt “two dogs playing the snow” suggests that there are two tasks: generating images of “dogs” and “snow”.\\nThis necessitates segmenting the prompt into various tasks (i.e. keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems [8].\\nFinally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': '100000',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': '75000',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': '50000',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': ' | 25000\\n | ---\\n | SD 1.5 | SDXL 1.0 | SD 1.5 | SDXL 1.0 0\\n | --- | --- | --- | ---\\n | We propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the refiner plugs in an adapter’s model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods [16], the retriever scores the relevance of each embedding against the user’s entire prompt to retrieve a set of candidate adapters. Finally, the composer segments the prompt into disjoint tasks, further prunes irrelevant can- didate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that\\n | LoRA | Textual Inversion |  | \\n | Civit AI |  | Hugging Face | \\n',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': 'Figure 3.\\nNumber of Adapters.\\nCivit AI boasts 100K+ adapters for Stable Diffusion, outpacing that of Hugging Face.\\nLow-Rank Adaptation (LoRA) is the dominant approach for finetuning.\\nintroduce biases detrimental to image generation (§ 4.3).\\nFinally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': 'To evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs1, that contains pre-computed adapter documentations and embeddings from Stylus’s refiner.\\nOur results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints—shifting the CLIP-FID Pareto curve towards greater efficiency and achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators.\\nAs a system, Stylus is practical and does not present large overheads to the image generation process.\\nFinally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation.',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': '1Sourced from https://civitai.com/ [24].',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'Stylus'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316d2a050>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316d2a080>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316d2b070>]},\n",
       "  {'text': 'SD\\n\\n | v1.5 |  | \\n | --- | --- | ---\\n | “This LoRA generates huskies…” |  |  |  | “This LoRA generates snowboards…”\\n | --- | --- | --- | --- | ---\\n |  | Wooden dish rack on a counter holding plates, saucers, a bowl, mugs and glasses. | \\n | A boy holding an umbrella on the edge of a cliff.\\n | Figure 1. Adapter Selection. Given a user-provided prompt, our method identifies highly relevant adapters (e.g. Low-Rank Adaptation, LoRA) that are closely aligned with the prompt’s context and at least one of the prompt’s keywords. Composing relevant adapters into Sta- ble Diffusion improves visual fidelity, image diversity, and textual alignment. Note that these prompts are sampled from MS-COCO [19].\\n | A open field with large elephants standing in it. greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.\\n | Abstract\\n | Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by opensource communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions. To generate high quality images, This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt’s keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts’ keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves\\n | 1. Introduction\\n | In the evolving field of generative image models, finetuned adapters [7, 9] have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different adapters and model checkpoints, fueling the proliferation of creative AI art [24, 43]. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA) [12] emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.\\n | In light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts\\n | Refiner Composer\\n | Below is an ordered list of adapter descriptions based on relevance:\\n | … “This LoRA generates [XYZ]” #0: This LoRA generates huskies… #842: This LoRA introduces the style of Christmas cards… #3: This LoRA generates snowboards… …\\n | #3 #2 #1 #0 LoRA\\n | … #1 #2 #3 #4#0\\n | and the prompt “Two dogs playing in the snow”.\\n | Vector DB\\n | Adapter DB\\n | First, segment the prompt into different keywords. Second, for each keyword, identify the relevant adapters which directly match the keyword and the prompt’s context.\\n | Retriever\\n | Two dogs playing in the snow.\\n | #0 |  | #242 … | #3 | \\n\\n“dogs” “snow” “This LoRA makes snow more powdery…” “This LoRA generates high-quality images of dogs” #72 #72 #1337 #0 #242 Figure 2.\\nStylus algorithm.\\nStylus consists of three stages.\\nThe refiner plugs an adapter’s model card through a VLM to generate textual descriptions of an adapter’s task and then through an encoder to produce the corresponding text embedding.\\nThe retriever fetches candidate adapters that are relevant to the entire user prompt.\\nFinally, the composer prunes and jointly categorizes the remaining adapters based on the prompt’s tasks, which correspond to a set of keywords.\\n(see Fig. 1).\\nHowever, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings [16].\\nSpecifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data—a common issue on open-source platforms.\\nFurthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks.\\nFor instance, the prompt “two dogs playing the snow” suggests that there are two tasks: generating images of “dogs” and “snow”.\\nThis necessitates segmenting the prompt into various tasks (i.e. keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems [8].\\nFinally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).\\n100000\\n75000\\n50000\\n | 25000\\n | ---\\n | SD 1.5 | SDXL 1.0 | SD 1.5 | SDXL 1.0 0\\n | --- | --- | --- | ---\\n | We propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the refiner plugs in an adapter’s model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods [16], the retriever scores the relevance of each embedding against the user’s entire prompt to retrieve a set of candidate adapters. Finally, the composer segments the prompt into disjoint tasks, further prunes irrelevant can- didate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that\\n | LoRA | Textual Inversion |  | \\n | Civit AI |  | Hugging Face | \\n\\nFigure 3.\\nNumber of Adapters.\\nCivit AI boasts 100K+ adapters for Stable Diffusion, outpacing that of Hugging Face.\\nLow-Rank Adaptation (LoRA) is the dominant approach for finetuning.\\nintroduce biases detrimental to image generation (§ 4.3).\\nFinally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.\\nTo evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs1, that contains pre-computed adapter documentations and embeddings from Stylus’s refiner.\\nOur results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints—shifting the CLIP-FID Pareto curve towards greater efficiency and achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators.\\nAs a system, Stylus is practical and does not present large overheads to the image generation process.\\nFinally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation.\\n1Sourced from https://civitai.com/ [24].',\n",
       "   'title': 'SD',\n",
       "   'chunks': [{'text': '',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 0,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': ' | v1.5 |  | \\n | --- | --- | ---\\n | “This LoRA generates huskies…” |  |  |  | “This LoRA generates snowboards…”\\n | --- | --- | --- | --- | ---\\n |  | Wooden dish rack on a counter holding plates, saucers, a bowl, mugs and glasses. | \\n | A boy holding an umbrella on the edge of a cliff.\\n | Figure 1. Adapter Selection. Given a user-provided prompt, our method identifies highly relevant adapters (e.g. Low-Rank Adaptation, LoRA) that are closely aligned with the prompt’s context and at least one of the prompt’s keywords. Composing relevant adapters into Sta- ble Diffusion improves visual fidelity, image diversity, and textual alignment. Note that these prompts are sampled from MS-COCO [19].\\n | A open field with large elephants standing in it. greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.\\n | Abstract\\n | Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by opensource communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions. To generate high quality images, This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt’s keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts’ keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves\\n | 1. Introduction\\n | In the evolving field of generative image models, finetuned adapters [7, 9] have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different adapters and model checkpoints, fueling the proliferation of creative AI art [24, 43]. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA) [12] emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.\\n | In light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts\\n | Refiner Composer\\n | Below is an ordered list of adapter descriptions based on relevance:\\n | … “This LoRA generates [XYZ]” #0: This LoRA generates huskies… #842: This LoRA introduces the style of Christmas cards… #3: This LoRA generates snowboards… …\\n | #3 #2 #1 #0 LoRA\\n | … #1 #2 #3 #4#0\\n | and the prompt “Two dogs playing in the snow”.\\n | Vector DB\\n | Adapter DB\\n | First, segment the prompt into different keywords. Second, for each keyword, identify the relevant adapters which directly match the keyword and the prompt’s context.\\n | Retriever\\n | Two dogs playing in the snow.\\n | #0 |  | #242 … | #3 | \\n',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': '“dogs” “snow” “This LoRA makes snow more powdery…” “This LoRA generates high-quality images of dogs” #72 #72 #1337 #0 #242 Figure 2.\\nStylus algorithm.\\nStylus consists of three stages.\\nThe refiner plugs an adapter’s model card through a VLM to generate textual descriptions of an adapter’s task and then through an encoder to produce the corresponding text embedding.\\nThe retriever fetches candidate adapters that are relevant to the entire user prompt.\\nFinally, the composer prunes and jointly categorizes the remaining adapters based on the prompt’s tasks, which correspond to a set of keywords.',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': '(see Fig. 1).\\nHowever, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings [16].\\nSpecifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data—a common issue on open-source platforms.\\nFurthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks.\\nFor instance, the prompt “two dogs playing the snow” suggests that there are two tasks: generating images of “dogs” and “snow”.\\nThis necessitates segmenting the prompt into various tasks (i.e. keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems [8].\\nFinally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': '100000',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': '75000',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': '50000',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': ' | 25000\\n | ---\\n | SD 1.5 | SDXL 1.0 | SD 1.5 | SDXL 1.0 0\\n | --- | --- | --- | ---\\n | We propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the refiner plugs in an adapter’s model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods [16], the retriever scores the relevance of each embedding against the user’s entire prompt to retrieve a set of candidate adapters. Finally, the composer segments the prompt into disjoint tasks, further prunes irrelevant can- didate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that\\n | LoRA | Textual Inversion |  | \\n | Civit AI |  | Hugging Face | \\n',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': 'Figure 3.\\nNumber of Adapters.\\nCivit AI boasts 100K+ adapters for Stable Diffusion, outpacing that of Hugging Face.\\nLow-Rank Adaptation (LoRA) is the dominant approach for finetuning.\\nintroduce biases detrimental to image generation (§ 4.3).\\nFinally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': 'To evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs1, that contains pre-computed adapter documentations and embeddings from Stylus’s refiner.\\nOur results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints—shifting the CLIP-FID Pareto curve towards greater efficiency and achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators.\\nAs a system, Stylus is practical and does not present large overheads to the image generation process.\\nFinally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation.',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': '1Sourced from https://civitai.com/ [24].',\n",
       "     'title': 'Ion Stoica UC Berkeley > Stylus > SD',\n",
       "     'page': 1,\n",
       "     'source_doc': 'SD'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316d2a050>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316d2a080>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316d2b070>]},\n",
       "  {'text': '2. Related Works\\nAdapters.\\nAdapters efficiently fine-tune models on specific tasks with minimal parameter changes, reducing computational and storage requirements while maintaining similar performance to full fine-tuning [7, 9, 12].\\nOur study focuses on retrieving and merging multiple Low-Rank adapters (LoRA), the popular approach within existing open-source communities [24, 25, 43].\\nAdapter composition has emerged as a crucial mechanism for enhancing the capabilities of foundational models across various applications [17, 30, 34, 38, 39].\\nFor large language models (LLM), the linear combination of multiple adapters improves in-domain performance and cross-task generalization [3, 13, 14, 40, 41, 46].\\nIn the image domain, merging LoRAs effectively composes different tasks—concepts, characters, poses, actions, and styles—together, yielding images of high fidelity that closely align with user specifications [21, 47].\\nOur approach advances this further by actively segmenting user prompts into distinct tasks and merging the appropriate adapters for each task.\\nRetrieval-based Methods.\\nRetrieval-based methods, such as retrieval-augmented generation (RAG), significantly improve model responses by adding semantically similar texts from a vast external database [16].\\nThese methods convert text to vector embeddings using text encoders, which are then ranked against a user prompt based on similarity metrics [4, 8, 18, 23, 31, 33].\\nSimilarly, our work draws inspiration from RAG to encode adapters as vector embedings: leveraging visual-language foundational models (VLM) to generate semantic descriptions of adapters, which are then translated into embeddings.\\nA core limitation to RAG is limited precision, retrieving distracting irrelevant documents.\\nThis leads to a”needlein-the-haystack” problem, where more relevant documents are buried further down the list [8].\\nRecent work introduce reranking step; this technique uses cross-encoders to assess both the raw user prompt and the ranked set of raw texts individually, thereby discovering texts based on actual relevance [23, 32].\\nRerankers have been successfully integrated with various LLM-application frameworks [2, 20, 29].\\n3. Our Method: Stylus\\nAdapter selection presents distinct challenges compared to existing methods for retrieving text-based documents, as outlined in Section 2.\\nFirst, computing embeddings for adapters is a novel task, made more difficult without access to training datasets.\\nFurthermore, in the context of image generation, user prompts often specify multiple highly fine-grained tasks.\\nThis challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt.\\nFinally, composing multiple adapters can degrade image quality and inject foreign biases into the model.\\nOur three-stage framework below—Refine, Retrieve, and Compose—addresses the above challenges (Fig. 2).\\n3.1. Refiner\\nThe refiner is a two-stage pipeline designed to generate textual descriptions of an adapter’s task and the corresponding text embeddings for retrieval purposes.\\nThis approach mirrors retrieval-based methods [16], which pre-compute embeddings over an external database of texts.\\nGiven an adapter Ai, the first stage is a vision-language model (VLM) that takes in the adapter’s model card—a set of randomly sampled example images from the model card Ii ∈ {Ii1, Ii2,}, the corresponding prompts Pi ∈ {pi1, pi2,}, and an author-provided description,2 Di—and returns an improved description D∗ i. Optionally, the VLM also recommends the weight for LoRA-based adapters, as the adapter weight is usually specified either in the author’s description Di or the set of prompts Pi, a feature present in popular image generation software [1].\\nIf information cannot be found, the LoRA’s weight is set to 0.8.\\nIn our experiments, these improved descriptions were generated by Gemini Ultra [37] (see § A.1 for prompt).\\nThe second stage uses an embedding model (E) to generate embeddings e = E(D∗) for all adapters.\\nIn our experiments, we create embeddings from OpenAI’s text-embedding-3-large model [18, 26].\\nWe store pre-computed embeddings in a vector database.\\n3.2. Retriever\\nThe retriever fetches the most relevant adapters over the entirety of the user’s prompt using similarity metrics.\\nMathematically, the retriever employs the same embedding model (E) to process the user prompt, s, generating embedding es = E(s).\\nIt then calculates cosine similarity scores between the prompt’s embedding es and the embedding of each adapter in the matrix M. The top K adapters AK (K = 150, in our experiments) are selected based on the highest similarity scores: AK = arg top-Ki\\n(es·Mi ∥es∥∥Mi∥) , where Mi is the ith row of the embedding matrix, representing the ith adapter’s embedding.\\n3.3. Composer\\nThe composer serves a dual purpose: segmenting the prompt into tasks from a prompt’s keywords and assigning retrieved adapters to tasks.\\nThis implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the 2We note that a large set of author descriptions are inaccurate, misleading, or absent.\\nThe refiner helped correct for human errors by using generated images as the ground truth, significantly improving our system.\\nFigure 4.\\nQualitative comparison between Stylus over realistic (left) and cartoon (right) style Stable Diffusion checkpoints.\\nStylus produces highly detailed images that correctly depicts keywords in the context of the prompt.\\nFor the prompt “A graffiti of a corgi on the wall”, our method correctly depicts a spray-painted corgi, whereas the checkpoint generates a realistic dog.\\n |  | RealisticVision-v6 / COCO | RealisticVision-v6 / PartiPrompts | Counterfeit-v3 / COCO | Counterfeit-v3 / PartiPrompts\\n | --- | --- | --- | --- | ---\\n | 80\\n | Stylus SD 1.5 | 68.31% | 67.81% | 62.00% | 58.57%\\n\\n60\\n41.43% 38.00%\\n40\\n32.19% 31.69%\\n20\\n0\\nFigure 5.\\nHuman Evaluation.\\nStylus achieves a higher preference scores (2:1) over different datasets and Stable Diffusion checkpoints.\\nprompt through keyword grounding.\\nFor example, if the prompt is “pandas eating bamboo”, the composer may discard an irrelevant “grizzly bears” adapter and a biased “panda mascots” adapter.\\nMathematically, the composer (C) takes in the prompt (s) and the top K adapters (AK) from the retriever, classifying them over different tasks,\\nT (s) = {t1, t2,, tn}.\\nThis can be expressed as: C(s,AK) = {(ti,Aki) | ti ∈ T (s),Aki ⊆ AK, ∀Aj ∈ Aki, Sim(Aj, ti) ≥ τ} (1), where Aki is the subset of adapters per task ti, Sim(Aj, ti) measures the similarity score between an adapter and a task, and τ is an arbitary threshold.\\nWhile the composer can be trained with human-labeled data [28], we opt for a simpler approach that requires no training—prompting a long-context Large Language Model (LLM).\\nThe LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters.\\nIn our implementation, we choose Gemini 1.5, with a 128K context window, as the composer’s LLM (see App. A.2 for the full prompt).\\nMost importantly, Stylus’s composer parallels reranking, an advanced RAG technique.\\nRerankers employ cross encoders (F) that compare the retriever’s individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores: F(s, D∗).\\nThis prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment.\\nOur experimental ablations (§ 4.3) show that our composer outperforms existing rerankers (Cohere,\\nrerank-english-v2.0) [32].\\n3.4. Masking\\n | The composer identifies tasks assigns each task a set of relevant adapters as: C(Ak, s) = {(t1,Ak1), (t2,Ak2), | ti | from the prompt s Aki, formalized }. Our masking | and\\n | --- | --- | --- | ---\\n | strategy applies a binary mask, mask, Mi, can either be an one hot encoding, all ones, or all zeroes vector. Across all tasks, we perform a cross- product across masks, M1 ×M2 ×M3 × | Mi, | for each task ti. Each , generating | an\\n\\nexponential number of masking schemes.\\nThe combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images (§ 4.2.3).\\nThis approach also curtails the number of final adapters merged into the base model, minimizing the risk of composed adapters introducing undesirable effects to the image [47].\\nFinally, an adapter’s weight (i.e. LoRA), which is extracted from the refiner (§ 3.1), is divided by the total number of adapters (after masking) in its corresponding task.\\nThis solves the problem of image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality (see App. A.4).\\n4. Results\\n4.1. Experimental Setup\\nAdapter Testbed.\\nAdapter selection requires a large database of adapters to properly evaluate its performance.\\n24\\n23\\n22\\n21\\n20\\nSD 1.5\\n19\\nStylus\\n |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n\\nTable 1.\\nEvaluation over different retrieval methods (CFG=6).\\nStylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.\\nHowever, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:\\nRealistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.\\n4.2. Main Experiments\\n4.2.1 Human Evaluation.\\nTo demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.\\nTo conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.\\n4.2.2 Automatic Benchmarks.\\nWe assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.\\n4.2.3 VLM as a Judge\\nWe use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.\\nFigure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.\\n Lose Tie Win\\n100\\n20 29\\n80\\n59\\n60\\n58 52\\n40\\n20\\n41 19 22\\nPrompt Length\\n0\\n | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n\\n(a) VLM as a Judge with GPT-4V\\nFigure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.\\nLose Tie Win\\n100\\n35\\n80\\n60\\n40\\n40\\n20\\n |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n\\nuate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.\\n4.2.4 Diversity per Prompt\\nGiven identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:\\ncalculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.\\nGPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,\\n4FID fails to disentangle image fidelity from diversity [27, 35].\\nStylus\\nReranker\\nRetriever\\nRandom\\nA baby elephant underneath an adult elephant.\\n16s\\n25s\\nA building behind a stop sign and street sign.\\n44s\\nTime (s)\\nA group of friends jumping on a bed.\\nRetriever Composer Load Adapter Image Generation\\n | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n\\nFigure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.\\n | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n\\nFigure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.\\nAs shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.\\n4.3. Ablations\\n4.3.1 Alternative Retrieval-based Methods\\nWe benchmark Stylus’s performance relative to different retrieval methods.\\nFor all baselines below, we select the top three adapters and merge them into the base model.\\nRandom: Adapters are randomly sampled without replacement from StylusDocs.\\nRetriever: The retriever emulates standard RAG pipelines [16, 46], functionally equivalent to Stylus without the composer stage.\\nTop adapters are fetched via cosine similarity over adapter embeddings.\\nConversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion.\\nThe random baseline chooses adapters that are orthogonal to the user prompt.\\nThus, these adapters alter unrelated concepts, which does not affect image generation.\\nIn fact, we observed that the distribution of random policy’s images in Fig. 10 were nearly identical to Stable Diffusion.\\n |  | Stylus | Stylus\\n | --- | --- | ---\\n | Source | SD v1.5 | Source | SD v1.5\\n | --- | --- | --- | ---\\n\\nVolcanoes spewing lava\\nStudio Ghibli Style\\nFiery red Voxel Style Pencil Sketch Van Gogh Style\\n(a) Image Translation\\nStylus chooses relevant adapters that better adapt new styles and elements into existing images.\\nStylus Stylus\\nSD v1.5 SD v1.5\\nSource Source\\nDwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny\\n(b) Inpainting\\nStylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.\\nFigure 12.\\nStylus over different image-to-image tasks.\\n4.3.2 Breakdown of Stylus’s Inference Time\\nThis section breaks down the latency introduced by various components of Stylus.\\nWe note that image generation time is independent of Stylus, as adapter weights are merged into the base model [12].\\nFigure 11 demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts.\\nSpecifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds.\\nThe composer’s large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens.\\nFinally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process.\\nHowever, Stylus’s latency remains consistent across all batch sizes, as the composer and retriever run only once.\\nHence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases.\\n4.3.3 Image-Domain Tasks\\nBeyond text-to-image, Stylus applies across various imageto-image tasks.\\nFig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting, described as follows:\\nImage translation: Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt’s definition.\\nStylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style.\\nWe present examples in Fig 12a.\\nFor a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits.\\nFor a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.\\nInpainting: Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask.\\nStylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity.\\nWe provide further examples in Fig. 12b, demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right).\\n5. Conclusion\\nWe propose Stylus, a new algorithm that automatically selects and composes adapters to generate better images.\\nOur method leverages a three-stage framework that precomputes adapters as lookup embeddings and retrieves most relevant adapters based on prompts’ keywords.\\nTo evaluate Stylus, we develop StylusDocs, a processed dataset featuring 75K adapters and pre-computed adapter embeddings.\\nOur evaluation of Stylus, across automatic metrics, humans, and vision-language models, demonstrate that Stylus achieves better visual fidelity, textual alignment, and image diversity than existing Stable Diffusion checkpoints.\\nAcknowledgement\\nWe thank Lisa Dunlap, Ansh Chaurasia, Siyuan Zhuang, Sijun Tan, Tianjun Zhang, and Shishir Patil for their insightful discussion.\\nWe thank Google Deepmind for funding this project, providing AI infrastructure, and provisioning Gemini endpoints.\\nSky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, SAP, Uber, and VMware.',\n",
       "   'title': '2. Related Works',\n",
       "   'chunks': [{'text': 'Adapters.\\nAdapters efficiently fine-tune models on specific tasks with minimal parameter changes, reducing computational and storage requirements while maintaining similar performance to full fine-tuning [7, 9, 12].\\nOur study focuses on retrieving and merging multiple Low-Rank adapters (LoRA), the popular approach within existing open-source communities [24, 25, 43].',\n",
       "     'title': '2. Related Works',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Adapter composition has emerged as a crucial mechanism for enhancing the capabilities of foundational models across various applications [17, 30, 34, 38, 39].\\nFor large language models (LLM), the linear combination of multiple adapters improves in-domain performance and cross-task generalization [3, 13, 14, 40, 41, 46].\\nIn the image domain, merging LoRAs effectively composes different tasks—concepts, characters, poses, actions, and styles—together, yielding images of high fidelity that closely align with user specifications [21, 47].\\nOur approach advances this further by actively segmenting user prompts into distinct tasks and merging the appropriate adapters for each task.',\n",
       "     'title': '2. Related Works',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Retrieval-based Methods.\\nRetrieval-based methods, such as retrieval-augmented generation (RAG), significantly improve model responses by adding semantically similar texts from a vast external database [16].\\nThese methods convert text to vector embeddings using text encoders, which are then ranked against a user prompt based on similarity metrics [4, 8, 18, 23, 31, 33].\\nSimilarly, our work draws inspiration from RAG to encode adapters as vector embedings: leveraging visual-language foundational models (VLM) to generate semantic descriptions of adapters, which are then translated into embeddings.',\n",
       "     'title': '2. Related Works',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'A core limitation to RAG is limited precision, retrieving distracting irrelevant documents.\\nThis leads to a”needlein-the-haystack” problem, where more relevant documents are buried further down the list [8].\\nRecent work introduce reranking step; this technique uses cross-encoders to assess both the raw user prompt and the ranked set of raw texts individually, thereby discovering texts based on actual relevance [23, 32].\\nRerankers have been successfully integrated with various LLM-application frameworks [2, 20, 29].',\n",
       "     'title': '2. Related Works',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Adapter selection presents distinct challenges compared to existing methods for retrieving text-based documents, as outlined in Section 2.\\nFirst, computing embeddings for adapters is a novel task, made more difficult without access to training datasets.\\nFurthermore, in the context of image generation, user prompts often specify multiple highly fine-grained tasks.\\nThis challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt.\\nFinally, composing multiple adapters can degrade image quality and inject foreign biases into the model.\\nOur three-stage framework below—Refine, Retrieve, and Compose—addresses the above challenges (Fig. 2).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'The refiner is a two-stage pipeline designed to generate textual descriptions of an adapter’s task and the corresponding text embeddings for retrieval purposes.\\nThis approach mirrors retrieval-based methods [16], which pre-compute embeddings over an external database of texts.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.1. Refiner',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Given an adapter Ai, the first stage is a vision-language model (VLM) that takes in the adapter’s model card—a set of randomly sampled example images from the model card Ii ∈ {Ii1, Ii2,}, the corresponding prompts Pi ∈ {pi1, pi2,}, and an author-provided description,2 Di—and returns an improved description D∗ i. Optionally, the VLM also recommends the weight for LoRA-based adapters, as the adapter weight is usually specified either in the author’s description Di or the set of prompts Pi, a feature present in popular image generation software [1].\\nIf information cannot be found, the LoRA’s weight is set to 0.8.\\nIn our experiments, these improved descriptions were generated by Gemini Ultra [37] (see § A.1 for prompt).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.1. Refiner',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'The second stage uses an embedding model (E) to generate embeddings e = E(D∗) for all adapters.\\nIn our experiments, we create embeddings from OpenAI’s text-embedding-3-large model [18, 26].\\nWe store pre-computed embeddings in a vector database.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.1. Refiner',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'The retriever fetches the most relevant adapters over the entirety of the user’s prompt using similarity metrics.\\nMathematically, the retriever employs the same embedding model (E) to process the user prompt, s, generating embedding es = E(s).\\nIt then calculates cosine similarity scores between the prompt’s embedding es and the embedding of each adapter in the matrix M. The top K adapters AK (K = 150, in our experiments) are selected based on the highest similarity scores: AK = arg top-Ki',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.2. Retriever',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '(es·Mi ∥es∥∥Mi∥) , where Mi is the ith row of the embedding matrix, representing the ith adapter’s embedding.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.2. Retriever',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'The composer serves a dual purpose: segmenting the prompt into tasks from a prompt’s keywords and assigning retrieved adapters to tasks.\\nThis implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the 2We note that a large set of author descriptions are inaccurate, misleading, or absent.\\nThe refiner helped correct for human errors by using generated images as the ground truth, significantly improving our system.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 2,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Figure 4.\\nQualitative comparison between Stylus over realistic (left) and cartoon (right) style Stable Diffusion checkpoints.\\nStylus produces highly detailed images that correctly depicts keywords in the context of the prompt.\\nFor the prompt “A graffiti of a corgi on the wall”, our method correctly depicts a spray-painted corgi, whereas the checkpoint generates a realistic dog.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': ' |  | RealisticVision-v6 / COCO | RealisticVision-v6 / PartiPrompts | Counterfeit-v3 / COCO | Counterfeit-v3 / PartiPrompts\\n | --- | --- | --- | --- | ---\\n | 80\\n | Stylus SD 1.5 | 68.31% | 67.81% | 62.00% | 58.57%\\n',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '41.43% 38.00%',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '32.19% 31.69%',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '0',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Figure 5.\\nHuman Evaluation.\\nStylus achieves a higher preference scores (2:1) over different datasets and Stable Diffusion checkpoints.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'prompt through keyword grounding.\\nFor example, if the prompt is “pandas eating bamboo”, the composer may discard an irrelevant “grizzly bears” adapter and a biased “panda mascots” adapter.\\nMathematically, the composer (C) takes in the prompt (s) and the top K adapters (AK) from the retriever, classifying them over different tasks,',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'T (s) = {t1, t2,, tn}.\\nThis can be expressed as: C(s,AK) = {(ti,Aki) | ti ∈ T (s),Aki ⊆ AK, ∀Aj ∈ Aki, Sim(Aj, ti) ≥ τ} (1), where Aki is the subset of adapters per task ti, Sim(Aj, ti) measures the similarity score between an adapter and a task, and τ is an arbitary threshold.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'While the composer can be trained with human-labeled data [28], we opt for a simpler approach that requires no training—prompting a long-context Large Language Model (LLM).\\nThe LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters.\\nIn our implementation, we choose Gemini 1.5, with a 128K context window, as the composer’s LLM (see App. A.2 for the full prompt).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Most importantly, Stylus’s composer parallels reranking, an advanced RAG technique.\\nRerankers employ cross encoders (F) that compare the retriever’s individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores: F(s, D∗).\\nThis prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment.\\nOur experimental ablations (§ 4.3) show that our composer outperforms existing rerankers (Cohere,',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'rerank-english-v2.0) [32].',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': ' | The composer identifies tasks assigns each task a set of relevant adapters as: C(Ak, s) = {(t1,Ak1), (t2,Ak2), | ti | from the prompt s Aki, formalized }. Our masking | and\\n | --- | --- | --- | ---\\n | strategy applies a binary mask, mask, Mi, can either be an one hot encoding, all ones, or all zeroes vector. Across all tasks, we perform a cross- product across masks, M1 ×M2 ×M3 × | Mi, | for each task ti. Each , generating | an\\n',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'exponential number of masking schemes.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'The combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images (§ 4.2.3).\\nThis approach also curtails the number of final adapters merged into the base model, minimizing the risk of composed adapters introducing undesirable effects to the image [47].',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Finally, an adapter’s weight (i.e. LoRA), which is extracted from the refiner (§ 3.1), is divided by the total number of adapters (after masking) in its corresponding task.\\nThis solves the problem of image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality (see App. A.4).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Adapter Testbed.\\nAdapter selection requires a large database of adapters to properly evaluate its performance.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 3,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '24',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '23',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '22',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '21',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '19',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > SD 1.5',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': ' |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Table 1.\\nEvaluation over different retrieval methods (CFG=6).',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Stylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'However, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Realistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'To demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.1 Human Evaluation.',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'To conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.1 Human Evaluation.',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'We assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.2 Automatic Benchmarks.',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'We use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge',\n",
       "     'page': 4,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Figure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '100',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '20 29',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '80',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '59',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '58 52',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '41 19 22',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '0',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': ' | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '(a) VLM as a Judge with GPT-4V',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Figure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '100',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '35',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '80',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': ' |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'uate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Given identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'calculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'GPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '4FID fails to disentangle image fidelity from diversity [27, 35].',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'A baby elephant underneath an adult elephant.',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '16s',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '25s',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'A building behind a stop sign and street sign.',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': '44s',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'A group of friends jumping on a bed.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Retriever Composer Load Adapter Image Generation',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': ' | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Figure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': ' | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Figure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'As shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'We benchmark Stylus’s performance relative to different retrieval methods.\\nFor all baselines below, we select the top three adapters and merge them into the base model.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Random: Adapters are randomly sampled without replacement from StylusDocs.\\nRetriever: The retriever emulates standard RAG pipelines [16, 46], functionally equivalent to Stylus without the composer stage.\\nTop adapters are fetched via cosine similarity over adapter embeddings.\\nConversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion.\\nThe random baseline chooses adapters that are orthogonal to the user prompt.\\nThus, these adapters alter unrelated concepts, which does not affect image generation.\\nIn fact, we observed that the distribution of random policy’s images in Fig. 10 were nearly identical to Stable Diffusion.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods',\n",
       "     'page': 6,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': ' |  | Stylus | Stylus\\n | --- | --- | ---\\n | Source | SD v1.5 | Source | SD v1.5\\n | --- | --- | --- | ---\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Stylus chooses relevant adapters that better adapt new styles and elements into existing images.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (a) Image Translation',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Dwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (a) Image Translation > Source Source',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Stylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (b) Inpainting',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Figure 12.\\nStylus over different image-to-image tasks.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (b) Inpainting',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'This section breaks down the latency introduced by various components of Stylus.\\nWe note that image generation time is independent of Stylus, as adapter weights are merged into the base model [12].',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.2 Breakdown of Stylus’s Inference Time',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Figure 11 demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts.\\nSpecifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds.\\nThe composer’s large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens.\\nFinally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process.\\nHowever, Stylus’s latency remains consistent across all batch sizes, as the composer and retriever run only once.\\nHence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.2 Breakdown of Stylus’s Inference Time',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Beyond text-to-image, Stylus applies across various imageto-image tasks.\\nFig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting, described as follows:',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.3 Image-Domain Tasks',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Image translation: Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt’s definition.\\nStylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style.\\nWe present examples in Fig 12a.\\nFor a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits.\\nFor a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.3 Image-Domain Tasks',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'Inpainting: Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask.\\nStylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity.\\nWe provide further examples in Fig. 12b, demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right).',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.3 Image-Domain Tasks',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'We propose Stylus, a new algorithm that automatically selects and composes adapters to generate better images.\\nOur method leverages a three-stage framework that precomputes adapters as lookup embeddings and retrieves most relevant adapters based on prompts’ keywords.\\nTo evaluate Stylus, we develop StylusDocs, a processed dataset featuring 75K adapters and pre-computed adapter embeddings.\\nOur evaluation of Stylus, across automatic metrics, humans, and vision-language models, demonstrate that Stylus achieves better visual fidelity, textual alignment, and image diversity than existing Stable Diffusion checkpoints.',\n",
       "     'title': '2. Related Works > 5. Conclusion',\n",
       "     'page': 7,\n",
       "     'source_doc': '2. Related Works'},\n",
       "    {'text': 'We thank Lisa Dunlap, Ansh Chaurasia, Siyuan Zhuang, Sijun Tan, Tianjun Zhang, and Shishir Patil for their insightful discussion.\\nWe thank Google Deepmind for funding this project, providing AI infrastructure, and provisioning Gemini endpoints.\\nSky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, SAP, Uber, and VMware.',\n",
       "     'title': '2. Related Works > 5. Conclusion > Acknowledgement',\n",
       "     'page': 8,\n",
       "     'source_doc': '2. Related Works'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316d2b880>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316d2be20>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae4340>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae5450>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae57e0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae6440>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae6650>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae6d10>]},\n",
       "  {'text': '3. Our Method: Stylus\\nAdapter selection presents distinct challenges compared to existing methods for retrieving text-based documents, as outlined in Section 2.\\nFirst, computing embeddings for adapters is a novel task, made more difficult without access to training datasets.\\nFurthermore, in the context of image generation, user prompts often specify multiple highly fine-grained tasks.\\nThis challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt.\\nFinally, composing multiple adapters can degrade image quality and inject foreign biases into the model.\\nOur three-stage framework below—Refine, Retrieve, and Compose—addresses the above challenges (Fig. 2).\\n3.1. Refiner\\nThe refiner is a two-stage pipeline designed to generate textual descriptions of an adapter’s task and the corresponding text embeddings for retrieval purposes.\\nThis approach mirrors retrieval-based methods [16], which pre-compute embeddings over an external database of texts.\\nGiven an adapter Ai, the first stage is a vision-language model (VLM) that takes in the adapter’s model card—a set of randomly sampled example images from the model card Ii ∈ {Ii1, Ii2,}, the corresponding prompts Pi ∈ {pi1, pi2,}, and an author-provided description,2 Di—and returns an improved description D∗ i. Optionally, the VLM also recommends the weight for LoRA-based adapters, as the adapter weight is usually specified either in the author’s description Di or the set of prompts Pi, a feature present in popular image generation software [1].\\nIf information cannot be found, the LoRA’s weight is set to 0.8.\\nIn our experiments, these improved descriptions were generated by Gemini Ultra [37] (see § A.1 for prompt).\\nThe second stage uses an embedding model (E) to generate embeddings e = E(D∗) for all adapters.\\nIn our experiments, we create embeddings from OpenAI’s text-embedding-3-large model [18, 26].\\nWe store pre-computed embeddings in a vector database.\\n3.2. Retriever\\nThe retriever fetches the most relevant adapters over the entirety of the user’s prompt using similarity metrics.\\nMathematically, the retriever employs the same embedding model (E) to process the user prompt, s, generating embedding es = E(s).\\nIt then calculates cosine similarity scores between the prompt’s embedding es and the embedding of each adapter in the matrix M. The top K adapters AK (K = 150, in our experiments) are selected based on the highest similarity scores: AK = arg top-Ki\\n(es·Mi ∥es∥∥Mi∥) , where Mi is the ith row of the embedding matrix, representing the ith adapter’s embedding.\\n3.3. Composer\\nThe composer serves a dual purpose: segmenting the prompt into tasks from a prompt’s keywords and assigning retrieved adapters to tasks.\\nThis implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the 2We note that a large set of author descriptions are inaccurate, misleading, or absent.\\nThe refiner helped correct for human errors by using generated images as the ground truth, significantly improving our system.\\nFigure 4.\\nQualitative comparison between Stylus over realistic (left) and cartoon (right) style Stable Diffusion checkpoints.\\nStylus produces highly detailed images that correctly depicts keywords in the context of the prompt.\\nFor the prompt “A graffiti of a corgi on the wall”, our method correctly depicts a spray-painted corgi, whereas the checkpoint generates a realistic dog.\\n |  | RealisticVision-v6 / COCO | RealisticVision-v6 / PartiPrompts | Counterfeit-v3 / COCO | Counterfeit-v3 / PartiPrompts\\n | --- | --- | --- | --- | ---\\n | 80\\n | Stylus SD 1.5 | 68.31% | 67.81% | 62.00% | 58.57%\\n\\n60\\n41.43% 38.00%\\n40\\n32.19% 31.69%\\n20\\n0\\nFigure 5.\\nHuman Evaluation.\\nStylus achieves a higher preference scores (2:1) over different datasets and Stable Diffusion checkpoints.\\nprompt through keyword grounding.\\nFor example, if the prompt is “pandas eating bamboo”, the composer may discard an irrelevant “grizzly bears” adapter and a biased “panda mascots” adapter.\\nMathematically, the composer (C) takes in the prompt (s) and the top K adapters (AK) from the retriever, classifying them over different tasks,\\nT (s) = {t1, t2,, tn}.\\nThis can be expressed as: C(s,AK) = {(ti,Aki) | ti ∈ T (s),Aki ⊆ AK, ∀Aj ∈ Aki, Sim(Aj, ti) ≥ τ} (1), where Aki is the subset of adapters per task ti, Sim(Aj, ti) measures the similarity score between an adapter and a task, and τ is an arbitary threshold.\\nWhile the composer can be trained with human-labeled data [28], we opt for a simpler approach that requires no training—prompting a long-context Large Language Model (LLM).\\nThe LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters.\\nIn our implementation, we choose Gemini 1.5, with a 128K context window, as the composer’s LLM (see App. A.2 for the full prompt).\\nMost importantly, Stylus’s composer parallels reranking, an advanced RAG technique.\\nRerankers employ cross encoders (F) that compare the retriever’s individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores: F(s, D∗).\\nThis prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment.\\nOur experimental ablations (§ 4.3) show that our composer outperforms existing rerankers (Cohere,\\nrerank-english-v2.0) [32].\\n3.4. Masking\\n | The composer identifies tasks assigns each task a set of relevant adapters as: C(Ak, s) = {(t1,Ak1), (t2,Ak2), | ti | from the prompt s Aki, formalized }. Our masking | and\\n | --- | --- | --- | ---\\n | strategy applies a binary mask, mask, Mi, can either be an one hot encoding, all ones, or all zeroes vector. Across all tasks, we perform a cross- product across masks, M1 ×M2 ×M3 × | Mi, | for each task ti. Each , generating | an\\n\\nexponential number of masking schemes.\\nThe combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images (§ 4.2.3).\\nThis approach also curtails the number of final adapters merged into the base model, minimizing the risk of composed adapters introducing undesirable effects to the image [47].\\nFinally, an adapter’s weight (i.e. LoRA), which is extracted from the refiner (§ 3.1), is divided by the total number of adapters (after masking) in its corresponding task.\\nThis solves the problem of image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality (see App. A.4).',\n",
       "   'title': '3. Our Method: Stylus',\n",
       "   'chunks': [{'text': 'Adapter selection presents distinct challenges compared to existing methods for retrieving text-based documents, as outlined in Section 2.\\nFirst, computing embeddings for adapters is a novel task, made more difficult without access to training datasets.\\nFurthermore, in the context of image generation, user prompts often specify multiple highly fine-grained tasks.\\nThis challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt.\\nFinally, composing multiple adapters can degrade image quality and inject foreign biases into the model.\\nOur three-stage framework below—Refine, Retrieve, and Compose—addresses the above challenges (Fig. 2).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus',\n",
       "     'page': 2,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'The refiner is a two-stage pipeline designed to generate textual descriptions of an adapter’s task and the corresponding text embeddings for retrieval purposes.\\nThis approach mirrors retrieval-based methods [16], which pre-compute embeddings over an external database of texts.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.1. Refiner',\n",
       "     'page': 2,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'Given an adapter Ai, the first stage is a vision-language model (VLM) that takes in the adapter’s model card—a set of randomly sampled example images from the model card Ii ∈ {Ii1, Ii2,}, the corresponding prompts Pi ∈ {pi1, pi2,}, and an author-provided description,2 Di—and returns an improved description D∗ i. Optionally, the VLM also recommends the weight for LoRA-based adapters, as the adapter weight is usually specified either in the author’s description Di or the set of prompts Pi, a feature present in popular image generation software [1].\\nIf information cannot be found, the LoRA’s weight is set to 0.8.\\nIn our experiments, these improved descriptions were generated by Gemini Ultra [37] (see § A.1 for prompt).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.1. Refiner',\n",
       "     'page': 2,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'The second stage uses an embedding model (E) to generate embeddings e = E(D∗) for all adapters.\\nIn our experiments, we create embeddings from OpenAI’s text-embedding-3-large model [18, 26].\\nWe store pre-computed embeddings in a vector database.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.1. Refiner',\n",
       "     'page': 2,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'The retriever fetches the most relevant adapters over the entirety of the user’s prompt using similarity metrics.\\nMathematically, the retriever employs the same embedding model (E) to process the user prompt, s, generating embedding es = E(s).\\nIt then calculates cosine similarity scores between the prompt’s embedding es and the embedding of each adapter in the matrix M. The top K adapters AK (K = 150, in our experiments) are selected based on the highest similarity scores: AK = arg top-Ki',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.2. Retriever',\n",
       "     'page': 2,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': '(es·Mi ∥es∥∥Mi∥) , where Mi is the ith row of the embedding matrix, representing the ith adapter’s embedding.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.2. Retriever',\n",
       "     'page': 2,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'The composer serves a dual purpose: segmenting the prompt into tasks from a prompt’s keywords and assigning retrieved adapters to tasks.\\nThis implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the 2We note that a large set of author descriptions are inaccurate, misleading, or absent.\\nThe refiner helped correct for human errors by using generated images as the ground truth, significantly improving our system.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 2,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'Figure 4.\\nQualitative comparison between Stylus over realistic (left) and cartoon (right) style Stable Diffusion checkpoints.\\nStylus produces highly detailed images that correctly depicts keywords in the context of the prompt.\\nFor the prompt “A graffiti of a corgi on the wall”, our method correctly depicts a spray-painted corgi, whereas the checkpoint generates a realistic dog.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': ' |  | RealisticVision-v6 / COCO | RealisticVision-v6 / PartiPrompts | Counterfeit-v3 / COCO | Counterfeit-v3 / PartiPrompts\\n | --- | --- | --- | --- | ---\\n | 80\\n | Stylus SD 1.5 | 68.31% | 67.81% | 62.00% | 58.57%\\n',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': '41.43% 38.00%',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': '32.19% 31.69%',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': '0',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'Figure 5.\\nHuman Evaluation.\\nStylus achieves a higher preference scores (2:1) over different datasets and Stable Diffusion checkpoints.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'prompt through keyword grounding.\\nFor example, if the prompt is “pandas eating bamboo”, the composer may discard an irrelevant “grizzly bears” adapter and a biased “panda mascots” adapter.\\nMathematically, the composer (C) takes in the prompt (s) and the top K adapters (AK) from the retriever, classifying them over different tasks,',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'T (s) = {t1, t2,, tn}.\\nThis can be expressed as: C(s,AK) = {(ti,Aki) | ti ∈ T (s),Aki ⊆ AK, ∀Aj ∈ Aki, Sim(Aj, ti) ≥ τ} (1), where Aki is the subset of adapters per task ti, Sim(Aj, ti) measures the similarity score between an adapter and a task, and τ is an arbitary threshold.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'While the composer can be trained with human-labeled data [28], we opt for a simpler approach that requires no training—prompting a long-context Large Language Model (LLM).\\nThe LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters.\\nIn our implementation, we choose Gemini 1.5, with a 128K context window, as the composer’s LLM (see App. A.2 for the full prompt).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'Most importantly, Stylus’s composer parallels reranking, an advanced RAG technique.\\nRerankers employ cross encoders (F) that compare the retriever’s individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores: F(s, D∗).\\nThis prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment.\\nOur experimental ablations (§ 4.3) show that our composer outperforms existing rerankers (Cohere,',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'rerank-english-v2.0) [32].',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': ' | The composer identifies tasks assigns each task a set of relevant adapters as: C(Ak, s) = {(t1,Ak1), (t2,Ak2), | ti | from the prompt s Aki, formalized }. Our masking | and\\n | --- | --- | --- | ---\\n | strategy applies a binary mask, mask, Mi, can either be an one hot encoding, all ones, or all zeroes vector. Across all tasks, we perform a cross- product across masks, M1 ×M2 ×M3 × | Mi, | for each task ti. Each , generating | an\\n',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'exponential number of masking schemes.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'The combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images (§ 4.2.3).\\nThis approach also curtails the number of final adapters merged into the base model, minimizing the risk of composed adapters introducing undesirable effects to the image [47].',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'},\n",
       "    {'text': 'Finally, an adapter’s weight (i.e. LoRA), which is extracted from the refiner (§ 3.1), is divided by the total number of adapters (after masking) in its corresponding task.\\nThis solves the problem of image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality (see App. A.4).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '3. Our Method: Stylus'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316d2b880>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff316d2be20>]},\n",
       "  {'text': '3.1. Refiner\\nThe refiner is a two-stage pipeline designed to generate textual descriptions of an adapter’s task and the corresponding text embeddings for retrieval purposes.\\nThis approach mirrors retrieval-based methods [16], which pre-compute embeddings over an external database of texts.\\nGiven an adapter Ai, the first stage is a vision-language model (VLM) that takes in the adapter’s model card—a set of randomly sampled example images from the model card Ii ∈ {Ii1, Ii2,}, the corresponding prompts Pi ∈ {pi1, pi2,}, and an author-provided description,2 Di—and returns an improved description D∗ i. Optionally, the VLM also recommends the weight for LoRA-based adapters, as the adapter weight is usually specified either in the author’s description Di or the set of prompts Pi, a feature present in popular image generation software [1].\\nIf information cannot be found, the LoRA’s weight is set to 0.8.\\nIn our experiments, these improved descriptions were generated by Gemini Ultra [37] (see § A.1 for prompt).\\nThe second stage uses an embedding model (E) to generate embeddings e = E(D∗) for all adapters.\\nIn our experiments, we create embeddings from OpenAI’s text-embedding-3-large model [18, 26].\\nWe store pre-computed embeddings in a vector database.',\n",
       "   'title': '3.1. Refiner',\n",
       "   'chunks': [{'text': 'The refiner is a two-stage pipeline designed to generate textual descriptions of an adapter’s task and the corresponding text embeddings for retrieval purposes.\\nThis approach mirrors retrieval-based methods [16], which pre-compute embeddings over an external database of texts.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.1. Refiner',\n",
       "     'page': 2,\n",
       "     'source_doc': '3.1. Refiner'},\n",
       "    {'text': 'Given an adapter Ai, the first stage is a vision-language model (VLM) that takes in the adapter’s model card—a set of randomly sampled example images from the model card Ii ∈ {Ii1, Ii2,}, the corresponding prompts Pi ∈ {pi1, pi2,}, and an author-provided description,2 Di—and returns an improved description D∗ i. Optionally, the VLM also recommends the weight for LoRA-based adapters, as the adapter weight is usually specified either in the author’s description Di or the set of prompts Pi, a feature present in popular image generation software [1].\\nIf information cannot be found, the LoRA’s weight is set to 0.8.\\nIn our experiments, these improved descriptions were generated by Gemini Ultra [37] (see § A.1 for prompt).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.1. Refiner',\n",
       "     'page': 2,\n",
       "     'source_doc': '3.1. Refiner'},\n",
       "    {'text': 'The second stage uses an embedding model (E) to generate embeddings e = E(D∗) for all adapters.\\nIn our experiments, we create embeddings from OpenAI’s text-embedding-3-large model [18, 26].\\nWe store pre-computed embeddings in a vector database.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.1. Refiner',\n",
       "     'page': 2,\n",
       "     'source_doc': '3.1. Refiner'}],\n",
       "   'tables': []},\n",
       "  {'text': '3.2. Retriever\\nThe retriever fetches the most relevant adapters over the entirety of the user’s prompt using similarity metrics.\\nMathematically, the retriever employs the same embedding model (E) to process the user prompt, s, generating embedding es = E(s).\\nIt then calculates cosine similarity scores between the prompt’s embedding es and the embedding of each adapter in the matrix M. The top K adapters AK (K = 150, in our experiments) are selected based on the highest similarity scores: AK = arg top-Ki\\n(es·Mi ∥es∥∥Mi∥) , where Mi is the ith row of the embedding matrix, representing the ith adapter’s embedding.',\n",
       "   'title': '3.2. Retriever',\n",
       "   'chunks': [{'text': 'The retriever fetches the most relevant adapters over the entirety of the user’s prompt using similarity metrics.\\nMathematically, the retriever employs the same embedding model (E) to process the user prompt, s, generating embedding es = E(s).\\nIt then calculates cosine similarity scores between the prompt’s embedding es and the embedding of each adapter in the matrix M. The top K adapters AK (K = 150, in our experiments) are selected based on the highest similarity scores: AK = arg top-Ki',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.2. Retriever',\n",
       "     'page': 2,\n",
       "     'source_doc': '3.2. Retriever'},\n",
       "    {'text': '(es·Mi ∥es∥∥Mi∥) , where Mi is the ith row of the embedding matrix, representing the ith adapter’s embedding.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.2. Retriever',\n",
       "     'page': 2,\n",
       "     'source_doc': '3.2. Retriever'}],\n",
       "   'tables': []},\n",
       "  {'text': '3.3. Composer\\nThe composer serves a dual purpose: segmenting the prompt into tasks from a prompt’s keywords and assigning retrieved adapters to tasks.\\nThis implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the 2We note that a large set of author descriptions are inaccurate, misleading, or absent.\\nThe refiner helped correct for human errors by using generated images as the ground truth, significantly improving our system.\\nFigure 4.\\nQualitative comparison between Stylus over realistic (left) and cartoon (right) style Stable Diffusion checkpoints.\\nStylus produces highly detailed images that correctly depicts keywords in the context of the prompt.\\nFor the prompt “A graffiti of a corgi on the wall”, our method correctly depicts a spray-painted corgi, whereas the checkpoint generates a realistic dog.\\n |  | RealisticVision-v6 / COCO | RealisticVision-v6 / PartiPrompts | Counterfeit-v3 / COCO | Counterfeit-v3 / PartiPrompts\\n | --- | --- | --- | --- | ---\\n | 80\\n | Stylus SD 1.5 | 68.31% | 67.81% | 62.00% | 58.57%\\n\\n60\\n41.43% 38.00%\\n40\\n32.19% 31.69%\\n20\\n0\\nFigure 5.\\nHuman Evaluation.\\nStylus achieves a higher preference scores (2:1) over different datasets and Stable Diffusion checkpoints.\\nprompt through keyword grounding.\\nFor example, if the prompt is “pandas eating bamboo”, the composer may discard an irrelevant “grizzly bears” adapter and a biased “panda mascots” adapter.\\nMathematically, the composer (C) takes in the prompt (s) and the top K adapters (AK) from the retriever, classifying them over different tasks,\\nT (s) = {t1, t2,, tn}.\\nThis can be expressed as: C(s,AK) = {(ti,Aki) | ti ∈ T (s),Aki ⊆ AK, ∀Aj ∈ Aki, Sim(Aj, ti) ≥ τ} (1), where Aki is the subset of adapters per task ti, Sim(Aj, ti) measures the similarity score between an adapter and a task, and τ is an arbitary threshold.\\nWhile the composer can be trained with human-labeled data [28], we opt for a simpler approach that requires no training—prompting a long-context Large Language Model (LLM).\\nThe LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters.\\nIn our implementation, we choose Gemini 1.5, with a 128K context window, as the composer’s LLM (see App. A.2 for the full prompt).\\nMost importantly, Stylus’s composer parallels reranking, an advanced RAG technique.\\nRerankers employ cross encoders (F) that compare the retriever’s individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores: F(s, D∗).\\nThis prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment.\\nOur experimental ablations (§ 4.3) show that our composer outperforms existing rerankers (Cohere,\\nrerank-english-v2.0) [32].',\n",
       "   'title': '3.3. Composer',\n",
       "   'chunks': [{'text': 'The composer serves a dual purpose: segmenting the prompt into tasks from a prompt’s keywords and assigning retrieved adapters to tasks.\\nThis implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the 2We note that a large set of author descriptions are inaccurate, misleading, or absent.\\nThe refiner helped correct for human errors by using generated images as the ground truth, significantly improving our system.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 2,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': 'Figure 4.\\nQualitative comparison between Stylus over realistic (left) and cartoon (right) style Stable Diffusion checkpoints.\\nStylus produces highly detailed images that correctly depicts keywords in the context of the prompt.\\nFor the prompt “A graffiti of a corgi on the wall”, our method correctly depicts a spray-painted corgi, whereas the checkpoint generates a realistic dog.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': ' |  | RealisticVision-v6 / COCO | RealisticVision-v6 / PartiPrompts | Counterfeit-v3 / COCO | Counterfeit-v3 / PartiPrompts\\n | --- | --- | --- | --- | ---\\n | 80\\n | Stylus SD 1.5 | 68.31% | 67.81% | 62.00% | 58.57%\\n',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': '41.43% 38.00%',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': '32.19% 31.69%',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': '0',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': 'Figure 5.\\nHuman Evaluation.\\nStylus achieves a higher preference scores (2:1) over different datasets and Stable Diffusion checkpoints.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': 'prompt through keyword grounding.\\nFor example, if the prompt is “pandas eating bamboo”, the composer may discard an irrelevant “grizzly bears” adapter and a biased “panda mascots” adapter.\\nMathematically, the composer (C) takes in the prompt (s) and the top K adapters (AK) from the retriever, classifying them over different tasks,',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': 'T (s) = {t1, t2,, tn}.\\nThis can be expressed as: C(s,AK) = {(ti,Aki) | ti ∈ T (s),Aki ⊆ AK, ∀Aj ∈ Aki, Sim(Aj, ti) ≥ τ} (1), where Aki is the subset of adapters per task ti, Sim(Aj, ti) measures the similarity score between an adapter and a task, and τ is an arbitary threshold.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': 'While the composer can be trained with human-labeled data [28], we opt for a simpler approach that requires no training—prompting a long-context Large Language Model (LLM).\\nThe LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters.\\nIn our implementation, we choose Gemini 1.5, with a 128K context window, as the composer’s LLM (see App. A.2 for the full prompt).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': 'Most importantly, Stylus’s composer parallels reranking, an advanced RAG technique.\\nRerankers employ cross encoders (F) that compare the retriever’s individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores: F(s, D∗).\\nThis prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment.\\nOur experimental ablations (§ 4.3) show that our composer outperforms existing rerankers (Cohere,',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'},\n",
       "    {'text': 'rerank-english-v2.0) [32].',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.3. Composer',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.3. Composer'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316d2b880>]},\n",
       "  {'text': '3.4. Masking\\n | The composer identifies tasks assigns each task a set of relevant adapters as: C(Ak, s) = {(t1,Ak1), (t2,Ak2), | ti | from the prompt s Aki, formalized }. Our masking | and\\n | --- | --- | --- | ---\\n | strategy applies a binary mask, mask, Mi, can either be an one hot encoding, all ones, or all zeroes vector. Across all tasks, we perform a cross- product across masks, M1 ×M2 ×M3 × | Mi, | for each task ti. Each , generating | an\\n\\nexponential number of masking schemes.\\nThe combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images (§ 4.2.3).\\nThis approach also curtails the number of final adapters merged into the base model, minimizing the risk of composed adapters introducing undesirable effects to the image [47].\\nFinally, an adapter’s weight (i.e. LoRA), which is extracted from the refiner (§ 3.1), is divided by the total number of adapters (after masking) in its corresponding task.\\nThis solves the problem of image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality (see App. A.4).',\n",
       "   'title': '3.4. Masking',\n",
       "   'chunks': [{'text': ' | The composer identifies tasks assigns each task a set of relevant adapters as: C(Ak, s) = {(t1,Ak1), (t2,Ak2), | ti | from the prompt s Aki, formalized }. Our masking | and\\n | --- | --- | --- | ---\\n | strategy applies a binary mask, mask, Mi, can either be an one hot encoding, all ones, or all zeroes vector. Across all tasks, we perform a cross- product across masks, M1 ×M2 ×M3 × | Mi, | for each task ti. Each , generating | an\\n',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.4. Masking'},\n",
       "    {'text': 'exponential number of masking schemes.',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.4. Masking'},\n",
       "    {'text': 'The combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images (§ 4.2.3).\\nThis approach also curtails the number of final adapters merged into the base model, minimizing the risk of composed adapters introducing undesirable effects to the image [47].',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.4. Masking'},\n",
       "    {'text': 'Finally, an adapter’s weight (i.e. LoRA), which is extracted from the refiner (§ 3.1), is divided by the total number of adapters (after masking) in its corresponding task.\\nThis solves the problem of image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality (see App. A.4).',\n",
       "     'title': '2. Related Works > 3. Our Method: Stylus > 3.4. Masking',\n",
       "     'page': 3,\n",
       "     'source_doc': '3.4. Masking'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff316d2be20>]},\n",
       "  {'text': '4. Results\\n4.1. Experimental Setup\\nAdapter Testbed.\\nAdapter selection requires a large database of adapters to properly evaluate its performance.\\n24\\n23\\n22\\n21\\n20\\nSD 1.5\\n19\\nStylus\\n |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n\\nTable 1.\\nEvaluation over different retrieval methods (CFG=6).\\nStylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.\\nHowever, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:\\nRealistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.\\n4.2. Main Experiments\\n4.2.1 Human Evaluation.\\nTo demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.\\nTo conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.\\n4.2.2 Automatic Benchmarks.\\nWe assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.\\n4.2.3 VLM as a Judge\\nWe use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.\\nFigure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.\\n Lose Tie Win\\n100\\n20 29\\n80\\n59\\n60\\n58 52\\n40\\n20\\n41 19 22\\nPrompt Length\\n0\\n | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n\\n(a) VLM as a Judge with GPT-4V\\nFigure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.\\nLose Tie Win\\n100\\n35\\n80\\n60\\n40\\n40\\n20\\n |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n\\nuate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.\\n4.2.4 Diversity per Prompt\\nGiven identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:\\ncalculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.\\nGPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,\\n4FID fails to disentangle image fidelity from diversity [27, 35].\\nStylus\\nReranker\\nRetriever\\nRandom\\nA baby elephant underneath an adult elephant.\\n16s\\n25s\\nA building behind a stop sign and street sign.\\n44s\\nTime (s)\\nA group of friends jumping on a bed.\\nRetriever Composer Load Adapter Image Generation\\n | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n\\nFigure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.\\n | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n\\nFigure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.\\nAs shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.\\n4.3. Ablations\\n4.3.1 Alternative Retrieval-based Methods\\nWe benchmark Stylus’s performance relative to different retrieval methods.\\nFor all baselines below, we select the top three adapters and merge them into the base model.\\nRandom: Adapters are randomly sampled without replacement from StylusDocs.\\nRetriever: The retriever emulates standard RAG pipelines [16, 46], functionally equivalent to Stylus without the composer stage.\\nTop adapters are fetched via cosine similarity over adapter embeddings.\\nConversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion.\\nThe random baseline chooses adapters that are orthogonal to the user prompt.\\nThus, these adapters alter unrelated concepts, which does not affect image generation.\\nIn fact, we observed that the distribution of random policy’s images in Fig. 10 were nearly identical to Stable Diffusion.\\n |  | Stylus | Stylus\\n | --- | --- | ---\\n | Source | SD v1.5 | Source | SD v1.5\\n | --- | --- | --- | ---\\n\\nVolcanoes spewing lava\\nStudio Ghibli Style\\nFiery red Voxel Style Pencil Sketch Van Gogh Style\\n(a) Image Translation\\nStylus chooses relevant adapters that better adapt new styles and elements into existing images.\\nStylus Stylus\\nSD v1.5 SD v1.5\\nSource Source\\nDwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny\\n(b) Inpainting\\nStylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.\\nFigure 12.\\nStylus over different image-to-image tasks.\\n4.3.2 Breakdown of Stylus’s Inference Time\\nThis section breaks down the latency introduced by various components of Stylus.\\nWe note that image generation time is independent of Stylus, as adapter weights are merged into the base model [12].\\nFigure 11 demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts.\\nSpecifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds.\\nThe composer’s large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens.\\nFinally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process.\\nHowever, Stylus’s latency remains consistent across all batch sizes, as the composer and retriever run only once.\\nHence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases.\\n4.3.3 Image-Domain Tasks\\nBeyond text-to-image, Stylus applies across various imageto-image tasks.\\nFig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting, described as follows:\\nImage translation: Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt’s definition.\\nStylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style.\\nWe present examples in Fig 12a.\\nFor a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits.\\nFor a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.\\nInpainting: Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask.\\nStylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity.\\nWe provide further examples in Fig. 12b, demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right).',\n",
       "   'title': '4. Results',\n",
       "   'chunks': [{'text': 'Adapter Testbed.\\nAdapter selection requires a large database of adapters to properly evaluate its performance.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 3,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '24',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '23',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '22',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '21',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '19',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > SD 1.5',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': ' |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Table 1.\\nEvaluation over different retrieval methods (CFG=6).',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Stylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'However, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Realistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'To demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.1 Human Evaluation.',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'To conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.1 Human Evaluation.',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'We assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.2 Automatic Benchmarks.',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'We use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge',\n",
       "     'page': 4,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Figure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '100',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '20 29',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '80',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '59',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '58 52',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '41 19 22',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '0',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': ' | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '(a) VLM as a Judge with GPT-4V',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Figure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '100',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '35',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '80',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': ' |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'uate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Given identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'calculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'GPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '4FID fails to disentangle image fidelity from diversity [27, 35].',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'A baby elephant underneath an adult elephant.',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '16s',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '25s',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'A building behind a stop sign and street sign.',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': '44s',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'A group of friends jumping on a bed.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Retriever Composer Load Adapter Image Generation',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': ' | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Figure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': ' | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Figure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'As shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'We benchmark Stylus’s performance relative to different retrieval methods.\\nFor all baselines below, we select the top three adapters and merge them into the base model.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Random: Adapters are randomly sampled without replacement from StylusDocs.\\nRetriever: The retriever emulates standard RAG pipelines [16, 46], functionally equivalent to Stylus without the composer stage.\\nTop adapters are fetched via cosine similarity over adapter embeddings.\\nConversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion.\\nThe random baseline chooses adapters that are orthogonal to the user prompt.\\nThus, these adapters alter unrelated concepts, which does not affect image generation.\\nIn fact, we observed that the distribution of random policy’s images in Fig. 10 were nearly identical to Stable Diffusion.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods',\n",
       "     'page': 6,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': ' |  | Stylus | Stylus\\n | --- | --- | ---\\n | Source | SD v1.5 | Source | SD v1.5\\n | --- | --- | --- | ---\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods',\n",
       "     'page': 7,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Stylus chooses relevant adapters that better adapt new styles and elements into existing images.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (a) Image Translation',\n",
       "     'page': 7,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Dwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (a) Image Translation > Source Source',\n",
       "     'page': 7,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Stylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (b) Inpainting',\n",
       "     'page': 7,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Figure 12.\\nStylus over different image-to-image tasks.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (b) Inpainting',\n",
       "     'page': 7,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'This section breaks down the latency introduced by various components of Stylus.\\nWe note that image generation time is independent of Stylus, as adapter weights are merged into the base model [12].',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.2 Breakdown of Stylus’s Inference Time',\n",
       "     'page': 7,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Figure 11 demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts.\\nSpecifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds.\\nThe composer’s large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens.\\nFinally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process.\\nHowever, Stylus’s latency remains consistent across all batch sizes, as the composer and retriever run only once.\\nHence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.2 Breakdown of Stylus’s Inference Time',\n",
       "     'page': 7,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Beyond text-to-image, Stylus applies across various imageto-image tasks.\\nFig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting, described as follows:',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.3 Image-Domain Tasks',\n",
       "     'page': 7,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Image translation: Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt’s definition.\\nStylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style.\\nWe present examples in Fig 12a.\\nFor a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits.\\nFor a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.3 Image-Domain Tasks',\n",
       "     'page': 7,\n",
       "     'source_doc': '4. Results'},\n",
       "    {'text': 'Inpainting: Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask.\\nStylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity.\\nWe provide further examples in Fig. 12b, demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right).',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.3 Image-Domain Tasks',\n",
       "     'page': 7,\n",
       "     'source_doc': '4. Results'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edae4340>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae5450>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae57e0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae6440>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae6650>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae6d10>]},\n",
       "  {'text': '4.1. Experimental Setup\\nAdapter Testbed.\\nAdapter selection requires a large database of adapters to properly evaluate its performance.\\n24\\n23\\n22\\n21\\n20\\nSD 1.5\\n19\\nStylus\\n |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n\\nTable 1.\\nEvaluation over different retrieval methods (CFG=6).\\nStylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.\\nHowever, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:\\nRealistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.',\n",
       "   'title': '4.1. Experimental Setup',\n",
       "   'chunks': [{'text': 'Adapter Testbed.\\nAdapter selection requires a large database of adapters to properly evaluate its performance.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 3,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': '24',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': '23',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': '22',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': '21',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': '19',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > SD 1.5',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': ' |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': 'Table 1.\\nEvaluation over different retrieval methods (CFG=6).',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': 'Stylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': 'However, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'},\n",
       "    {'text': 'Realistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.1. Experimental Setup'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edae4340>]},\n",
       "  {'text': 'SD 1.5\\n19',\n",
       "   'title': 'SD 1.5',\n",
       "   'chunks': [{'text': '19',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > SD 1.5',\n",
       "     'page': 4,\n",
       "     'source_doc': 'SD 1.5'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Stylus\\n |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n\\nTable 1.\\nEvaluation over different retrieval methods (CFG=6).\\nStylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.\\nHowever, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:\\nRealistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.',\n",
       "   'title': 'Stylus',\n",
       "   'chunks': [{'text': ' |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': 'Table 1.\\nEvaluation over different retrieval methods (CFG=6).',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': 'Stylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': 'However, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': 'Realistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.',\n",
       "     'title': '2. Related Works > 4. Results > 4.1. Experimental Setup > Stylus',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Stylus'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edae4340>]},\n",
       "  {'text': '4.2. Main Experiments\\n4.2.1 Human Evaluation.\\nTo demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.\\nTo conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.\\n4.2.2 Automatic Benchmarks.\\nWe assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.\\n4.2.3 VLM as a Judge\\nWe use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.\\nFigure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.\\n Lose Tie Win\\n100\\n20 29\\n80\\n59\\n60\\n58 52\\n40\\n20\\n41 19 22\\nPrompt Length\\n0\\n | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n\\n(a) VLM as a Judge with GPT-4V\\nFigure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.\\nLose Tie Win\\n100\\n35\\n80\\n60\\n40\\n40\\n20\\n |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n\\nuate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.\\n4.2.4 Diversity per Prompt\\nGiven identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:\\ncalculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.\\nGPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,\\n4FID fails to disentangle image fidelity from diversity [27, 35].',\n",
       "   'title': '4.2. Main Experiments',\n",
       "   'chunks': [{'text': 'To demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.1 Human Evaluation.',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': 'To conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.1 Human Evaluation.',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': 'We assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.2 Automatic Benchmarks.',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': 'We use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': 'Figure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '100',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '20 29',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '80',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '59',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '58 52',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '41 19 22',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '0',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': ' | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '(a) VLM as a Judge with GPT-4V',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': 'Figure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '100',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '35',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '80',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': ' |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': 'uate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': 'Given identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': 'calculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': 'GPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'},\n",
       "    {'text': '4FID fails to disentangle image fidelity from diversity [27, 35].',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2. Main Experiments'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edae5450>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae57e0>]},\n",
       "  {'text': '4.2.1 Human Evaluation.\\nTo demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.\\nTo conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.',\n",
       "   'title': '4.2.1 Human Evaluation.',\n",
       "   'chunks': [{'text': 'To demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.1 Human Evaluation.',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.2.1 Human Evaluation.'},\n",
       "    {'text': 'To conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.1 Human Evaluation.',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.2.1 Human Evaluation.'}],\n",
       "   'tables': []},\n",
       "  {'text': '4.2.2 Automatic Benchmarks.\\nWe assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.',\n",
       "   'title': '4.2.2 Automatic Benchmarks.',\n",
       "   'chunks': [{'text': 'We assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.2 Automatic Benchmarks.',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.2.2 Automatic Benchmarks.'}],\n",
       "   'tables': []},\n",
       "  {'text': '4.2.3 VLM as a Judge\\nWe use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.\\nFigure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.\\n Lose Tie Win\\n100\\n20 29\\n80\\n59\\n60\\n58 52\\n40\\n20\\n41 19 22\\nPrompt Length\\n0\\n | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n\\n(a) VLM as a Judge with GPT-4V\\nFigure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.\\nLose Tie Win\\n100\\n35\\n80\\n60\\n40\\n40\\n20\\n |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n\\nuate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.',\n",
       "   'title': '4.2.3 VLM as a Judge',\n",
       "   'chunks': [{'text': 'We use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge',\n",
       "     'page': 4,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': 'Figure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '100',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '20 29',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '80',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '59',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '58 52',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '41 19 22',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '0',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': ' | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '(a) VLM as a Judge with GPT-4V',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': 'Figure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '100',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '35',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '80',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': ' |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'},\n",
       "    {'text': 'uate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.3 VLM as a Judge'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edae5450>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae57e0>]},\n",
       "  {'text': ' Lose Tie Win\\n100\\n20 29\\n80\\n59\\n60\\n58 52\\n40\\n20\\n41 19 22',\n",
       "   'title': ' Lose Tie Win',\n",
       "   'chunks': [{'text': '100',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': ' Lose Tie Win'},\n",
       "    {'text': '20 29',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': ' Lose Tie Win'},\n",
       "    {'text': '80',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': ' Lose Tie Win'},\n",
       "    {'text': '59',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': ' Lose Tie Win'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': ' Lose Tie Win'},\n",
       "    {'text': '58 52',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': ' Lose Tie Win'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': ' Lose Tie Win'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': ' Lose Tie Win'},\n",
       "    {'text': '41 19 22',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge >  Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': ' Lose Tie Win'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Prompt Length\\n0\\n | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n\\n(a) VLM as a Judge with GPT-4V\\nFigure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.',\n",
       "   'title': 'Prompt Length',\n",
       "   'chunks': [{'text': '0',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Prompt Length'},\n",
       "    {'text': ' | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Prompt Length'},\n",
       "    {'text': '(a) VLM as a Judge with GPT-4V',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Prompt Length'},\n",
       "    {'text': 'Figure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Prompt Length',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Prompt Length'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edae5450>]},\n",
       "  {'text': 'Lose Tie Win\\n100\\n35\\n80\\n60\\n40\\n40\\n20\\n |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n\\nuate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.',\n",
       "   'title': 'Lose Tie Win',\n",
       "   'chunks': [{'text': '100',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Lose Tie Win'},\n",
       "    {'text': '35',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Lose Tie Win'},\n",
       "    {'text': '80',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Lose Tie Win'},\n",
       "    {'text': '60',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Lose Tie Win'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Lose Tie Win'},\n",
       "    {'text': '40',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Lose Tie Win'},\n",
       "    {'text': '20',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Lose Tie Win'},\n",
       "    {'text': ' |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Lose Tie Win'},\n",
       "    {'text': 'uate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.3 VLM as a Judge > Lose Tie Win',\n",
       "     'page': 5,\n",
       "     'source_doc': 'Lose Tie Win'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edae57e0>]},\n",
       "  {'text': '4.2.4 Diversity per Prompt\\nGiven identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:\\ncalculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.\\nGPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,\\n4FID fails to disentangle image fidelity from diversity [27, 35].',\n",
       "   'title': '4.2.4 Diversity per Prompt',\n",
       "   'chunks': [{'text': 'Given identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.4 Diversity per Prompt'},\n",
       "    {'text': 'calculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.4 Diversity per Prompt'},\n",
       "    {'text': 'GPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.4 Diversity per Prompt'},\n",
       "    {'text': '4FID fails to disentangle image fidelity from diversity [27, 35].',\n",
       "     'title': '2. Related Works > 4. Results > 4.2. Main Experiments > 4.2.4 Diversity per Prompt',\n",
       "     'page': 5,\n",
       "     'source_doc': '4.2.4 Diversity per Prompt'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Stylus', 'title': 'Stylus', 'chunks': [], 'tables': []},\n",
       "  {'text': 'Reranker', 'title': 'Reranker', 'chunks': [], 'tables': []},\n",
       "  {'text': 'Retriever', 'title': 'Retriever', 'chunks': [], 'tables': []},\n",
       "  {'text': 'Random\\nA baby elephant underneath an adult elephant.\\n16s\\n25s\\nA building behind a stop sign and street sign.\\n44s\\nTime (s)\\nA group of friends jumping on a bed.\\nRetriever Composer Load Adapter Image Generation\\n | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n\\nFigure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.\\n | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n\\nFigure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.\\nAs shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.',\n",
       "   'title': 'Random',\n",
       "   'chunks': [{'text': 'A baby elephant underneath an adult elephant.',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': '16s',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': '25s',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': 'A building behind a stop sign and street sign.',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': '44s',\n",
       "     'title': '2. Related Works > 4. Results > Random',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': 'A group of friends jumping on a bed.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': 'Retriever Composer Load Adapter Image Generation',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': ' | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': 'Figure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': ' | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': 'Figure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'},\n",
       "    {'text': 'As shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Random'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edae6440>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae6650>]},\n",
       "  {'text': 'Time (s)\\nA group of friends jumping on a bed.\\nRetriever Composer Load Adapter Image Generation\\n | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n\\nFigure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.\\n | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n\\nFigure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.\\nAs shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.',\n",
       "   'title': 'Time (s)',\n",
       "   'chunks': [{'text': 'A group of friends jumping on a bed.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Time (s)'},\n",
       "    {'text': 'Retriever Composer Load Adapter Image Generation',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Time (s)'},\n",
       "    {'text': ' | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Time (s)'},\n",
       "    {'text': 'Figure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Time (s)'},\n",
       "    {'text': ' | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Time (s)'},\n",
       "    {'text': 'Figure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Time (s)'},\n",
       "    {'text': 'As shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.',\n",
       "     'title': '2. Related Works > 4. Results > Random > Time (s)',\n",
       "     'page': 6,\n",
       "     'source_doc': 'Time (s)'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edae6440>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edae6650>]},\n",
       "  {'text': '4.3. Ablations',\n",
       "   'title': '4.3. Ablations',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': '4.3.1 Alternative Retrieval-based Methods\\nWe benchmark Stylus’s performance relative to different retrieval methods.\\nFor all baselines below, we select the top three adapters and merge them into the base model.\\nRandom: Adapters are randomly sampled without replacement from StylusDocs.\\nRetriever: The retriever emulates standard RAG pipelines [16, 46], functionally equivalent to Stylus without the composer stage.\\nTop adapters are fetched via cosine similarity over adapter embeddings.\\nConversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion.\\nThe random baseline chooses adapters that are orthogonal to the user prompt.\\nThus, these adapters alter unrelated concepts, which does not affect image generation.\\nIn fact, we observed that the distribution of random policy’s images in Fig. 10 were nearly identical to Stable Diffusion.\\n |  | Stylus | Stylus\\n | --- | --- | ---\\n | Source | SD v1.5 | Source | SD v1.5\\n | --- | --- | --- | ---\\n\\nVolcanoes spewing lava\\nStudio Ghibli Style\\nFiery red Voxel Style Pencil Sketch Van Gogh Style\\n(a) Image Translation\\nStylus chooses relevant adapters that better adapt new styles and elements into existing images.\\nStylus Stylus\\nSD v1.5 SD v1.5\\nSource Source\\nDwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny\\n(b) Inpainting\\nStylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.\\nFigure 12.\\nStylus over different image-to-image tasks.',\n",
       "   'title': '4.3.1 Alternative Retrieval-based Methods',\n",
       "   'chunks': [{'text': 'We benchmark Stylus’s performance relative to different retrieval methods.\\nFor all baselines below, we select the top three adapters and merge them into the base model.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods',\n",
       "     'page': 6,\n",
       "     'source_doc': '4.3.1 Alternative Retrieval-based Methods'},\n",
       "    {'text': 'Random: Adapters are randomly sampled without replacement from StylusDocs.\\nRetriever: The retriever emulates standard RAG pipelines [16, 46], functionally equivalent to Stylus without the composer stage.\\nTop adapters are fetched via cosine similarity over adapter embeddings.\\nConversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion.\\nThe random baseline chooses adapters that are orthogonal to the user prompt.\\nThus, these adapters alter unrelated concepts, which does not affect image generation.\\nIn fact, we observed that the distribution of random policy’s images in Fig. 10 were nearly identical to Stable Diffusion.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods',\n",
       "     'page': 6,\n",
       "     'source_doc': '4.3.1 Alternative Retrieval-based Methods'},\n",
       "    {'text': ' |  | Stylus | Stylus\\n | --- | --- | ---\\n | Source | SD v1.5 | Source | SD v1.5\\n | --- | --- | --- | ---\\n',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods',\n",
       "     'page': 7,\n",
       "     'source_doc': '4.3.1 Alternative Retrieval-based Methods'},\n",
       "    {'text': 'Stylus chooses relevant adapters that better adapt new styles and elements into existing images.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (a) Image Translation',\n",
       "     'page': 7,\n",
       "     'source_doc': '4.3.1 Alternative Retrieval-based Methods'},\n",
       "    {'text': 'Dwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (a) Image Translation > Source Source',\n",
       "     'page': 7,\n",
       "     'source_doc': '4.3.1 Alternative Retrieval-based Methods'},\n",
       "    {'text': 'Stylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (b) Inpainting',\n",
       "     'page': 7,\n",
       "     'source_doc': '4.3.1 Alternative Retrieval-based Methods'},\n",
       "    {'text': 'Figure 12.\\nStylus over different image-to-image tasks.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (b) Inpainting',\n",
       "     'page': 7,\n",
       "     'source_doc': '4.3.1 Alternative Retrieval-based Methods'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edae6d10>]},\n",
       "  {'text': 'Volcanoes spewing lava',\n",
       "   'title': 'Volcanoes spewing lava',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Studio Ghibli Style',\n",
       "   'title': 'Studio Ghibli Style',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Fiery red Voxel Style Pencil Sketch Van Gogh Style',\n",
       "   'title': 'Fiery red Voxel Style Pencil Sketch Van Gogh Style',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': '(a) Image Translation\\nStylus chooses relevant adapters that better adapt new styles and elements into existing images.\\nStylus Stylus\\nSD v1.5 SD v1.5\\nSource Source\\nDwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny',\n",
       "   'title': '(a) Image Translation',\n",
       "   'chunks': [{'text': 'Stylus chooses relevant adapters that better adapt new styles and elements into existing images.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (a) Image Translation',\n",
       "     'page': 7,\n",
       "     'source_doc': '(a) Image Translation'},\n",
       "    {'text': 'Dwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (a) Image Translation > Source Source',\n",
       "     'page': 7,\n",
       "     'source_doc': '(a) Image Translation'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Stylus Stylus',\n",
       "   'title': 'Stylus Stylus',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'SD v1.5 SD v1.5',\n",
       "   'title': 'SD v1.5 SD v1.5',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Source Source\\nDwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny',\n",
       "   'title': 'Source Source',\n",
       "   'chunks': [{'text': 'Dwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (a) Image Translation > Source Source',\n",
       "     'page': 7,\n",
       "     'source_doc': 'Source Source'}],\n",
       "   'tables': []},\n",
       "  {'text': '(b) Inpainting\\nStylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.\\nFigure 12.\\nStylus over different image-to-image tasks.',\n",
       "   'title': '(b) Inpainting',\n",
       "   'chunks': [{'text': 'Stylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (b) Inpainting',\n",
       "     'page': 7,\n",
       "     'source_doc': '(b) Inpainting'},\n",
       "    {'text': 'Figure 12.\\nStylus over different image-to-image tasks.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.1 Alternative Retrieval-based Methods > (b) Inpainting',\n",
       "     'page': 7,\n",
       "     'source_doc': '(b) Inpainting'}],\n",
       "   'tables': []},\n",
       "  {'text': '4.3.2 Breakdown of Stylus’s Inference Time\\nThis section breaks down the latency introduced by various components of Stylus.\\nWe note that image generation time is independent of Stylus, as adapter weights are merged into the base model [12].\\nFigure 11 demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts.\\nSpecifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds.\\nThe composer’s large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens.\\nFinally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process.\\nHowever, Stylus’s latency remains consistent across all batch sizes, as the composer and retriever run only once.\\nHence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases.',\n",
       "   'title': '4.3.2 Breakdown of Stylus’s Inference Time',\n",
       "   'chunks': [{'text': 'This section breaks down the latency introduced by various components of Stylus.\\nWe note that image generation time is independent of Stylus, as adapter weights are merged into the base model [12].',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.2 Breakdown of Stylus’s Inference Time',\n",
       "     'page': 7,\n",
       "     'source_doc': '4.3.2 Breakdown of Stylus’s Inference Time'},\n",
       "    {'text': 'Figure 11 demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts.\\nSpecifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds.\\nThe composer’s large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens.\\nFinally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process.\\nHowever, Stylus’s latency remains consistent across all batch sizes, as the composer and retriever run only once.\\nHence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.2 Breakdown of Stylus’s Inference Time',\n",
       "     'page': 7,\n",
       "     'source_doc': '4.3.2 Breakdown of Stylus’s Inference Time'}],\n",
       "   'tables': []},\n",
       "  {'text': '4.3.3 Image-Domain Tasks\\nBeyond text-to-image, Stylus applies across various imageto-image tasks.\\nFig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting, described as follows:\\nImage translation: Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt’s definition.\\nStylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style.\\nWe present examples in Fig 12a.\\nFor a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits.\\nFor a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.\\nInpainting: Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask.\\nStylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity.\\nWe provide further examples in Fig. 12b, demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right).',\n",
       "   'title': '4.3.3 Image-Domain Tasks',\n",
       "   'chunks': [{'text': 'Beyond text-to-image, Stylus applies across various imageto-image tasks.\\nFig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting, described as follows:',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.3 Image-Domain Tasks',\n",
       "     'page': 7,\n",
       "     'source_doc': '4.3.3 Image-Domain Tasks'},\n",
       "    {'text': 'Image translation: Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt’s definition.\\nStylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style.\\nWe present examples in Fig 12a.\\nFor a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits.\\nFor a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.3 Image-Domain Tasks',\n",
       "     'page': 7,\n",
       "     'source_doc': '4.3.3 Image-Domain Tasks'},\n",
       "    {'text': 'Inpainting: Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask.\\nStylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity.\\nWe provide further examples in Fig. 12b, demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right).',\n",
       "     'title': '2. Related Works > 4. Results > 4.3.3 Image-Domain Tasks',\n",
       "     'page': 7,\n",
       "     'source_doc': '4.3.3 Image-Domain Tasks'}],\n",
       "   'tables': []},\n",
       "  {'text': '5. Conclusion\\nWe propose Stylus, a new algorithm that automatically selects and composes adapters to generate better images.\\nOur method leverages a three-stage framework that precomputes adapters as lookup embeddings and retrieves most relevant adapters based on prompts’ keywords.\\nTo evaluate Stylus, we develop StylusDocs, a processed dataset featuring 75K adapters and pre-computed adapter embeddings.\\nOur evaluation of Stylus, across automatic metrics, humans, and vision-language models, demonstrate that Stylus achieves better visual fidelity, textual alignment, and image diversity than existing Stable Diffusion checkpoints.\\nAcknowledgement\\nWe thank Lisa Dunlap, Ansh Chaurasia, Siyuan Zhuang, Sijun Tan, Tianjun Zhang, and Shishir Patil for their insightful discussion.\\nWe thank Google Deepmind for funding this project, providing AI infrastructure, and provisioning Gemini endpoints.\\nSky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, SAP, Uber, and VMware.',\n",
       "   'title': '5. Conclusion',\n",
       "   'chunks': [{'text': 'We propose Stylus, a new algorithm that automatically selects and composes adapters to generate better images.\\nOur method leverages a three-stage framework that precomputes adapters as lookup embeddings and retrieves most relevant adapters based on prompts’ keywords.\\nTo evaluate Stylus, we develop StylusDocs, a processed dataset featuring 75K adapters and pre-computed adapter embeddings.\\nOur evaluation of Stylus, across automatic metrics, humans, and vision-language models, demonstrate that Stylus achieves better visual fidelity, textual alignment, and image diversity than existing Stable Diffusion checkpoints.',\n",
       "     'title': '2. Related Works > 5. Conclusion',\n",
       "     'page': 7,\n",
       "     'source_doc': '5. Conclusion'},\n",
       "    {'text': 'We thank Lisa Dunlap, Ansh Chaurasia, Siyuan Zhuang, Sijun Tan, Tianjun Zhang, and Shishir Patil for their insightful discussion.\\nWe thank Google Deepmind for funding this project, providing AI infrastructure, and provisioning Gemini endpoints.\\nSky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, SAP, Uber, and VMware.',\n",
       "     'title': '2. Related Works > 5. Conclusion > Acknowledgement',\n",
       "     'page': 8,\n",
       "     'source_doc': '5. Conclusion'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Acknowledgement\\nWe thank Lisa Dunlap, Ansh Chaurasia, Siyuan Zhuang, Sijun Tan, Tianjun Zhang, and Shishir Patil for their insightful discussion.\\nWe thank Google Deepmind for funding this project, providing AI infrastructure, and provisioning Gemini endpoints.\\nSky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, SAP, Uber, and VMware.',\n",
       "   'title': 'Acknowledgement',\n",
       "   'chunks': [{'text': 'We thank Lisa Dunlap, Ansh Chaurasia, Siyuan Zhuang, Sijun Tan, Tianjun Zhang, and Shishir Patil for their insightful discussion.\\nWe thank Google Deepmind for funding this project, providing AI infrastructure, and provisioning Gemini endpoints.\\nSky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, SAP, Uber, and VMware.',\n",
       "     'title': '2. Related Works > 5. Conclusion > Acknowledgement',\n",
       "     'page': 8,\n",
       "     'source_doc': 'Acknowledgement'}],\n",
       "   'tables': []},\n",
       "  {'text': 'References\\n[1] AUTOMATIC1111.\\nStable Diffusion Web UI, Aug. 2022.\\n3, 5 [2] Harrison Chase.\\nLangChain, Oct. 2022.\\n3 [3] Alexandra Chronopoulou, Matthew E. Peters, Alexander Fraser, and Jesse Dodge.\\nAdaptersoup: Weight averaging to improve generalization of pretrained language models, 2023.\\n3 [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\nBert: Pre-training of deep bidirectional transformers for language understanding, 2019.\\n3 [5] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto.\\nAlpacafarm: A simulation framework for methods that learn from human feedback.\\nIn\\nA. Oh, T\\nNeumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 30039–30069.\\nCurran Associates, Inc., 2023.\\n5 [6] Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, and Serena Yeung-Levy.\\nDescribing differences in image sets with natural language.\\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n6, 14 [7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or.\\nAn image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\\n1, 3 [8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang.\\nRetrieval-augmented generation for large language models: A survey, 2024.\\n2, 3 [9] David Ha, Andrew Dai, and Quoc V. Le.\\nHypernetworks, 2016.\\n1, 3 [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\\nClipscore: A reference-free evaluation metric for image captioning, 2022.\\n5 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\\nGans trained by a two time-scale update rule converge to a local nash equilibrium, 2018.\\n5, 6 [12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLora: Low-rank adaptation of large language models, 2021.\\n1, 3, 8 [13] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin.\\nLorahub: Efficient cross-task generalization via dynamic lora composition, 2024.\\n3, 5 [14] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.\\nPersonalized soups: Personalized large language model alignment via post-hoc parameter merging, 2023.\\n3 [15] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\\nElucidating the design space of diffusion-based generative models, 2022.\\n5 [16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\\nRetrieval-augmented generation for knowledge-intensive nlp tasks, 2021.\\n2, 3, 7 [17] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi.\\nPlayground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024.\\n3 [18] Jimmy Lin, Ronak Pradeep, Tommaso Teofili, and Jasper Xian.\\nVector search with openai embeddings: Lucene is all you need, 2023.\\n3, 5 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.\\nMicrosoft coco: Common objects in context, 2015.\\n1, 5, 11 [20] Jerry Liu.\\nLlamaIndex, 11 2022.\\n3 [21] Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Antonio Torralba.\\nUnsupervised compositional concepts discovery with text-to-image generative models, 2023.\\n3 [22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\\nDPM-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023.\\n5 [23] Tengyu Ma.\\nVectorize your data to gear up your ai stack., 2023.\\n3 [24] Justin Maier.\\nThe home of open-source generative ai, 2022.\\n1, 2, 3, 5, 11 [25] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.\\nPeft: State-of-the-art parameter-efficient fine-tuning methods.\\nhttps://github.com/huggingface/peft, 2022.\\n3 [26] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz.\\nSfr-embeddingmistral:enhance text retrieval with transfer learning.\\nSalesforce AI Research Blog, 2024.\\n3 [27] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo.\\nReliable fidelity and diversity metrics for generative models.\\nIn Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 7176–7185.\\nPMLR, 2020.\\n6 [28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\\nTraining language models to follow instructions with human feedback, 2022.\\n4\\nMalte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch, Massimiliano Pippi, Mayank Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir Blagojevic, Thomas Stadelmann, Tanay Soni, and Sebastian Lee.\\nHaystack: the end-to-end NLP framework for pragmatic builders, Nov. 2019.\\n3 [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.\\nSdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\\n3 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\\nLearning transferable visual models from natural language supervision, 2021.\\n3 [32] Nils Reimers.\\nSay goodbye to irrelevant search results: Cohere rerank is here, 2023.\\n3, 4, 7 [33] Nils Reimers and Iryna Gurevych.\\nSentence-bert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\\nAssociation for Computational Linguistics, 11 2019.\\n3 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\\nHigh-resolution image synthesis with latent diffusion models, 2022.\\n3, 5 [35] Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly.\\nAssessing generative models via precision and recall.\\nIn Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 5234–5243, 2018.\\n6 [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision.\\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2818–2826.\\nIEEE Computer Society, 2016.\\n6 [37] Gemini Team.\\nGemini: A family of highly capable multimodal models, 2023.\\n3, 5, 11 [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\\nLlama: Open and efficient foundation language models, 2023.\\n3 [39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\nLlama 2: Open foundation and fine-tuned chat models, 2023.\\n3 [40] Hanqing Wang, Bowen Ping, Shuo Wang, Xu Han, Yun Chen, Zhiyuan Liu, and Maosong Sun.\\nLora-flow: Dynamic lora fusion for large language models in generative tasks, 2024.\\n3 [41] Xinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Graham Neubig.\\nEfficient test time adapter ensembling for low-resource language varieties.\\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 730–737, Punta Cana, Dominican Republic, Nov. 2021.\\nAssociation for Computational Linguistics.\\n3 [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\n11, 14 [43] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\\nHuggingface’s transformers: State-of-the-art natural language processing, 2020.\\n1, 3, 5, 11 [44] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models, 2023.\\n11, 14 [45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu.\\nScaling autoregressive models for content-rich text-to-image generation, 2022.\\n5, 6 [46] Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, and Fei Wu.\\nLoraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild, 2024.\\n3, 5, 7 [47] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen.\\nMulti-lora composition for image generation, 2024.\\n3, 4, 6\\nA. Appendix\\n0.5\\nA.1. Details of the Refiner VLM\\n0.4\\nWe provide a complete example input to the refiner’s VLM in Tab.\\n2.\\nThe prompt utilizes Chain-of-Thought (CoT) prompting, which decomposes the VLM’s goal of producing better adapter descriptions into two steps [42, 44].\\nInitially, the VLM categorizes the adapter’s task into one of several topics—such as concepts, styles, characters, or poses.\\nSubsequently, the VLM is prompted to elaborate on why the adapter is associated with a particular topic and how it modifies images within that context.\\nWe found that this two step logical process significantly improved the structure and quality of model responses.\\n0.3\\n0.2\\n0.1\\n0.0\\nCategory\\n(a) Distribution of adapters across categories.\\n0.004\\nA.2. Details of the Composer LLM\\n0.003\\nWe provide a full example prompt of the composer’s LLM component in Tab.\\n3, which is plugged through the Gemini\\n0.002\\n1.5 endpoint [37].\\nOur experiments feed in descriptions of the top 150 adapters into the LLM’s context.\\nUsing a Chain-of-Thought (CoT) approach, the prompt is structured to first identify keywords or tasks, then allocate appropriate adapters to these tasks.\\nIf necessary, it merges keywords for adapters that span multiple tasks [42, 44].\\n0.001\\n0.000\\nTop 500 Adapters sorted by Download Count\\nA.3. Stylus-Bench Characterization\\nThis section describes StylusDocs, which comprises of 76K Low Rank Adapters (LoRAs) from public repositories, including Civit AI and Hugging Face [24, 43].\\nWe excluded NSFW-labeled adapters from the Civit AI dataset, which originally contained over 100K LoRAs.\\nFigure 13 illustrates the distribution of adapters across various semantic categories and their popularity, measured by download counts.\\nA significant majority, 70%, of adapters belong to the character and celebrity category, primarily consisting of anime or game characters.\\nAnother 13% of adapters modify image style, 8% adjust clothing, and 4% represent various concepts (Fig. 13a).\\nThese statistics indicate that our experiments consider a minor proportion of adapters, as the COCO dataset does not feature characters or celebrities [19].\\nDespite this, Stylus outperforms base Stable Diffusion.\\nFurthermore, the popularity of adapters follows a Pareto distribution, where the top adapters receive exponentially more downloads than the others (Fig. 13a).\\nHowever, the top adapter accounts for only 0.5% of total downloads, which suggests that the distribution is long-tailed.\\nA.4. Failure Modes\\nWe detail different failure modes that were discovered while developing Stylus.\\nImage saturation.\\nThe quality of image generation is highly depend on adapters’ weights.\\nIf the assigned weight is above the recommended value, the adapter negatively impacts image generation, leading to a growing number of visual inconsistencies and artifacts.\\nIn Fig. 14a, assigning a high weight to a “James Bond” LoRA increases images exposure and introducing significant visual tearing.\\nStylus mitigates over-saturation with its refiner component, which extract the right adapter weights from the adapter’s model card.\\nLastly, Stylus uniformly weights adapters based on their associated tasks, ensuring that similar adapters do not significantly impact their corresponding tasks.\\n0.005\\n(b) Top 500 adapters ranked by percentage of downloads.\\nFigure 13.\\nWorkload Characterization of StylusDocs.\\n(a) Most adapters are categorized as characters or celebrities.\\n(b) Adapter popularity exhibits a power-law distribution, with the top adapters receiving exponentially more downloads than the others.\\nTask Blocking.\\nComposing adapters presents the risk of overwriting existing concepts or tasks specified in the prompt and other selected adapters.\\nWe illustrate several examples in Figure 2—a train LoRA overrides the toy train concept (left), a park bench LoRA masks a person in an orange blanket (middle), and a fancy cake LoRA erases the image of a man eating the cake (right).\\nTask blocking typically arises from two main issues: the adapter weight set too high or too many adapters merged into the base model.\\nStylus addresses this by reducing an adapter’s weight with uniform weighting per task, while the masking scheme reduces the number of selected adapters.\\nAlthough Stylus does not completely solve task blocking, it offers simple heuristics to mitigate the issue.\\nImage Prompts\\nPrompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.\\nPrompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>\\nModel Card Description\\n• Title: Dwayne”The Rock” Johnson (LoRA)\\n• Tags: Celebrity, Photorealistic, Hollywood, Celeb\\n• Trigger Words: Th3R0ck\\n• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.\\nYour goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:\\n1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.\\n• Some prompts may specify the adapter weight (i.e. <lora:NAME:WEIGHT>).\\nIf provided, you will need to infer the adapter’s name and weight.\\nPrioritize this weight over the author’s recommended weight.\\n2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.\\n• The model card description may be incorrect, misleading, or incomplete.\\n• The model card may specify the weight of the model adapter, or the recommended range.\\nFind the recommended weight of the adapter (default is 0.8).\\n% Chain-of-Thought Prompting\\nAgain, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.\\nFirst, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:\\n• Do not describe any training or dataset-related details.\\n• Provide additional context from your prior knowledge if there is insufficient information.\\n• Do not hallucinate and repeat text.\\nOutput only english words and sentences.\\nSecond, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.\\nPlease format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.\\nTask Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.\\nLow quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.\\nRetrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth\\nProvided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.\\n% Chain-of-Thought Prompting\\nFirst, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.\\nHere are the requirements for tasks:\\n• Tasks should never introduce new information to the prompt.\\nThe topic must be selected from the prompt’s keywords.\\n• Different tasks must be orthogonal from each other.\\n• All tasks combined must span the entirety of the prompt.\\n• Prioritize choosing narrower tasks.\\nYou may merge tasks if a relevant adapter spans several tasks.\\nSecond, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.\\nHere are the requirements for adapters:\\n• Adapters should only be used at most once across all tasks.\\nIf an adapter is used in one task, it should not be used in another task.\\n• Adapters should not introduce novel concepts or biases to the topic or the prompt.\\nDo not include such adapters.\\n• Adapters cannot encompass a broader scope relative to its assigned task.\\nFor example, if the task is about a “dog”, the adapter cannot be about general “animals”.\\n• Adapters cannot be too narrow in scope relative to its assigned task.\\nFor example, if a task is about pandas, do not choose highly specific pandas such as the character “Po” from Kung Fu Panda.\\nHowever, it is acceptable to choose adapters that modify the style of the task, such as “Red Pandas”.\\n• If an adapter spans multiple tasks, merge these tasks together.\\nFor example, if there is an adapter that is about “fluffy cats”, merge the topics “fluffy” and “cats” together.\\n• Avoid choosing NSFW and anthropormorphic adapters.\\nFinally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.\\nGive me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.\\ncomposer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.\\nRetrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the\\nw=1.0 w=1.5 w=2.0\\n(a) Image Saturation\\nAssigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.\\nStylus\\nSD\\nv1.5\\n(b) Task Blocking\\nAdapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).\\nStylus\\nSD\\nv1.5\\n(c) Task Diversity\\nAdding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).\\nStylus\\nSD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.\\nA.5. VLM as a Judge\\nThe full prompts to GPT-4V as a judge for textual alignment, visual fidelity, and image diversity are specified in Tables 4 and 5.\\nSystem Prompt:\\nYou are a photoshop expert judging which image has better composition quality.\\n | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n\\nUser:\\nThis is IMAGE A. Reply ’ACK’.\\n% Generated Image from Group A\\nAssistant: ACK User:\\nThis is IMAGE B. Reply ’ACK’.\\n% Generated Images from Group B\\nAssistant: ACK User:\\nRate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.\\nImage A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• You must pick a group for ’Better Quality’ / ’Better Alignment’, neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.\\nputting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.\\nVisual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.\\nSystem Prompt:\\nYou are a photoshop expert judging which set of images is more diverse.\\nScoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).\\nDiversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.\\n• Theme Interpretation: The theme can vary based on interpretation.\\nThe theme “it’s raining cats and dogs” can have a literal interpretation as cats and dogs falling from the sky or a figurative interpretation as heavy rain.\\nThe images are diverse, since they show both weather and animals.\\nIf the group only contains images of heavy rain or animals, a diversity score of 1 should be given.\\n• Main Subject: The main subject changes based on the focus across different subjects.\\nA set of images that contains a mix of images of apples and children dressed as different kinds of apples is more diverse than a set with only children dressed as apples.\\nNote the more diverse set has children as the subject for some images and apples as the subject for other images.\\nUser:\\nThis is GROUP A. Reply ’ACK’.\\n% Set of 5 Generated Images from Group A\\nAssistant: ACK User:\\nThis is GROUP B. Reply ’ACK’.\\n% Set of 5 Generated Images from Group B\\nAssistant: ACK User:\\nRate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.\\nGroup A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• Don’t forget to reward different main subjects in the diversity score.\\n• You must pick a group for ’More Diversity,’ neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 5.\\nFull prompt judging diversity using VLM.\\nwith a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.\\nDiversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.\\nA.6. Additional Diversity Scores\\nFig. 15 decomposes dFID scores over the top 100 keywords in the PartiPrompts dataset.\\nWe highlight that the largest differences stem from concepts, appearances, attributes, or styles.\\nFor example, Stylus excels over concepts ranging from animals (“bears”, “sloth”, and ‘squirrel‘) to objects (“microphone”, “box”, and “jacket”). Selected attributes can include but are not limited to: (“white”, “blue”, and “photographic”). Regardless of keyword, Stylus attains higher diversity scores across the board.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5 Parti Prompts Diversity\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nSD 1.5 Stylus\\nFigure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       "   'title': 'References',\n",
       "   'chunks': [{'text': '[1] AUTOMATIC1111.\\nStable Diffusion Web UI, Aug. 2022.\\n3, 5 [2] Harrison Chase.\\nLangChain, Oct. 2022.\\n3 [3] Alexandra Chronopoulou, Matthew E. Peters, Alexander Fraser, and Jesse Dodge.\\nAdaptersoup: Weight averaging to improve generalization of pretrained language models, 2023.\\n3 [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\nBert: Pre-training of deep bidirectional transformers for language understanding, 2019.\\n3 [5] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto.\\nAlpacafarm: A simulation framework for methods that learn from human feedback.\\nIn',\n",
       "     'title': 'References',\n",
       "     'page': 8,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 30039–30069.\\nCurran Associates, Inc., 2023.\\n5 [6] Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, and Serena Yeung-Levy.\\nDescribing differences in image sets with natural language.\\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n6, 14 [7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or.\\nAn image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\\n1, 3 [8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang.\\nRetrieval-augmented generation for large language models: A survey, 2024.\\n2, 3 [9] David Ha, Andrew Dai, and Quoc V. Le.\\nHypernetworks, 2016.\\n1, 3 [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\\nClipscore: A reference-free evaluation metric for image captioning, 2022.\\n5 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\\nGans trained by a two time-scale update rule converge to a local nash equilibrium, 2018.\\n5, 6 [12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLora: Low-rank adaptation of large language models, 2021.\\n1, 3, 8 [13] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin.\\nLorahub: Efficient cross-task generalization via dynamic lora composition, 2024.\\n3, 5 [14] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.\\nPersonalized soups: Personalized large language model alignment via post-hoc parameter merging, 2023.\\n3 [15] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\\nElucidating the design space of diffusion-based generative models, 2022.\\n5 [16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\\nRetrieval-augmented generation for knowledge-intensive nlp tasks, 2021.\\n2, 3, 7 [17] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi.\\nPlayground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024.\\n3 [18] Jimmy Lin, Ronak Pradeep, Tommaso Teofili, and Jasper Xian.\\nVector search with openai embeddings: Lucene is all you need, 2023.\\n3, 5 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.\\nMicrosoft coco: Common objects in context, 2015.\\n1, 5, 11 [20] Jerry Liu.\\nLlamaIndex, 11 2022.\\n3 [21] Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Antonio Torralba.\\nUnsupervised compositional concepts discovery with text-to-image generative models, 2023.\\n3 [22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\\nDPM-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023.\\n5 [23] Tengyu Ma.\\nVectorize your data to gear up your ai stack., 2023.\\n3 [24] Justin Maier.\\nThe home of open-source generative ai, 2022.\\n1, 2, 3, 5, 11 [25] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.\\nPeft: State-of-the-art parameter-efficient fine-tuning methods.',\n",
       "     'title': 'References > A. Oh, T',\n",
       "     'page': 8,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'https://github.com/huggingface/peft, 2022.\\n3 [26] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz.\\nSfr-embeddingmistral:enhance text retrieval with transfer learning.\\nSalesforce AI Research Blog, 2024.\\n3 [27] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo.\\nReliable fidelity and diversity metrics for generative models.\\nIn Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 7176–7185.\\nPMLR, 2020.\\n6 [28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\\nTraining language models to follow instructions with human feedback, 2022.\\n4',\n",
       "     'title': 'References > A. Oh, T',\n",
       "     'page': 8,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Malte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch, Massimiliano Pippi, Mayank Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir Blagojevic, Thomas Stadelmann, Tanay Soni, and Sebastian Lee.\\nHaystack: the end-to-end NLP framework for pragmatic builders, Nov. 2019.\\n3 [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.\\nSdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\\n3 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\\nLearning transferable visual models from natural language supervision, 2021.\\n3 [32] Nils Reimers.\\nSay goodbye to irrelevant search results: Cohere rerank is here, 2023.\\n3, 4, 7 [33] Nils Reimers and Iryna Gurevych.\\nSentence-bert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\\nAssociation for Computational Linguistics, 11 2019.\\n3 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\\nHigh-resolution image synthesis with latent diffusion models, 2022.\\n3, 5 [35] Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly.\\nAssessing generative models via precision and recall.\\nIn Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 5234–5243, 2018.\\n6 [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision.\\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2818–2826.\\nIEEE Computer Society, 2016.\\n6 [37] Gemini Team.\\nGemini: A family of highly capable multimodal models, 2023.\\n3, 5, 11 [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\\nLlama: Open and efficient foundation language models, 2023.\\n3 [39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\nLlama 2: Open foundation and fine-tuned chat models, 2023.\\n3 [40] Hanqing Wang, Bowen Ping, Shuo Wang, Xu Han, Yun Chen, Zhiyuan Liu, and Maosong Sun.\\nLora-flow: Dynamic lora fusion for large language models in generative tasks, 2024.\\n3 [41] Xinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Graham Neubig.\\nEfficient test time adapter ensembling for low-resource language varieties.\\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 730–737, Punta Cana, Dominican Republic, Nov. 2021.\\nAssociation for Computational Linguistics.\\n3 [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\n11, 14 [43] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\\nHuggingface’s transformers: State-of-the-art natural language processing, 2020.\\n1, 3, 5, 11 [44] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models, 2023.\\n11, 14 [45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu.\\nScaling autoregressive models for content-rich text-to-image generation, 2022.\\n5, 6 [46] Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, and Fei Wu.\\nLoraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild, 2024.\\n3, 5, 7 [47] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen.\\nMulti-lora composition for image generation, 2024.\\n3, 4, 6',\n",
       "     'title': 'References > A. Oh, T',\n",
       "     'page': 9,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.4',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'We provide a complete example input to the refiner’s VLM in Tab.\\n2.\\nThe prompt utilizes Chain-of-Thought (CoT) prompting, which decomposes the VLM’s goal of producing better adapter descriptions into two steps [42, 44].\\nInitially, the VLM categorizes the adapter’s task into one of several topics—such as concepts, styles, characters, or poses.\\nSubsequently, the VLM is prompted to elaborate on why the adapter is associated with a particular topic and how it modifies images within that context.\\nWe found that this two step logical process significantly improved the structure and quality of model responses.',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.3',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.2',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.1',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '(a) Distribution of adapters across categories.',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM > Category',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.003',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'We provide a full example prompt of the composer’s LLM component in Tab.\\n3, which is plugged through the Gemini',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.002',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Top 500 Adapters sorted by Download Count',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'This section describes StylusDocs, which comprises of 76K Low Rank Adapters (LoRAs) from public repositories, including Civit AI and Hugging Face [24, 43].\\nWe excluded NSFW-labeled adapters from the Civit AI dataset, which originally contained over 100K LoRAs.\\nFigure 13 illustrates the distribution of adapters across various semantic categories and their popularity, measured by download counts.\\nA significant majority, 70%, of adapters belong to the character and celebrity category, primarily consisting of anime or game characters.\\nAnother 13% of adapters modify image style, 8% adjust clothing, and 4% represent various concepts (Fig. 13a).\\nThese statistics indicate that our experiments consider a minor proportion of adapters, as the COCO dataset does not feature characters or celebrities [19].\\nDespite this, Stylus outperforms base Stable Diffusion.\\nFurthermore, the popularity of adapters follows a Pareto distribution, where the top adapters receive exponentially more downloads than the others (Fig. 13a).\\nHowever, the top adapter accounts for only 0.5% of total downloads, which suggests that the distribution is long-tailed.',\n",
       "     'title': 'References > A. Appendix > A.3. Stylus-Bench Characterization',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'We detail different failure modes that were discovered while developing Stylus.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Image saturation.\\nThe quality of image generation is highly depend on adapters’ weights.\\nIf the assigned weight is above the recommended value, the adapter negatively impacts image generation, leading to a growing number of visual inconsistencies and artifacts.\\nIn Fig. 14a, assigning a high weight to a “James Bond” LoRA increases images exposure and introducing significant visual tearing.\\nStylus mitigates over-saturation with its refiner component, which extract the right adapter weights from the adapter’s model card.\\nLastly, Stylus uniformly weights adapters based on their associated tasks, ensuring that similar adapters do not significantly impact their corresponding tasks.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Figure 13.\\nWorkload Characterization of StylusDocs.\\n(a) Most adapters are categorized as characters or celebrities.\\n(b) Adapter popularity exhibits a power-law distribution, with the top adapters receiving exponentially more downloads than the others.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Task Blocking.\\nComposing adapters presents the risk of overwriting existing concepts or tasks specified in the prompt and other selected adapters.\\nWe illustrate several examples in Figure 2—a train LoRA overrides the toy train concept (left), a park bench LoRA masks a person in an orange blanket (middle), and a fancy cake LoRA erases the image of a man eating the cake (right).\\nTask blocking typically arises from two main issues: the adapter weight set too high or too many adapters merged into the base model.\\nStylus addresses this by reducing an adapter’s weight with uniform weighting per task, while the masking scheme reduces the number of selected adapters.\\nAlthough Stylus does not completely solve task blocking, it offers simple heuristics to mitigate the issue.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Prompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Image Prompts',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Prompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Image Prompts',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '• Title: Dwayne”The Rock” Johnson (LoRA)',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '• Tags: Celebrity, Photorealistic, Hollywood, Celeb',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '• Trigger Words: Th3R0ck',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Your goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '% Chain-of-Thought Prompting',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Again, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'First, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Second, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Please format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Task Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Low quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Retrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Provided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '% Chain-of-Thought Prompting',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'First, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Here are the requirements for tasks:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Second, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Here are the requirements for adapters:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Finally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Give me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'composer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Retrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'w=1.0 w=1.5 w=2.0',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Assigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (a) Image Saturation',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (a) Image Saturation > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Adapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (b) Task Blocking',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (b) Task Blocking > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Adding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Figure 14.\\nCategorization of Different Failure Modes.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'To distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'The full prompts to GPT-4V as a judge for textual alignment, visual fidelity, and image diversity are specified in Tables 4 and 5.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge',\n",
       "     'page': 13,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'You are a photoshop expert judging which image has better composition quality.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': ' | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'This is IMAGE A. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '% Generated Image from Group A',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'This is IMAGE B. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '% Generated Images from Group B',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Rate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '% Prevent VLM from returning neutral results.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'I’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Table 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'putting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Visual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'You are a photoshop expert judging which set of images is more diverse.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Scoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Diversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'This is GROUP A. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '% Set of 5 Generated Images from Group A',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'This is GROUP B. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '% Set of 5 Generated Images from Group B',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Rate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '% Prevent VLM from returning neutral results.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'I’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Table 5.\\nFull prompt judging diversity using VLM.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'with a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Diversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Fig. 15 decomposes dFID scores over the top 100 keywords in the PartiPrompts dataset.\\nWe highlight that the largest differences stem from concepts, appearances, attributes, or styles.\\nFor example, Stylus excels over concepts ranging from animals (“bears”, “sloth”, and ‘squirrel‘) to objects (“microphone”, “box”, and “jacket”). Selected attributes can include but are not limited to: (“white”, “blue”, and “photographic”). Regardless of keyword, Stylus attains higher diversity scores across the board.',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 15,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'},\n",
       "    {'text': 'Figure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity > SD 1.5 Stylus',\n",
       "     'page': 16,\n",
       "     'source_doc': 'References'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edb106d0>]},\n",
       "  {'text': 'A. Oh, T\\nNeumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 30039–30069.\\nCurran Associates, Inc., 2023.\\n5 [6] Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, and Serena Yeung-Levy.\\nDescribing differences in image sets with natural language.\\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n6, 14 [7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or.\\nAn image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\\n1, 3 [8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang.\\nRetrieval-augmented generation for large language models: A survey, 2024.\\n2, 3 [9] David Ha, Andrew Dai, and Quoc V. Le.\\nHypernetworks, 2016.\\n1, 3 [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\\nClipscore: A reference-free evaluation metric for image captioning, 2022.\\n5 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\\nGans trained by a two time-scale update rule converge to a local nash equilibrium, 2018.\\n5, 6 [12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLora: Low-rank adaptation of large language models, 2021.\\n1, 3, 8 [13] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin.\\nLorahub: Efficient cross-task generalization via dynamic lora composition, 2024.\\n3, 5 [14] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.\\nPersonalized soups: Personalized large language model alignment via post-hoc parameter merging, 2023.\\n3 [15] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\\nElucidating the design space of diffusion-based generative models, 2022.\\n5 [16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\\nRetrieval-augmented generation for knowledge-intensive nlp tasks, 2021.\\n2, 3, 7 [17] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi.\\nPlayground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024.\\n3 [18] Jimmy Lin, Ronak Pradeep, Tommaso Teofili, and Jasper Xian.\\nVector search with openai embeddings: Lucene is all you need, 2023.\\n3, 5 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.\\nMicrosoft coco: Common objects in context, 2015.\\n1, 5, 11 [20] Jerry Liu.\\nLlamaIndex, 11 2022.\\n3 [21] Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Antonio Torralba.\\nUnsupervised compositional concepts discovery with text-to-image generative models, 2023.\\n3 [22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\\nDPM-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023.\\n5 [23] Tengyu Ma.\\nVectorize your data to gear up your ai stack., 2023.\\n3 [24] Justin Maier.\\nThe home of open-source generative ai, 2022.\\n1, 2, 3, 5, 11 [25] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.\\nPeft: State-of-the-art parameter-efficient fine-tuning methods.\\nhttps://github.com/huggingface/peft, 2022.\\n3 [26] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz.\\nSfr-embeddingmistral:enhance text retrieval with transfer learning.\\nSalesforce AI Research Blog, 2024.\\n3 [27] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo.\\nReliable fidelity and diversity metrics for generative models.\\nIn Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 7176–7185.\\nPMLR, 2020.\\n6 [28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\\nTraining language models to follow instructions with human feedback, 2022.\\n4\\nMalte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch, Massimiliano Pippi, Mayank Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir Blagojevic, Thomas Stadelmann, Tanay Soni, and Sebastian Lee.\\nHaystack: the end-to-end NLP framework for pragmatic builders, Nov. 2019.\\n3 [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.\\nSdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\\n3 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\\nLearning transferable visual models from natural language supervision, 2021.\\n3 [32] Nils Reimers.\\nSay goodbye to irrelevant search results: Cohere rerank is here, 2023.\\n3, 4, 7 [33] Nils Reimers and Iryna Gurevych.\\nSentence-bert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\\nAssociation for Computational Linguistics, 11 2019.\\n3 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\\nHigh-resolution image synthesis with latent diffusion models, 2022.\\n3, 5 [35] Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly.\\nAssessing generative models via precision and recall.\\nIn Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 5234–5243, 2018.\\n6 [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision.\\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2818–2826.\\nIEEE Computer Society, 2016.\\n6 [37] Gemini Team.\\nGemini: A family of highly capable multimodal models, 2023.\\n3, 5, 11 [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\\nLlama: Open and efficient foundation language models, 2023.\\n3 [39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\nLlama 2: Open foundation and fine-tuned chat models, 2023.\\n3 [40] Hanqing Wang, Bowen Ping, Shuo Wang, Xu Han, Yun Chen, Zhiyuan Liu, and Maosong Sun.\\nLora-flow: Dynamic lora fusion for large language models in generative tasks, 2024.\\n3 [41] Xinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Graham Neubig.\\nEfficient test time adapter ensembling for low-resource language varieties.\\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 730–737, Punta Cana, Dominican Republic, Nov. 2021.\\nAssociation for Computational Linguistics.\\n3 [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\n11, 14 [43] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\\nHuggingface’s transformers: State-of-the-art natural language processing, 2020.\\n1, 3, 5, 11 [44] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models, 2023.\\n11, 14 [45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu.\\nScaling autoregressive models for content-rich text-to-image generation, 2022.\\n5, 6 [46] Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, and Fei Wu.\\nLoraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild, 2024.\\n3, 5, 7 [47] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen.\\nMulti-lora composition for image generation, 2024.\\n3, 4, 6',\n",
       "   'title': 'A. Oh, T',\n",
       "   'chunks': [{'text': 'Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 30039–30069.\\nCurran Associates, Inc., 2023.\\n5 [6] Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, and Serena Yeung-Levy.\\nDescribing differences in image sets with natural language.\\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n6, 14 [7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or.\\nAn image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\\n1, 3 [8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang.\\nRetrieval-augmented generation for large language models: A survey, 2024.\\n2, 3 [9] David Ha, Andrew Dai, and Quoc V. Le.\\nHypernetworks, 2016.\\n1, 3 [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\\nClipscore: A reference-free evaluation metric for image captioning, 2022.\\n5 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\\nGans trained by a two time-scale update rule converge to a local nash equilibrium, 2018.\\n5, 6 [12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLora: Low-rank adaptation of large language models, 2021.\\n1, 3, 8 [13] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin.\\nLorahub: Efficient cross-task generalization via dynamic lora composition, 2024.\\n3, 5 [14] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.\\nPersonalized soups: Personalized large language model alignment via post-hoc parameter merging, 2023.\\n3 [15] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\\nElucidating the design space of diffusion-based generative models, 2022.\\n5 [16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\\nRetrieval-augmented generation for knowledge-intensive nlp tasks, 2021.\\n2, 3, 7 [17] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi.\\nPlayground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024.\\n3 [18] Jimmy Lin, Ronak Pradeep, Tommaso Teofili, and Jasper Xian.\\nVector search with openai embeddings: Lucene is all you need, 2023.\\n3, 5 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.\\nMicrosoft coco: Common objects in context, 2015.\\n1, 5, 11 [20] Jerry Liu.\\nLlamaIndex, 11 2022.\\n3 [21] Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Antonio Torralba.\\nUnsupervised compositional concepts discovery with text-to-image generative models, 2023.\\n3 [22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\\nDPM-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023.\\n5 [23] Tengyu Ma.\\nVectorize your data to gear up your ai stack., 2023.\\n3 [24] Justin Maier.\\nThe home of open-source generative ai, 2022.\\n1, 2, 3, 5, 11 [25] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.\\nPeft: State-of-the-art parameter-efficient fine-tuning methods.',\n",
       "     'title': 'References > A. Oh, T',\n",
       "     'page': 8,\n",
       "     'source_doc': 'A. Oh, T'},\n",
       "    {'text': 'https://github.com/huggingface/peft, 2022.\\n3 [26] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz.\\nSfr-embeddingmistral:enhance text retrieval with transfer learning.\\nSalesforce AI Research Blog, 2024.\\n3 [27] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo.\\nReliable fidelity and diversity metrics for generative models.\\nIn Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 7176–7185.\\nPMLR, 2020.\\n6 [28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\\nTraining language models to follow instructions with human feedback, 2022.\\n4',\n",
       "     'title': 'References > A. Oh, T',\n",
       "     'page': 8,\n",
       "     'source_doc': 'A. Oh, T'},\n",
       "    {'text': 'Malte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch, Massimiliano Pippi, Mayank Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir Blagojevic, Thomas Stadelmann, Tanay Soni, and Sebastian Lee.\\nHaystack: the end-to-end NLP framework for pragmatic builders, Nov. 2019.\\n3 [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.\\nSdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\\n3 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\\nLearning transferable visual models from natural language supervision, 2021.\\n3 [32] Nils Reimers.\\nSay goodbye to irrelevant search results: Cohere rerank is here, 2023.\\n3, 4, 7 [33] Nils Reimers and Iryna Gurevych.\\nSentence-bert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\\nAssociation for Computational Linguistics, 11 2019.\\n3 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\\nHigh-resolution image synthesis with latent diffusion models, 2022.\\n3, 5 [35] Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly.\\nAssessing generative models via precision and recall.\\nIn Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 5234–5243, 2018.\\n6 [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision.\\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2818–2826.\\nIEEE Computer Society, 2016.\\n6 [37] Gemini Team.\\nGemini: A family of highly capable multimodal models, 2023.\\n3, 5, 11 [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\\nLlama: Open and efficient foundation language models, 2023.\\n3 [39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\nLlama 2: Open foundation and fine-tuned chat models, 2023.\\n3 [40] Hanqing Wang, Bowen Ping, Shuo Wang, Xu Han, Yun Chen, Zhiyuan Liu, and Maosong Sun.\\nLora-flow: Dynamic lora fusion for large language models in generative tasks, 2024.\\n3 [41] Xinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Graham Neubig.\\nEfficient test time adapter ensembling for low-resource language varieties.\\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 730–737, Punta Cana, Dominican Republic, Nov. 2021.\\nAssociation for Computational Linguistics.\\n3 [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\n11, 14 [43] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\\nHuggingface’s transformers: State-of-the-art natural language processing, 2020.\\n1, 3, 5, 11 [44] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models, 2023.\\n11, 14 [45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu.\\nScaling autoregressive models for content-rich text-to-image generation, 2022.\\n5, 6 [46] Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, and Fei Wu.\\nLoraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild, 2024.\\n3, 5, 7 [47] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen.\\nMulti-lora composition for image generation, 2024.\\n3, 4, 6',\n",
       "     'title': 'References > A. Oh, T',\n",
       "     'page': 9,\n",
       "     'source_doc': 'A. Oh, T'}],\n",
       "   'tables': []},\n",
       "  {'text': 'A. Appendix\\n0.5\\nA.1. Details of the Refiner VLM\\n0.4\\nWe provide a complete example input to the refiner’s VLM in Tab.\\n2.\\nThe prompt utilizes Chain-of-Thought (CoT) prompting, which decomposes the VLM’s goal of producing better adapter descriptions into two steps [42, 44].\\nInitially, the VLM categorizes the adapter’s task into one of several topics—such as concepts, styles, characters, or poses.\\nSubsequently, the VLM is prompted to elaborate on why the adapter is associated with a particular topic and how it modifies images within that context.\\nWe found that this two step logical process significantly improved the structure and quality of model responses.\\n0.3\\n0.2\\n0.1\\n0.0\\nCategory\\n(a) Distribution of adapters across categories.\\n0.004\\nA.2. Details of the Composer LLM\\n0.003\\nWe provide a full example prompt of the composer’s LLM component in Tab.\\n3, which is plugged through the Gemini\\n0.002\\n1.5 endpoint [37].\\nOur experiments feed in descriptions of the top 150 adapters into the LLM’s context.\\nUsing a Chain-of-Thought (CoT) approach, the prompt is structured to first identify keywords or tasks, then allocate appropriate adapters to these tasks.\\nIf necessary, it merges keywords for adapters that span multiple tasks [42, 44].\\n0.001\\n0.000\\nTop 500 Adapters sorted by Download Count\\nA.3. Stylus-Bench Characterization\\nThis section describes StylusDocs, which comprises of 76K Low Rank Adapters (LoRAs) from public repositories, including Civit AI and Hugging Face [24, 43].\\nWe excluded NSFW-labeled adapters from the Civit AI dataset, which originally contained over 100K LoRAs.\\nFigure 13 illustrates the distribution of adapters across various semantic categories and their popularity, measured by download counts.\\nA significant majority, 70%, of adapters belong to the character and celebrity category, primarily consisting of anime or game characters.\\nAnother 13% of adapters modify image style, 8% adjust clothing, and 4% represent various concepts (Fig. 13a).\\nThese statistics indicate that our experiments consider a minor proportion of adapters, as the COCO dataset does not feature characters or celebrities [19].\\nDespite this, Stylus outperforms base Stable Diffusion.\\nFurthermore, the popularity of adapters follows a Pareto distribution, where the top adapters receive exponentially more downloads than the others (Fig. 13a).\\nHowever, the top adapter accounts for only 0.5% of total downloads, which suggests that the distribution is long-tailed.\\nA.4. Failure Modes\\nWe detail different failure modes that were discovered while developing Stylus.\\nImage saturation.\\nThe quality of image generation is highly depend on adapters’ weights.\\nIf the assigned weight is above the recommended value, the adapter negatively impacts image generation, leading to a growing number of visual inconsistencies and artifacts.\\nIn Fig. 14a, assigning a high weight to a “James Bond” LoRA increases images exposure and introducing significant visual tearing.\\nStylus mitigates over-saturation with its refiner component, which extract the right adapter weights from the adapter’s model card.\\nLastly, Stylus uniformly weights adapters based on their associated tasks, ensuring that similar adapters do not significantly impact their corresponding tasks.\\n0.005\\n(b) Top 500 adapters ranked by percentage of downloads.\\nFigure 13.\\nWorkload Characterization of StylusDocs.\\n(a) Most adapters are categorized as characters or celebrities.\\n(b) Adapter popularity exhibits a power-law distribution, with the top adapters receiving exponentially more downloads than the others.\\nTask Blocking.\\nComposing adapters presents the risk of overwriting existing concepts or tasks specified in the prompt and other selected adapters.\\nWe illustrate several examples in Figure 2—a train LoRA overrides the toy train concept (left), a park bench LoRA masks a person in an orange blanket (middle), and a fancy cake LoRA erases the image of a man eating the cake (right).\\nTask blocking typically arises from two main issues: the adapter weight set too high or too many adapters merged into the base model.\\nStylus addresses this by reducing an adapter’s weight with uniform weighting per task, while the masking scheme reduces the number of selected adapters.\\nAlthough Stylus does not completely solve task blocking, it offers simple heuristics to mitigate the issue.\\nImage Prompts\\nPrompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.\\nPrompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>\\nModel Card Description\\n• Title: Dwayne”The Rock” Johnson (LoRA)\\n• Tags: Celebrity, Photorealistic, Hollywood, Celeb\\n• Trigger Words: Th3R0ck\\n• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.\\nYour goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:\\n1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.\\n• Some prompts may specify the adapter weight (i.e. <lora:NAME:WEIGHT>).\\nIf provided, you will need to infer the adapter’s name and weight.\\nPrioritize this weight over the author’s recommended weight.\\n2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.\\n• The model card description may be incorrect, misleading, or incomplete.\\n• The model card may specify the weight of the model adapter, or the recommended range.\\nFind the recommended weight of the adapter (default is 0.8).\\n% Chain-of-Thought Prompting\\nAgain, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.\\nFirst, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:\\n• Do not describe any training or dataset-related details.\\n• Provide additional context from your prior knowledge if there is insufficient information.\\n• Do not hallucinate and repeat text.\\nOutput only english words and sentences.\\nSecond, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.\\nPlease format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.\\nTask Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.\\nLow quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.\\nRetrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth\\nProvided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.\\n% Chain-of-Thought Prompting\\nFirst, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.\\nHere are the requirements for tasks:\\n• Tasks should never introduce new information to the prompt.\\nThe topic must be selected from the prompt’s keywords.\\n• Different tasks must be orthogonal from each other.\\n• All tasks combined must span the entirety of the prompt.\\n• Prioritize choosing narrower tasks.\\nYou may merge tasks if a relevant adapter spans several tasks.\\nSecond, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.\\nHere are the requirements for adapters:\\n• Adapters should only be used at most once across all tasks.\\nIf an adapter is used in one task, it should not be used in another task.\\n• Adapters should not introduce novel concepts or biases to the topic or the prompt.\\nDo not include such adapters.\\n• Adapters cannot encompass a broader scope relative to its assigned task.\\nFor example, if the task is about a “dog”, the adapter cannot be about general “animals”.\\n• Adapters cannot be too narrow in scope relative to its assigned task.\\nFor example, if a task is about pandas, do not choose highly specific pandas such as the character “Po” from Kung Fu Panda.\\nHowever, it is acceptable to choose adapters that modify the style of the task, such as “Red Pandas”.\\n• If an adapter spans multiple tasks, merge these tasks together.\\nFor example, if there is an adapter that is about “fluffy cats”, merge the topics “fluffy” and “cats” together.\\n• Avoid choosing NSFW and anthropormorphic adapters.\\nFinally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.\\nGive me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.\\ncomposer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.\\nRetrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the\\nw=1.0 w=1.5 w=2.0\\n(a) Image Saturation\\nAssigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.\\nStylus\\nSD\\nv1.5\\n(b) Task Blocking\\nAdapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).\\nStylus\\nSD\\nv1.5\\n(c) Task Diversity\\nAdding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).\\nStylus\\nSD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.\\nA.5. VLM as a Judge\\nThe full prompts to GPT-4V as a judge for textual alignment, visual fidelity, and image diversity are specified in Tables 4 and 5.\\nSystem Prompt:\\nYou are a photoshop expert judging which image has better composition quality.\\n | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n\\nUser:\\nThis is IMAGE A. Reply ’ACK’.\\n% Generated Image from Group A\\nAssistant: ACK User:\\nThis is IMAGE B. Reply ’ACK’.\\n% Generated Images from Group B\\nAssistant: ACK User:\\nRate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.\\nImage A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• You must pick a group for ’Better Quality’ / ’Better Alignment’, neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.\\nputting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.\\nVisual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.\\nSystem Prompt:\\nYou are a photoshop expert judging which set of images is more diverse.\\nScoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).\\nDiversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.\\n• Theme Interpretation: The theme can vary based on interpretation.\\nThe theme “it’s raining cats and dogs” can have a literal interpretation as cats and dogs falling from the sky or a figurative interpretation as heavy rain.\\nThe images are diverse, since they show both weather and animals.\\nIf the group only contains images of heavy rain or animals, a diversity score of 1 should be given.\\n• Main Subject: The main subject changes based on the focus across different subjects.\\nA set of images that contains a mix of images of apples and children dressed as different kinds of apples is more diverse than a set with only children dressed as apples.\\nNote the more diverse set has children as the subject for some images and apples as the subject for other images.\\nUser:\\nThis is GROUP A. Reply ’ACK’.\\n% Set of 5 Generated Images from Group A\\nAssistant: ACK User:\\nThis is GROUP B. Reply ’ACK’.\\n% Set of 5 Generated Images from Group B\\nAssistant: ACK User:\\nRate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.\\nGroup A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• Don’t forget to reward different main subjects in the diversity score.\\n• You must pick a group for ’More Diversity,’ neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 5.\\nFull prompt judging diversity using VLM.\\nwith a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.\\nDiversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.\\nA.6. Additional Diversity Scores\\nFig. 15 decomposes dFID scores over the top 100 keywords in the PartiPrompts dataset.\\nWe highlight that the largest differences stem from concepts, appearances, attributes, or styles.\\nFor example, Stylus excels over concepts ranging from animals (“bears”, “sloth”, and ‘squirrel‘) to objects (“microphone”, “box”, and “jacket”). Selected attributes can include but are not limited to: (“white”, “blue”, and “photographic”). Regardless of keyword, Stylus attains higher diversity scores across the board.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5 Parti Prompts Diversity\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nSD 1.5 Stylus\\nFigure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       "   'title': 'A. Appendix',\n",
       "   'chunks': [{'text': '0.5',\n",
       "     'title': 'References > A. Appendix',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.4',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'We provide a complete example input to the refiner’s VLM in Tab.\\n2.\\nThe prompt utilizes Chain-of-Thought (CoT) prompting, which decomposes the VLM’s goal of producing better adapter descriptions into two steps [42, 44].\\nInitially, the VLM categorizes the adapter’s task into one of several topics—such as concepts, styles, characters, or poses.\\nSubsequently, the VLM is prompted to elaborate on why the adapter is associated with a particular topic and how it modifies images within that context.\\nWe found that this two step logical process significantly improved the structure and quality of model responses.',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.3',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.2',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.1',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '(a) Distribution of adapters across categories.',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM > Category',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.003',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'We provide a full example prompt of the composer’s LLM component in Tab.\\n3, which is plugged through the Gemini',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.002',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Top 500 Adapters sorted by Download Count',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'This section describes StylusDocs, which comprises of 76K Low Rank Adapters (LoRAs) from public repositories, including Civit AI and Hugging Face [24, 43].\\nWe excluded NSFW-labeled adapters from the Civit AI dataset, which originally contained over 100K LoRAs.\\nFigure 13 illustrates the distribution of adapters across various semantic categories and their popularity, measured by download counts.\\nA significant majority, 70%, of adapters belong to the character and celebrity category, primarily consisting of anime or game characters.\\nAnother 13% of adapters modify image style, 8% adjust clothing, and 4% represent various concepts (Fig. 13a).\\nThese statistics indicate that our experiments consider a minor proportion of adapters, as the COCO dataset does not feature characters or celebrities [19].\\nDespite this, Stylus outperforms base Stable Diffusion.\\nFurthermore, the popularity of adapters follows a Pareto distribution, where the top adapters receive exponentially more downloads than the others (Fig. 13a).\\nHowever, the top adapter accounts for only 0.5% of total downloads, which suggests that the distribution is long-tailed.',\n",
       "     'title': 'References > A. Appendix > A.3. Stylus-Bench Characterization',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'We detail different failure modes that were discovered while developing Stylus.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Image saturation.\\nThe quality of image generation is highly depend on adapters’ weights.\\nIf the assigned weight is above the recommended value, the adapter negatively impacts image generation, leading to a growing number of visual inconsistencies and artifacts.\\nIn Fig. 14a, assigning a high weight to a “James Bond” LoRA increases images exposure and introducing significant visual tearing.\\nStylus mitigates over-saturation with its refiner component, which extract the right adapter weights from the adapter’s model card.\\nLastly, Stylus uniformly weights adapters based on their associated tasks, ensuring that similar adapters do not significantly impact their corresponding tasks.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Figure 13.\\nWorkload Characterization of StylusDocs.\\n(a) Most adapters are categorized as characters or celebrities.\\n(b) Adapter popularity exhibits a power-law distribution, with the top adapters receiving exponentially more downloads than the others.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Task Blocking.\\nComposing adapters presents the risk of overwriting existing concepts or tasks specified in the prompt and other selected adapters.\\nWe illustrate several examples in Figure 2—a train LoRA overrides the toy train concept (left), a park bench LoRA masks a person in an orange blanket (middle), and a fancy cake LoRA erases the image of a man eating the cake (right).\\nTask blocking typically arises from two main issues: the adapter weight set too high or too many adapters merged into the base model.\\nStylus addresses this by reducing an adapter’s weight with uniform weighting per task, while the masking scheme reduces the number of selected adapters.\\nAlthough Stylus does not completely solve task blocking, it offers simple heuristics to mitigate the issue.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Prompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Image Prompts',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Prompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Image Prompts',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '• Title: Dwayne”The Rock” Johnson (LoRA)',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '• Tags: Celebrity, Photorealistic, Hollywood, Celeb',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '• Trigger Words: Th3R0ck',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Your goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '% Chain-of-Thought Prompting',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Again, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'First, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Second, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Please format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Task Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Low quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Retrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Provided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '% Chain-of-Thought Prompting',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'First, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Here are the requirements for tasks:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Second, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Here are the requirements for adapters:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Finally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Give me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'composer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Retrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'w=1.0 w=1.5 w=2.0',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Assigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (a) Image Saturation',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (a) Image Saturation > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Adapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (b) Task Blocking',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (b) Task Blocking > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Adding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Figure 14.\\nCategorization of Different Failure Modes.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'To distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'The full prompts to GPT-4V as a judge for textual alignment, visual fidelity, and image diversity are specified in Tables 4 and 5.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'You are a photoshop expert judging which image has better composition quality.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': ' | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'This is IMAGE A. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '% Generated Image from Group A',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'This is IMAGE B. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '% Generated Images from Group B',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Rate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '% Prevent VLM from returning neutral results.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'I’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Table 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'putting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Visual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'You are a photoshop expert judging which set of images is more diverse.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Scoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Diversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'This is GROUP A. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '% Set of 5 Generated Images from Group A',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'This is GROUP B. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '% Set of 5 Generated Images from Group B',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Rate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '% Prevent VLM from returning neutral results.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'I’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Table 5.\\nFull prompt judging diversity using VLM.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'with a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Diversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Fig. 15 decomposes dFID scores over the top 100 keywords in the PartiPrompts dataset.\\nWe highlight that the largest differences stem from concepts, appearances, attributes, or styles.\\nFor example, Stylus excels over concepts ranging from animals (“bears”, “sloth”, and ‘squirrel‘) to objects (“microphone”, “box”, and “jacket”). Selected attributes can include but are not limited to: (“white”, “blue”, and “photographic”). Regardless of keyword, Stylus attains higher diversity scores across the board.',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'},\n",
       "    {'text': 'Figure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity > SD 1.5 Stylus',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A. Appendix'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edb106d0>]},\n",
       "  {'text': 'A.1. Details of the Refiner VLM\\n0.4\\nWe provide a complete example input to the refiner’s VLM in Tab.\\n2.\\nThe prompt utilizes Chain-of-Thought (CoT) prompting, which decomposes the VLM’s goal of producing better adapter descriptions into two steps [42, 44].\\nInitially, the VLM categorizes the adapter’s task into one of several topics—such as concepts, styles, characters, or poses.\\nSubsequently, the VLM is prompted to elaborate on why the adapter is associated with a particular topic and how it modifies images within that context.\\nWe found that this two step logical process significantly improved the structure and quality of model responses.\\n0.3\\n0.2\\n0.1\\n0.0\\nCategory\\n(a) Distribution of adapters across categories.\\n0.004',\n",
       "   'title': 'A.1. Details of the Refiner VLM',\n",
       "   'chunks': [{'text': '0.4',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.1. Details of the Refiner VLM'},\n",
       "    {'text': 'We provide a complete example input to the refiner’s VLM in Tab.\\n2.\\nThe prompt utilizes Chain-of-Thought (CoT) prompting, which decomposes the VLM’s goal of producing better adapter descriptions into two steps [42, 44].\\nInitially, the VLM categorizes the adapter’s task into one of several topics—such as concepts, styles, characters, or poses.\\nSubsequently, the VLM is prompted to elaborate on why the adapter is associated with a particular topic and how it modifies images within that context.\\nWe found that this two step logical process significantly improved the structure and quality of model responses.',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.1. Details of the Refiner VLM'},\n",
       "    {'text': '0.3',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.1. Details of the Refiner VLM'},\n",
       "    {'text': '0.2',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.1. Details of the Refiner VLM'},\n",
       "    {'text': '0.1',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.1. Details of the Refiner VLM'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.1. Details of the Refiner VLM'},\n",
       "    {'text': '(a) Distribution of adapters across categories.',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM > Category',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.1. Details of the Refiner VLM'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Category\\n(a) Distribution of adapters across categories.\\n0.004',\n",
       "   'title': 'Category',\n",
       "   'chunks': [{'text': '(a) Distribution of adapters across categories.',\n",
       "     'title': 'References > A. Appendix > A.1. Details of the Refiner VLM > Category',\n",
       "     'page': 10,\n",
       "     'source_doc': 'Category'}],\n",
       "   'tables': []},\n",
       "  {'text': 'A.2. Details of the Composer LLM\\n0.003\\nWe provide a full example prompt of the composer’s LLM component in Tab.\\n3, which is plugged through the Gemini\\n0.002\\n1.5 endpoint [37].\\nOur experiments feed in descriptions of the top 150 adapters into the LLM’s context.\\nUsing a Chain-of-Thought (CoT) approach, the prompt is structured to first identify keywords or tasks, then allocate appropriate adapters to these tasks.\\nIf necessary, it merges keywords for adapters that span multiple tasks [42, 44].\\n0.001\\n0.000\\nTop 500 Adapters sorted by Download Count',\n",
       "   'title': 'A.2. Details of the Composer LLM',\n",
       "   'chunks': [{'text': '0.003',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.2. Details of the Composer LLM'},\n",
       "    {'text': 'We provide a full example prompt of the composer’s LLM component in Tab.\\n3, which is plugged through the Gemini',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.2. Details of the Composer LLM'},\n",
       "    {'text': '0.002',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.2. Details of the Composer LLM'},\n",
       "    {'text': 'Top 500 Adapters sorted by Download Count',\n",
       "     'title': 'References > A. Appendix > A.2. Details of the Composer LLM',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.2. Details of the Composer LLM'}],\n",
       "   'tables': []},\n",
       "  {'text': 'A.3. Stylus-Bench Characterization\\nThis section describes StylusDocs, which comprises of 76K Low Rank Adapters (LoRAs) from public repositories, including Civit AI and Hugging Face [24, 43].\\nWe excluded NSFW-labeled adapters from the Civit AI dataset, which originally contained over 100K LoRAs.\\nFigure 13 illustrates the distribution of adapters across various semantic categories and their popularity, measured by download counts.\\nA significant majority, 70%, of adapters belong to the character and celebrity category, primarily consisting of anime or game characters.\\nAnother 13% of adapters modify image style, 8% adjust clothing, and 4% represent various concepts (Fig. 13a).\\nThese statistics indicate that our experiments consider a minor proportion of adapters, as the COCO dataset does not feature characters or celebrities [19].\\nDespite this, Stylus outperforms base Stable Diffusion.\\nFurthermore, the popularity of adapters follows a Pareto distribution, where the top adapters receive exponentially more downloads than the others (Fig. 13a).\\nHowever, the top adapter accounts for only 0.5% of total downloads, which suggests that the distribution is long-tailed.',\n",
       "   'title': 'A.3. Stylus-Bench Characterization',\n",
       "   'chunks': [{'text': 'This section describes StylusDocs, which comprises of 76K Low Rank Adapters (LoRAs) from public repositories, including Civit AI and Hugging Face [24, 43].\\nWe excluded NSFW-labeled adapters from the Civit AI dataset, which originally contained over 100K LoRAs.\\nFigure 13 illustrates the distribution of adapters across various semantic categories and their popularity, measured by download counts.\\nA significant majority, 70%, of adapters belong to the character and celebrity category, primarily consisting of anime or game characters.\\nAnother 13% of adapters modify image style, 8% adjust clothing, and 4% represent various concepts (Fig. 13a).\\nThese statistics indicate that our experiments consider a minor proportion of adapters, as the COCO dataset does not feature characters or celebrities [19].\\nDespite this, Stylus outperforms base Stable Diffusion.\\nFurthermore, the popularity of adapters follows a Pareto distribution, where the top adapters receive exponentially more downloads than the others (Fig. 13a).\\nHowever, the top adapter accounts for only 0.5% of total downloads, which suggests that the distribution is long-tailed.',\n",
       "     'title': 'References > A. Appendix > A.3. Stylus-Bench Characterization',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.3. Stylus-Bench Characterization'}],\n",
       "   'tables': []},\n",
       "  {'text': 'A.4. Failure Modes\\nWe detail different failure modes that were discovered while developing Stylus.\\nImage saturation.\\nThe quality of image generation is highly depend on adapters’ weights.\\nIf the assigned weight is above the recommended value, the adapter negatively impacts image generation, leading to a growing number of visual inconsistencies and artifacts.\\nIn Fig. 14a, assigning a high weight to a “James Bond” LoRA increases images exposure and introducing significant visual tearing.\\nStylus mitigates over-saturation with its refiner component, which extract the right adapter weights from the adapter’s model card.\\nLastly, Stylus uniformly weights adapters based on their associated tasks, ensuring that similar adapters do not significantly impact their corresponding tasks.\\n0.005\\n(b) Top 500 adapters ranked by percentage of downloads.\\nFigure 13.\\nWorkload Characterization of StylusDocs.\\n(a) Most adapters are categorized as characters or celebrities.\\n(b) Adapter popularity exhibits a power-law distribution, with the top adapters receiving exponentially more downloads than the others.\\nTask Blocking.\\nComposing adapters presents the risk of overwriting existing concepts or tasks specified in the prompt and other selected adapters.\\nWe illustrate several examples in Figure 2—a train LoRA overrides the toy train concept (left), a park bench LoRA masks a person in an orange blanket (middle), and a fancy cake LoRA erases the image of a man eating the cake (right).\\nTask blocking typically arises from two main issues: the adapter weight set too high or too many adapters merged into the base model.\\nStylus addresses this by reducing an adapter’s weight with uniform weighting per task, while the masking scheme reduces the number of selected adapters.\\nAlthough Stylus does not completely solve task blocking, it offers simple heuristics to mitigate the issue.\\nImage Prompts\\nPrompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.\\nPrompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>\\nModel Card Description\\n• Title: Dwayne”The Rock” Johnson (LoRA)\\n• Tags: Celebrity, Photorealistic, Hollywood, Celeb\\n• Trigger Words: Th3R0ck\\n• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.\\nYour goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:\\n1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.\\n• Some prompts may specify the adapter weight (i.e. <lora:NAME:WEIGHT>).\\nIf provided, you will need to infer the adapter’s name and weight.\\nPrioritize this weight over the author’s recommended weight.\\n2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.\\n• The model card description may be incorrect, misleading, or incomplete.\\n• The model card may specify the weight of the model adapter, or the recommended range.\\nFind the recommended weight of the adapter (default is 0.8).\\n% Chain-of-Thought Prompting\\nAgain, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.\\nFirst, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:\\n• Do not describe any training or dataset-related details.\\n• Provide additional context from your prior knowledge if there is insufficient information.\\n• Do not hallucinate and repeat text.\\nOutput only english words and sentences.\\nSecond, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.\\nPlease format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.\\nTask Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.\\nLow quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.\\nRetrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth\\nProvided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.\\n% Chain-of-Thought Prompting\\nFirst, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.\\nHere are the requirements for tasks:\\n• Tasks should never introduce new information to the prompt.\\nThe topic must be selected from the prompt’s keywords.\\n• Different tasks must be orthogonal from each other.\\n• All tasks combined must span the entirety of the prompt.\\n• Prioritize choosing narrower tasks.\\nYou may merge tasks if a relevant adapter spans several tasks.\\nSecond, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.\\nHere are the requirements for adapters:\\n• Adapters should only be used at most once across all tasks.\\nIf an adapter is used in one task, it should not be used in another task.\\n• Adapters should not introduce novel concepts or biases to the topic or the prompt.\\nDo not include such adapters.\\n• Adapters cannot encompass a broader scope relative to its assigned task.\\nFor example, if the task is about a “dog”, the adapter cannot be about general “animals”.\\n• Adapters cannot be too narrow in scope relative to its assigned task.\\nFor example, if a task is about pandas, do not choose highly specific pandas such as the character “Po” from Kung Fu Panda.\\nHowever, it is acceptable to choose adapters that modify the style of the task, such as “Red Pandas”.\\n• If an adapter spans multiple tasks, merge these tasks together.\\nFor example, if there is an adapter that is about “fluffy cats”, merge the topics “fluffy” and “cats” together.\\n• Avoid choosing NSFW and anthropormorphic adapters.\\nFinally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.\\nGive me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.\\ncomposer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.\\nRetrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the\\nw=1.0 w=1.5 w=2.0\\n(a) Image Saturation\\nAssigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.\\nStylus\\nSD\\nv1.5\\n(b) Task Blocking\\nAdapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).\\nStylus\\nSD\\nv1.5\\n(c) Task Diversity\\nAdding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).\\nStylus\\nSD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       "   'title': 'A.4. Failure Modes',\n",
       "   'chunks': [{'text': 'We detail different failure modes that were discovered while developing Stylus.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Image saturation.\\nThe quality of image generation is highly depend on adapters’ weights.\\nIf the assigned weight is above the recommended value, the adapter negatively impacts image generation, leading to a growing number of visual inconsistencies and artifacts.\\nIn Fig. 14a, assigning a high weight to a “James Bond” LoRA increases images exposure and introducing significant visual tearing.\\nStylus mitigates over-saturation with its refiner component, which extract the right adapter weights from the adapter’s model card.\\nLastly, Stylus uniformly weights adapters based on their associated tasks, ensuring that similar adapters do not significantly impact their corresponding tasks.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Figure 13.\\nWorkload Characterization of StylusDocs.\\n(a) Most adapters are categorized as characters or celebrities.\\n(b) Adapter popularity exhibits a power-law distribution, with the top adapters receiving exponentially more downloads than the others.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Task Blocking.\\nComposing adapters presents the risk of overwriting existing concepts or tasks specified in the prompt and other selected adapters.\\nWe illustrate several examples in Figure 2—a train LoRA overrides the toy train concept (left), a park bench LoRA masks a person in an orange blanket (middle), and a fancy cake LoRA erases the image of a man eating the cake (right).\\nTask blocking typically arises from two main issues: the adapter weight set too high or too many adapters merged into the base model.\\nStylus addresses this by reducing an adapter’s weight with uniform weighting per task, while the masking scheme reduces the number of selected adapters.\\nAlthough Stylus does not completely solve task blocking, it offers simple heuristics to mitigate the issue.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes',\n",
       "     'page': 10,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Prompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Image Prompts',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Prompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Image Prompts',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': '• Title: Dwayne”The Rock” Johnson (LoRA)',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': '• Tags: Celebrity, Photorealistic, Hollywood, Celeb',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': '• Trigger Words: Th3R0ck',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': '• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Your goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': '1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': '2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': '% Chain-of-Thought Prompting',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Again, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'First, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Second, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Please format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Task Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Low quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Retrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Provided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': '% Chain-of-Thought Prompting',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'First, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Here are the requirements for tasks:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Second, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Here are the requirements for adapters:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Finally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Give me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'composer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Retrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'w=1.0 w=1.5 w=2.0',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Assigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (a) Image Saturation',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (a) Image Saturation > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Adapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (b) Task Blocking',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (b) Task Blocking > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Adding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': '(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'Figure 14.\\nCategorization of Different Failure Modes.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.4. Failure Modes'},\n",
       "    {'text': 'To distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.4. Failure Modes'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Image Prompts\\nPrompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.\\nPrompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>',\n",
       "   'title': 'Image Prompts',\n",
       "   'chunks': [{'text': 'Prompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Image Prompts',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Image Prompts'},\n",
       "    {'text': 'Prompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Image Prompts',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Image Prompts'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Model Card Description\\n• Title: Dwayne”The Rock” Johnson (LoRA)\\n• Tags: Celebrity, Photorealistic, Hollywood, Celeb\\n• Trigger Words: Th3R0ck\\n• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.\\nYour goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:\\n1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.\\n• Some prompts may specify the adapter weight (i.e. <lora:NAME:WEIGHT>).\\nIf provided, you will need to infer the adapter’s name and weight.\\nPrioritize this weight over the author’s recommended weight.\\n2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.\\n• The model card description may be incorrect, misleading, or incomplete.\\n• The model card may specify the weight of the model adapter, or the recommended range.\\nFind the recommended weight of the adapter (default is 0.8).\\n% Chain-of-Thought Prompting\\nAgain, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.\\nFirst, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:\\n• Do not describe any training or dataset-related details.\\n• Provide additional context from your prior knowledge if there is insufficient information.\\n• Do not hallucinate and repeat text.\\nOutput only english words and sentences.\\nSecond, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.\\nPlease format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.\\nTask Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.\\nLow quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.\\nRetrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth\\nProvided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.\\n% Chain-of-Thought Prompting\\nFirst, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.\\nHere are the requirements for tasks:\\n• Tasks should never introduce new information to the prompt.\\nThe topic must be selected from the prompt’s keywords.\\n• Different tasks must be orthogonal from each other.\\n• All tasks combined must span the entirety of the prompt.\\n• Prioritize choosing narrower tasks.\\nYou may merge tasks if a relevant adapter spans several tasks.\\nSecond, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.\\nHere are the requirements for adapters:\\n• Adapters should only be used at most once across all tasks.\\nIf an adapter is used in one task, it should not be used in another task.\\n• Adapters should not introduce novel concepts or biases to the topic or the prompt.\\nDo not include such adapters.\\n• Adapters cannot encompass a broader scope relative to its assigned task.\\nFor example, if the task is about a “dog”, the adapter cannot be about general “animals”.\\n• Adapters cannot be too narrow in scope relative to its assigned task.\\nFor example, if a task is about pandas, do not choose highly specific pandas such as the character “Po” from Kung Fu Panda.\\nHowever, it is acceptable to choose adapters that modify the style of the task, such as “Red Pandas”.\\n• If an adapter spans multiple tasks, merge these tasks together.\\nFor example, if there is an adapter that is about “fluffy cats”, merge the topics “fluffy” and “cats” together.\\n• Avoid choosing NSFW and anthropormorphic adapters.\\nFinally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.\\nGive me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.\\ncomposer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.\\nRetrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the\\nw=1.0 w=1.5 w=2.0',\n",
       "   'title': 'Model Card Description',\n",
       "   'chunks': [{'text': '• Title: Dwayne”The Rock” Johnson (LoRA)',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': '• Tags: Celebrity, Photorealistic, Hollywood, Celeb',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': '• Trigger Words: Th3R0ck',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': '• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Your goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': '1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': '2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': '% Chain-of-Thought Prompting',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Again, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'First, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Second, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Please format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Task Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Low quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 11,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Retrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Provided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': '% Chain-of-Thought Prompting',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'First, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Here are the requirements for tasks:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Second, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Here are the requirements for adapters:',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Finally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Give me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'composer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'Retrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 12,\n",
       "     'source_doc': 'Model Card Description'},\n",
       "    {'text': 'w=1.0 w=1.5 w=2.0',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > Model Card Description',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Model Card Description'}],\n",
       "   'tables': []},\n",
       "  {'text': '(a) Image Saturation\\nAssigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.\\nStylus\\nSD\\nv1.5',\n",
       "   'title': '(a) Image Saturation',\n",
       "   'chunks': [{'text': 'Assigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (a) Image Saturation',\n",
       "     'page': 13,\n",
       "     'source_doc': '(a) Image Saturation'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (a) Image Saturation > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': '(a) Image Saturation'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Stylus\\nSD\\nv1.5',\n",
       "   'title': 'Stylus',\n",
       "   'chunks': [{'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (a) Image Saturation > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Stylus'}],\n",
       "   'tables': []},\n",
       "  {'text': 'SD\\nv1.5',\n",
       "   'title': 'SD',\n",
       "   'chunks': [{'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (a) Image Saturation > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'SD'}],\n",
       "   'tables': []},\n",
       "  {'text': '(b) Task Blocking\\nAdapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).\\nStylus\\nSD\\nv1.5',\n",
       "   'title': '(b) Task Blocking',\n",
       "   'chunks': [{'text': 'Adapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (b) Task Blocking',\n",
       "     'page': 13,\n",
       "     'source_doc': '(b) Task Blocking'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (b) Task Blocking > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': '(b) Task Blocking'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Stylus\\nSD\\nv1.5',\n",
       "   'title': 'Stylus',\n",
       "   'chunks': [{'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (b) Task Blocking > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Stylus'}],\n",
       "   'tables': []},\n",
       "  {'text': 'SD\\nv1.5',\n",
       "   'title': 'SD',\n",
       "   'chunks': [{'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (b) Task Blocking > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'SD'}],\n",
       "   'tables': []},\n",
       "  {'text': '(c) Task Diversity\\nAdding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).\\nStylus\\nSD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       "   'title': '(c) Task Diversity',\n",
       "   'chunks': [{'text': 'Adding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity',\n",
       "     'page': 13,\n",
       "     'source_doc': '(c) Task Diversity'},\n",
       "    {'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': '(c) Task Diversity'},\n",
       "    {'text': '(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': '(c) Task Diversity'},\n",
       "    {'text': 'Figure 14.\\nCategorization of Different Failure Modes.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': '(c) Task Diversity'},\n",
       "    {'text': 'To distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': '(c) Task Diversity'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Stylus\\nSD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       "   'title': 'Stylus',\n",
       "   'chunks': [{'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': '(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': 'Figure 14.\\nCategorization of Different Failure Modes.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Stylus'},\n",
       "    {'text': 'To distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'Stylus'}],\n",
       "   'tables': []},\n",
       "  {'text': 'SD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       "   'title': 'SD',\n",
       "   'chunks': [{'text': 'v1.5',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': '(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': 'Figure 14.\\nCategorization of Different Failure Modes.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'SD'},\n",
       "    {'text': 'To distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       "     'title': 'References > A. Appendix > A.4. Failure Modes > (c) Task Diversity > Stylus > SD',\n",
       "     'page': 13,\n",
       "     'source_doc': 'SD'}],\n",
       "   'tables': []},\n",
       "  {'text': 'A.5. VLM as a Judge\\nThe full prompts to GPT-4V as a judge for textual alignment, visual fidelity, and image diversity are specified in Tables 4 and 5.\\nSystem Prompt:\\nYou are a photoshop expert judging which image has better composition quality.\\n | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n\\nUser:\\nThis is IMAGE A. Reply ’ACK’.\\n% Generated Image from Group A\\nAssistant: ACK User:\\nThis is IMAGE B. Reply ’ACK’.\\n% Generated Images from Group B\\nAssistant: ACK User:\\nRate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.\\nImage A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• You must pick a group for ’Better Quality’ / ’Better Alignment’, neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.\\nputting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.\\nVisual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.\\nSystem Prompt:\\nYou are a photoshop expert judging which set of images is more diverse.\\nScoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).\\nDiversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.\\n• Theme Interpretation: The theme can vary based on interpretation.\\nThe theme “it’s raining cats and dogs” can have a literal interpretation as cats and dogs falling from the sky or a figurative interpretation as heavy rain.\\nThe images are diverse, since they show both weather and animals.\\nIf the group only contains images of heavy rain or animals, a diversity score of 1 should be given.\\n• Main Subject: The main subject changes based on the focus across different subjects.\\nA set of images that contains a mix of images of apples and children dressed as different kinds of apples is more diverse than a set with only children dressed as apples.\\nNote the more diverse set has children as the subject for some images and apples as the subject for other images.\\nUser:\\nThis is GROUP A. Reply ’ACK’.\\n% Set of 5 Generated Images from Group A\\nAssistant: ACK User:\\nThis is GROUP B. Reply ’ACK’.\\n% Set of 5 Generated Images from Group B\\nAssistant: ACK User:\\nRate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.\\nGroup A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• Don’t forget to reward different main subjects in the diversity score.\\n• You must pick a group for ’More Diversity,’ neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 5.\\nFull prompt judging diversity using VLM.\\nwith a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.\\nDiversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       "   'title': 'A.5. VLM as a Judge',\n",
       "   'chunks': [{'text': 'The full prompts to GPT-4V as a judge for textual alignment, visual fidelity, and image diversity are specified in Tables 4 and 5.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge',\n",
       "     'page': 13,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'You are a photoshop expert judging which image has better composition quality.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': ' | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'This is IMAGE A. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': '% Generated Image from Group A',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'This is IMAGE B. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': '% Generated Images from Group B',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'Rate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': '% Prevent VLM from returning neutral results.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'I’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'Table 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'putting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'Visual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'You are a photoshop expert judging which set of images is more diverse.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'Scoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'Diversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'This is GROUP A. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': '% Set of 5 Generated Images from Group A',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'This is GROUP B. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': '% Set of 5 Generated Images from Group B',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'Rate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': '% Prevent VLM from returning neutral results.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'I’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'Table 5.\\nFull prompt judging diversity using VLM.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'with a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'},\n",
       "    {'text': 'Diversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.5. VLM as a Judge'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edb106d0>]},\n",
       "  {'text': 'System Prompt:\\nYou are a photoshop expert judging which image has better composition quality.\\n | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n',\n",
       "   'title': 'System Prompt:',\n",
       "   'chunks': [{'text': 'You are a photoshop expert judging which image has better composition quality.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'System Prompt:'},\n",
       "    {'text': ' | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'System Prompt:'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edb106d0>]},\n",
       "  {'text': 'User:\\nThis is IMAGE A. Reply ’ACK’.\\n% Generated Image from Group A',\n",
       "   'title': 'User:',\n",
       "   'chunks': [{'text': 'This is IMAGE A. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'User:'},\n",
       "    {'text': '% Generated Image from Group A',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'User:'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Assistant: ACK User:\\nThis is IMAGE B. Reply ’ACK’.\\n% Generated Images from Group B',\n",
       "   'title': 'Assistant: ACK User:',\n",
       "   'chunks': [{'text': 'This is IMAGE B. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': '% Generated Images from Group B',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Assistant: ACK User:'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Assistant: ACK User:\\nRate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.\\nImage A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• You must pick a group for ’Better Quality’ / ’Better Alignment’, neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.\\nputting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.\\nVisual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.',\n",
       "   'title': 'Assistant: ACK User:',\n",
       "   'chunks': [{'text': 'Rate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': '% Prevent VLM from returning neutral results.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': 'I’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': 'Table 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': 'putting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': 'Visual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Assistant: ACK User:'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• You must pick a group for ’Better Quality’ / ’Better Alignment’, neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.\\nputting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.\\nVisual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.',\n",
       "   'title': 'Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "   'chunks': [{'text': '% Prevent VLM from returning neutral results.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)'},\n",
       "    {'text': 'I’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)'},\n",
       "    {'text': 'Table 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)'},\n",
       "    {'text': 'putting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)'},\n",
       "    {'text': 'Visual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 14,\n",
       "     'source_doc': 'Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)'}],\n",
       "   'tables': []},\n",
       "  {'text': 'System Prompt:\\nYou are a photoshop expert judging which set of images is more diverse.\\nScoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).\\nDiversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.\\n• Theme Interpretation: The theme can vary based on interpretation.\\nThe theme “it’s raining cats and dogs” can have a literal interpretation as cats and dogs falling from the sky or a figurative interpretation as heavy rain.\\nThe images are diverse, since they show both weather and animals.\\nIf the group only contains images of heavy rain or animals, a diversity score of 1 should be given.\\n• Main Subject: The main subject changes based on the focus across different subjects.\\nA set of images that contains a mix of images of apples and children dressed as different kinds of apples is more diverse than a set with only children dressed as apples.\\nNote the more diverse set has children as the subject for some images and apples as the subject for other images.',\n",
       "   'title': 'System Prompt:',\n",
       "   'chunks': [{'text': 'You are a photoshop expert judging which set of images is more diverse.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'System Prompt:'},\n",
       "    {'text': 'Scoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'System Prompt:'},\n",
       "    {'text': 'Diversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > System Prompt:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'System Prompt:'}],\n",
       "   'tables': []},\n",
       "  {'text': 'User:\\nThis is GROUP A. Reply ’ACK’.\\n% Set of 5 Generated Images from Group A',\n",
       "   'title': 'User:',\n",
       "   'chunks': [{'text': 'This is GROUP A. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'User:'},\n",
       "    {'text': '% Set of 5 Generated Images from Group A',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'User:'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Assistant: ACK User:\\nThis is GROUP B. Reply ’ACK’.\\n% Set of 5 Generated Images from Group B',\n",
       "   'title': 'Assistant: ACK User:',\n",
       "   'chunks': [{'text': 'This is GROUP B. Reply ’ACK’.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': '% Set of 5 Generated Images from Group B',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Assistant: ACK User:'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Assistant: ACK User:\\nRate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.\\nGroup A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• Don’t forget to reward different main subjects in the diversity score.\\n• You must pick a group for ’More Diversity,’ neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 5.\\nFull prompt judging diversity using VLM.\\nwith a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.\\nDiversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       "   'title': 'Assistant: ACK User:',\n",
       "   'chunks': [{'text': 'Rate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User:',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': '% Prevent VLM from returning neutral results.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': 'I’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': 'Table 5.\\nFull prompt judging diversity using VLM.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': 'with a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Assistant: ACK User:'},\n",
       "    {'text': 'Diversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Assistant: ACK User:'}],\n",
       "   'tables': []},\n",
       "  {'text': 'Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• Don’t forget to reward different main subjects in the diversity score.\\n• You must pick a group for ’More Diversity,’ neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 5.\\nFull prompt judging diversity using VLM.\\nwith a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.\\nDiversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       "   'title': 'Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "   'chunks': [{'text': '% Prevent VLM from returning neutral results.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)'},\n",
       "    {'text': 'I’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)'},\n",
       "    {'text': 'Table 5.\\nFull prompt judging diversity using VLM.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)'},\n",
       "    {'text': 'with a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)'},\n",
       "    {'text': 'Diversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       "     'title': 'References > A. Appendix > A.5. VLM as a Judge > Assistant: ACK User: > Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)',\n",
       "     'page': 15,\n",
       "     'source_doc': 'Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)'}],\n",
       "   'tables': []},\n",
       "  {'text': 'A.6. Additional Diversity Scores\\nFig. 15 decomposes dFID scores over the top 100 keywords in the PartiPrompts dataset.\\nWe highlight that the largest differences stem from concepts, appearances, attributes, or styles.\\nFor example, Stylus excels over concepts ranging from animals (“bears”, “sloth”, and ‘squirrel‘) to objects (“microphone”, “box”, and “jacket”). Selected attributes can include but are not limited to: (“white”, “blue”, and “photographic”). Regardless of keyword, Stylus attains higher diversity scores across the board.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5 Parti Prompts Diversity\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nSD 1.5 Stylus\\nFigure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       "   'title': 'A.6. Additional Diversity Scores',\n",
       "   'chunks': [{'text': 'Fig. 15 decomposes dFID scores over the top 100 keywords in the PartiPrompts dataset.\\nWe highlight that the largest differences stem from concepts, appearances, attributes, or styles.\\nFor example, Stylus excels over concepts ranging from animals (“bears”, “sloth”, and ‘squirrel‘) to objects (“microphone”, “box”, and “jacket”). Selected attributes can include but are not limited to: (“white”, “blue”, and “photographic”). Regardless of keyword, Stylus attains higher diversity scores across the board.',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 15,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'},\n",
       "    {'text': 'Figure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity > SD 1.5 Stylus',\n",
       "     'page': 16,\n",
       "     'source_doc': 'A.6. Additional Diversity Scores'}],\n",
       "   'tables': []},\n",
       "  {'text': '2.5 Parti Prompts Diversity\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nSD 1.5 Stylus\\nFigure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       "   'title': '2.5 Parti Prompts Diversity',\n",
       "   'chunks': [{'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '0.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '0.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '1.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '1.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '2.0',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': '2.5',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'},\n",
       "    {'text': 'Figure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity > SD 1.5 Stylus',\n",
       "     'page': 16,\n",
       "     'source_doc': '2.5 Parti Prompts Diversity'}],\n",
       "   'tables': []},\n",
       "  {'text': 'SD 1.5 Stylus\\nFigure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       "   'title': 'SD 1.5 Stylus',\n",
       "   'chunks': [{'text': 'Figure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       "     'title': 'References > A. Appendix > A.6. Additional Diversity Scores > 2.5 Parti Prompts Diversity > SD 1.5 Stylus',\n",
       "     'page': 16,\n",
       "     'source_doc': 'SD 1.5 Stylus'}],\n",
       "   'tables': []}],\n",
       " [{'text': 'Hallucination of Multimodal Large Language Models: A Survey\\nZECHEN BAI, Show Lab, National University of Singapore, Singapore PICHAO WANG, Amazon Prime Video, USA TIANJUN XIAO, AWS Shanghai AI Lab, China TONG HE, AWS Shanghai AI Lab, China ZONGBO HAN, Show Lab, National University of Singapore, Singapore ZHENG ZHANG, AWS Shanghai AI Lab, China MIKE ZHENG SHOU∗, Show Lab, National University of Singapore, Singapore This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks.\\nDespite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications.\\nThis problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies.\\nWe review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue.\\nAdditionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research.\\nBy drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field.\\nThrough our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike.\\nResources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.',\n",
       "   'title': 'Hallucination of Multimodal Large Language Models: A Survey',\n",
       "   'chunks': [{'text': 'ZECHEN BAI, Show Lab, National University of Singapore, Singapore PICHAO WANG, Amazon Prime Video, USA TIANJUN XIAO, AWS Shanghai AI Lab, China TONG HE, AWS Shanghai AI Lab, China ZONGBO HAN, Show Lab, National University of Singapore, Singapore ZHENG ZHANG, AWS Shanghai AI Lab, China MIKE ZHENG SHOU∗, Show Lab, National University of Singapore, Singapore This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks.\\nDespite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications.\\nThis problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies.\\nWe review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue.\\nAdditionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research.\\nBy drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field.\\nThrough our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike.\\nResources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.',\n",
       "     'title': 'Hallucination of Multimodal Large Language Models: A Survey',\n",
       "     'page': 0,\n",
       "     'source_doc': 'Hallucination of Multimodal Large Language Models: A Survey'}],\n",
       "   'tables': []},\n",
       "  {'text': 'CCS Concepts: •Computingmethodologies→Computer vision;Natural language processing;Machine\\nlearning.\\nAdditional Key Words and Phrases: Hallucination, Multimodal, Large Language Models, Vision-Language Models.',\n",
       "   'title': 'CCS Concepts: •Computingmethodologies→Computer vision;Natural language processing;Machine',\n",
       "   'chunks': [{'text': 'learning.',\n",
       "     'title': 'CCS Concepts: •Computingmethodologies→Computer vision;Natural language processing;Machine',\n",
       "     'page': 0,\n",
       "     'source_doc': 'CCS Concepts: •Computingmethodologies→Computer vision;Natural language processing;Machine'},\n",
       "    {'text': 'Additional Key Words and Phrases: Hallucination, Multimodal, Large Language Models, Vision-Language Models.',\n",
       "     'title': 'CCS Concepts: •Computingmethodologies→Computer vision;Natural language processing;Machine',\n",
       "     'page': 0,\n",
       "     'source_doc': 'CCS Concepts: •Computingmethodologies→Computer vision;Natural language processing;Machine'}],\n",
       "   'tables': []},\n",
       "  {'text': 'ACM Reference Format:\\nZechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou.\\n2024.\\nHallucination of Multimodal Large Language Models: A Survey.\\nPreprint 1, 1 (April 2024), 30 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX ∗Corresponding Author Authors’ addresses: Zechen Bai, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore, zechenbai@u.nus.edu; Pichao Wang, Amazon Prime Video, Washington, USA, pichaowang@gmail.com; Tianjun Xiao, AWS Shanghai AI Lab, Shanghai, China, tianjux@amazon.com; Tong He, AWS Shanghai AI Lab, Shanghai, China, htong@ amazon.com; Zongbo Han, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore, hanzb1997@gmail.com; Zheng Zhang, AWS Shanghai AI Lab, Shanghai, China, zhaz@amazon.com; Mike Zheng Shou, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore, mike.zheng.shou@gmail.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\\nCopyrights for components of this work owned by others than the author(s) must be honored.\\nAbstracting with credit is permitted.\\nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\\nRequest permissions from permissions@acm.org.\\n© 2024 Copyright held by the owner/author(s).\\nPublication rights licensed to ACM. ACM 0000-0000/2024/4-ART https://doi.org/XXXXXXX.XXXXXXX',\n",
       "   'title': 'ACM Reference Format:',\n",
       "   'chunks': [{'text': 'Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou.\\n2024.\\nHallucination of Multimodal Large Language Models: A Survey.\\nPreprint 1, 1 (April 2024), 30 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX ∗Corresponding Author Authors’ addresses: Zechen Bai, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore, zechenbai@u.nus.edu; Pichao Wang, Amazon Prime Video, Washington, USA, pichaowang@gmail.com; Tianjun Xiao, AWS Shanghai AI Lab, Shanghai, China, tianjux@amazon.com; Tong He, AWS Shanghai AI Lab, Shanghai, China, htong@ amazon.com; Zongbo Han, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore, hanzb1997@gmail.com; Zheng Zhang, AWS Shanghai AI Lab, Shanghai, China, zhaz@amazon.com; Mike Zheng Shou, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore, mike.zheng.shou@gmail.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\\nCopyrights for components of this work owned by others than the author(s) must be honored.\\nAbstracting with credit is permitted.\\nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\\nRequest permissions from permissions@acm.org.\\n© 2024 Copyright held by the owner/author(s).\\nPublication rights licensed to ACM. ACM 0000-0000/2024/4-ART https://doi.org/XXXXXXX.XXXXXXX',\n",
       "     'title': 'ACM Reference Format:',\n",
       "     'page': 0,\n",
       "     'source_doc': 'ACM Reference Format:'}],\n",
       "   'tables': []},\n",
       "  {'text': 'INTRODUCTION\\nRecently, the emergence of large language models (LLMs) [29, 81, 85, 99, 132] has dominated a wide range of tasks in natural language processing (NLP), achieving unprecedented progress in language understanding [39, 47], generation [128, 140] and reasoning [20, 58, 87, 107, 115].\\nLeveraging the capabilities of robust LLMs, multimodal large language models (MLLMs) [22, 75, 111, 138], sometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\\nMLLMs show promising ability in multimodal tasks, such as image captioning [66], visual question answering [22, 75], etc.\\nHowever, there is a concerning trend associated with the rapid advancement in MLLMs.\\nThese models exhibit an inclination to generate hallucinations [69, 76, 137], resulting in seemingly plausible yet factually spurious content.\\nThe problem of hallucination originates from LLMs themselves.\\nIn the NLP community, the hallucination problem is empirically categorized into two types [44]: 1) factuality hallucination emphasizes the discrepancy between generated content and verifiable real-world facts, typically manifesting as factual inconsistency or fabrication; 2) faithfulness hallucination refers to the divergence of generated content from user instructions or the context provided by the input, as well as self-consistency within generated content.\\nIn contrast to pure LLMs, research efforts of hallucination in MLLMs mainly focus on the discrepancy between generated text response and provided visual content [69, 76, 137], i.e., cross-modal inconsistency.\\nThis difference suggests that studies in LLMs cannot be seemingly transferred to MLLMs.\\nTherefore, there is a growing need to comprehensively survey recent advancements in MLLMs’ hallucination phenomena to inspire new ideas and foster the field’s development.\\nIn the realm of computer vision, object recognition is the core task, including sub-tasks such as object classification [60], detection [27], and segmentation [37], etc.\\nSimilarly, studies on hallucination in MLLMs primarily focus on object hallucination.\\nIn pre-MLLM era, there is a pioneering work on object hallucination in image captioning [90], evaluating object existence by comparing captions and image content.\\nIn MLLMs, object hallucination has been empirically categorized into three categories: 1) category, which identifies nonexistent or incorrect object categories in the given image; 2) attribute, which emphasizes descriptions of the objects’ attributes, such as color, shape, material, etc; and 3) relation, which assesses the relationships among objects, such as human-object interactions or relative positions.\\nNote that some literature may consider objects counting, objects event, etc., as independent hallucination categories; however, in this work, we include them into attribute category.\\nAs numerous studies exist on the underlying causes of hallucinations in LLMs, the unique challenges posed by cutting-edge MLLMs warrant an in-depth investigation.\\nOur analysis specifically targets the unique origins of hallucinations in MLLMs, spanning a spectrum of contributing factors from data, model, training, to the inference stage.\\nIn addition, we provide a comprehensive overview of benchmarks and metrics designed specifically for evaluating hallucinations in MLLMs.\\nThen, we review and discuss recent works tailored to mitigate the problem of hallucination from the viewpoints of the identified causes.\\nThrough our comprehensive survey, we aim to contribute to advancing the field of MLLMs and offer valuable insights that deepen understanding of the opportunities and challenges associated with hallucinations in MLLMs.\\nThis exploration not only enhances our understanding of the limitations of current MLLMs but also offers essential guidance for future research and the development of more robust and trustworthy MLLMs.\\nComparison with existing surveys.\\nIn pursuit of reliable generative AI, hallucination stands out as a major challenge, leading to a series of survey papers on its recent advancements.\\nFor pure LLMs, there are several surveys [44, 129], describing the landscape of hallucination in LLMs.\\nIn contrast, there are very few surveys on hallucination in the field of MLLMs.\\nTo the best of our knowledge, there is only one concurrent work [76], a short survey on the hallucination problem of LVLMs.\\nHowever, our survey distinguishes itself in terms of both taxonomy and scope.\\nWe present a layered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape of this field.\\nAdditionally, our approach does not limit itself to specific model architectures as prescribed in the work of [76], but rather dissects the causes of hallucinations by tracing back to various affecting factors.\\nWe cover a larger range of literature both in terms of paper number and taxonomy structure.\\nFurthermore, our mitigation strategies are intricately linked to the underlying causes, ensuring a cohesive and targeted approach.\\nOrganization of this survey.\\nIn this paper, we present a comprehensive survey of the latest developments regarding hallucinations in MLLMs.\\nThe survey is organized as follows: We begin by providing sufficient context and defining concepts related to LLMs, MLLMs, hallucination, etc.\\nNext, we delve into an in-depth analysis of the factors contributing to hallucinations in MLLMs.\\nFollowing this, we present a set of metrics and benchmarks employed for evaluating hallucinations in MLLMs.\\nWe then elaborate on a range of approaches designed to mitigate hallucinations in MLLMs.\\nFinally, we delve into the challenges and open questions that frame the current limitations and future prospects of this field, offering insights and delineating potential pathways for forthcoming research.\\n2 DEFINITIONS\\n2.1 Large Language Models\\nBefore moving to multimodal large language models, it is essential to introduce the concept of large language models.\\nTypically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets.\\nProminent examples include GPT-3 [8], PaLM [18], LLaMA [99], and GPT-4 [82].\\nThrough scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning[8], Chain-of-Thought prompting[107] and instruction following[86], among others.\\nThe characteristics and behaviors of LLMs are intricately linked to their training processes.\\nLLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF).\\nBelow, we provide a concise overview of each stage to facilitate comprehension.\\nPre-trianing.\\nPre-training serves as a fundamental phase in the learning process of LLMs [134].\\nDuring this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence.\\nBy undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities.\\nThis pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively.\\nSupervised Fine-Tuning.\\nAlthough pre-training equips LLMs with substantial knowledge and skills, it’s important to acknowledge that its primary focus is on optimizing for completion.\\nConsequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user’s objective of obtaining desired responses.\\nTo address this disparity, the concept of Supervised FineTuning (SFT) [125] has been introduced.\\nSFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs.\\nReinforcement Learning from Human Feedback.\\nAlthough SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.\\nAmong the various methods, Reinforcement Learning from Human Feedback\\nData Quantity Insufficient Data e.g. AMBER [103], LLaVA-RLHF [96]\\nData Quality\\nStatistic Bias\\nVision Model\\n | Language Model | Parametric Knowledge | e.g. | VCD [64], Volcano [63]\\n | --- | --- | --- | ---\\n | Cross-modal Interface | Inferior Alignment | e.g. | HACL [52], Halle-Switch [123]\\n | Sequence Supervision | e.g. MOCHa [5], OPERA [45] |  | \\n | Visual Supervision | e.g. Chen et al. [16] |  | \\n | Human Feedback | e.g. RLHF-V [119] |  | \\n | Lose Visual Attention | e.g. OPERA [45], HaELM [104] |  | \\n | CHAIR | CHAIR [90] |  | \\n | POPE | POPE [69] |  | \\n | LLM-based | e.g. GAVIE [73], HaELM [104], HallusionBench [72] |  | \\n | Others | e.g. Faith-Score [55], AMBER [103] |  | \\n | Discriminative Task | e.g. POPE [69], RAH-Bench [16], FGHE [105] |  | \\n | Generative Task | e.g. GAVIE [73], Faith-Score [55] |  | \\n | Introducing | Negative Data | e.g. | LRV-Instruction [73]\\n | Introducing | Counterfactual Data | e.g. | HalluciDoctor [117]\\n | Mitigating Noises and Errors | e.g. ReCaption [105], EOS [120] |  | \\n | Scale-up Resolution | e.g. LLaVA-1.5 [74], InternVL [14], HallE-Switch [123] |  | \\n | Versatile | Vision Encoders | e.g. | VCoder [49], IVE [38]\\n | Dedicated Module | e.g. HallE-Switch [123] |  | \\n | Auxiliary Supervision\\n |  | Noisy Data | e.g. | HalluciDoctor [117], LLaVA-1.5 [74]\\n |  | Lack of Diversity | e.g. | LRV-Instruction [73], HalluciDoctor [117]\\n |  | Detailed descriptions (open question) | e.g. | Chen et al. [16], EOS [120]\\n |  | Frequent Objects | e.g. | POPE [69]\\n |  | Objects Occurrence | e.g. | LURE [137], VCD [64]\\n |  | Information Loss | e.g. | HallusionBench [72], AMBER [103]\\n |  | Feature Bias | e.g. | Tong et al. [98]\\n\\nfrom Data (§3.1) Hallucination from Model (§3.2)\\nHallucination Causes (§3)\\nHallucination Metrics and Benchmarks(§4)\\nHallucination from Training (§3.3) Hallucination from Inference (§3.4)\\nHallucination Metrics\\nHallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)\\n | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n\\nHallucination Mitigation (§5)\\nReinforcement Learning\\nGeneration Intervention\\nPost-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]\\nMitigating Inference-related Hallucinations (§5.4)\\nFig. 1.\\nThe main content flow and categorization of this survey.\\n(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.\\n | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n\\nFig. 2.\\nPopular architecture of multimodal large language model.\\n2.2 Multimodal Large Language Models\\nMLLMs [22, 75, 111, 138] typically refers to a series of models that enable LLMs to perceive and comprehend data from various modalities.\\nAmong them, vision+LLM is particularly prominent, owing to the extensive research on vision-language models (VLMs) [51, 88, 116] prior to LLMs.\\nAs a result, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models (LVLMs).\\nThe goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\" the world via images or videos.\\nCombined with strong reasoning and language generation abilities, MLLMs trigger a series of downstream tasks in multimodal domains, such as image/video captioning and visual question answering.\\nAdditionally, MLLMs serve as the foundation for applications in other fields, such as AI assistants, embodied agents, and robotics.\\nIntegrating the two modalities of vision and language involves primarily two types of approaches.\\nThe first line of work is built upon off-the-shelf pre-trained uni-modal models.\\nSpecifically, these MLLMs usually incorporate a learnable interface between pre-trained visual encoders and LLMs.\\nThe interface extracts and integrates information from visual modalities.\\nSuch interfaces can be further categorized into 1) learnable query-based and 2) projection layer based.\\nLearnable querybased methods, exemplified by Q-Former [66], as used in MiniGPT-4 [138] and Instruct-BLIP [22], utilize a set of learnable query tokens to capture visual signals via cross-attention.\\nProjection layerbased methods, as widely applied in LLaVA [75], Shikra [12], etc., involve training a linear projection layer or a Multi-Layer Perceptron (MLP) module to transform extracted visual features.\\nBoth types of interfaces aim to transform pre-trained visual features into the input space of pre-trained LLMs.\\nAnother line of work is represented by Fuyu-8B [4] and Gemini [97].\\nUnlike previous methods that leverage pre-trained uni-modal models, these works employ end-to-end training from scratch.\\nTaking Fuyu-8B as an example, it does not employ any pre-trained vision encoder.\\nInstead, it directly inputs image patches and employs a linear projection to transform the raw pixels of each patch into embeddings.\\nThe abstracted pipeline is depicted in Fig. 2.\\nMLLMs take input from both visual and textual modalities, learning from multimodal instructions and responses, which leads to remarkable performance across various multimodal tasks.\\nRegarding the training of MLLMs, we provide a concise overview of the training process for interface-based MLLMs.\\nGiven that end-to-end models are closed-source, the training details are unknown.\\nTypically, the training of interface-based MLLMs consists of two stages: 1) pre-training, 2) instruction tuning.\\nPlease describe this image in detail.\\nThe image shows a group of people standing in a park.\\nThere is a large tree in the background with pink blossoms.\\nThe people are all wearing different clothing, including a man in a brown jacket and a woman in a white shirt and jeans.\\nThere are also some people in the background who are not in the foreground of the image.\\nThe people are standing in a grassy area and there are some benches and a fence in the background.\\nThe sky is clear and there are some clouds in the distance.\\nWhat is the girl in the picture doing?\\nThe girl in the picture is standing with her arms outstretched, as if she is trying to catch something.\\nIt is not clear what she is trying to catch, but it could be a kite or a frisbee.\\nThe other people in the picture are standing around her, watching what she is doing.\\nIt appears that they are all having a good time together in the park.\\nCategory Hallucination\\nAttribute Hallucination\\nRelation Hallucination\\nFig. 3.\\nThree types of typical hallucination.\\nPre-training.\\nGiven that models from each modality are pre-trained on their respective data, the objective of this pre-training phase is to achieve cross-modal feature alignment.\\nDuring training, both the pre-trained visual encoder and LLM remain frozen, with only the cross-modal interface being trained.\\nSimilar to traditional VLMs training, as exemplified by CLIP [88], web-scale imagetext pairs [92] are utilized for training.\\nGiven that the final output is at the LLM side, the most widely used loss function in this stage is the text generation loss, typically cross-entropy loss, which aligns with the pre-training of LLMs.\\nCertain studies (e.g., [22, 66]) explore the incorporation of contrastive loss and image-text matching loss to further enhance alignment.\\nAfter training, the interface module maps the visual features into the input embedding space of the LLM.\\nInstruction Tuning.\\nSimilar to LLMs, after pre-training, the current model still lacks instruction following ability in the multimodal context.\\nDuring the instruction tuning stage, both machinegenerated datasets [75] and human-annotated QA datasets [48, 59, 80] are utilized to enhance the model’s ability to comprehend and follow multimodal instructions.\\nUnlike pre-training data, the format and quality of instruction tuning data significantly impact the model’s performance.\\nIt is usually in the format of visual content - instruction - response.\\nEmpirical studies also demonstrate that high-quality data significantly enhances the performance of MLLMs.\\nDuring this stage, there are various options for training, such as fine-tuning LLM parameters in full [75], or using techniques like LoRA [41] to tune specific LLM parameters.\\n2.3 Hallucinations in Multimodal Large Language Models\\nHallucination of MLLM generally refers to the phenomenon where the generated text response does not align with the corresponding visual content.\\nState-of-the-art studies in this field primarily focus on object hallucination, given that objects are central to research in computer vision and multimodal contexts.\\nRegarding inconsistency, two typical failure modes are: 1) missing objects, and 2) describing objects that are not present in the image or with incorrect statements.\\nEmpirically, the second mode has been shown to be less preferable to humans.\\nFor example, the LSMDC challenge [91] shows that correctness is more important to human judges than specificity.\\nIn contrast, the coverage of objects is less perceptible to humans.\\nThus, object coverage is not a primary focus in studies of object hallucination.\\nEmpirically, object hallucination can be categorized into three types: object category, object attribute, and object relation.\\nAn example of the three types of hallucination is shown in Fig. 3.\\n• Category.\\nMLLMs identify nonexistent object categories or incorrect categories in the given image.\\nFor example, in Fig. 3, \"some benches and a fence\", \"some clouds\", described in the text response do not exist in the given image.\\n• Attribute.\\nThe object categories identified by MLLMs are accurate, while the descriptions of these objects’ attributes (such as color, shape, material, content, counting, action, etc.) are wrong.\\nIn Fig. 3, \"pink blossoms\" is hallucinated by the MLLM as the color is inaccurate.\\n• Relation.\\nAll objects and their attributes are described correctly, but the relationships among them (such as human-object interactions or relative positions) do not align with the actual image content.\\nIn Fig. 3, \"standing around her, watching\" is a typical example of relation hallucination, as the objects are presented in the image but the relation is inaccurate.\\nIt’s worth noting that some literature may categorize objects counting, objects event, etc., as independent hallucination categories.\\nIn this work, we classify them under the attribute category.\\nThe definition of hallucination types aligns well with the domain of compositional generalization [79, 121] of VLMs, which investigates visio-linguistic generalization and reasoning abilities.\\n3 HALLUCINATION CAUSES\\nHallucinations have multifaceted origins, spanning the entire spectrum of MLLMs’ capability acquisition process.\\nIn this section, we delve into the root causes of hallucinations in MLLMs, primarily categorized into four aspects: Data, Model, Training, and Inference.\\n3.1 Data\\nData stands as the bedrock for MLLMs, enabling them to gain cross-modal understanding and instruction-following capabilities.\\nHowever, it can inadvertently become the source of MLLM hallucinations.\\nThis mainly manifests in three aspects: quantity, quality, and statistical bias.\\n3.1.1 Quantity\\nDeep learning models are data-hungry, especially large models like MLLMs.\\nThe amount of data plays an important role in building robust and reliable MLLMs.\\nCurrently, image-text pair datasets [92] and visual QA [48, 80] data are used for training MLLMs.\\nAlthough these datasets are usually larger than typical datasets in computer vision, they are still far less abundant than the text-only data used for training LLMs in terms of quantity.\\nInsufficient data could potentially lead to problematic cross-modal alignment, resulting in hallucinations [96, 103].\\n3.1.2 Quality\\nGiven the increasing demand for large-scale training data, heuristic data collection methods are employed to efficiently gather vast volumes of data.\\nWhile these methods provide extensive data, they offer no guarantee of quality, thereby increasing the risk of hallucinations.\\nData quality relevant to hallucinations can be further categorized into the following three facets.\\n• Noisy data.\\nAs mentioned in the definition section, training MLLMs involves two stages.\\nThe pre-training stage employs image-text pairs crawled from the web, which contain inaccurate, misaligned, or corrupted data samples.\\nThe noisy data would limit the cross-modal feature alignment [117, 120], which serves as the foundation of MLLMs.\\nAs for the instruction tuning data, prevalent methods, such as LLaVA [75], utilize the advanced GPT-4 [82] model to generate instructions.\\nHowever, ChatGPT is a language model that cannot interpret visual content, leading to the risk of noisy data.\\nMoreover, language models themselves suffer from the issue of hallucination [44], further increasing the risk.\\nLLaVA-1.5 [74] adds human annotated QA data into instruction following and shows improved results, revealing the effect of noisy data.\\n• Lack of diversity.\\nRecent works [73, 117] reveal that the diversity of data also plays a crucial role.\\nFor the data used in the two training stages, instruction tuning data are more likely to have this issue since it is usually in a relatively small amount.\\nOne prominent property is that most instruction following data samples are composed of conversations regarding the image content.\\nWe regard this type of data as positive instruction, as it always faithfully reflects the image content.\\nIn contrast, negative instruction data [73] and reject answering responses [11] are rare in the datasets.\\nGiven such training data, one potential drawback observed by recent studies [69, 73] is that current models tend to answer \"Yes\" for any instructions presented to the model, even when a proper answer should be \"No\", leading to hallucination.\\nThis phenomenon indicates the effect of data diversity.\\n• Detailed descriptions (open question) The impact of the level of detail in textual descriptions on this matter remains an open question.\\nAs discussed in Sec. 2.2, the texts in pre-training data, such as LAION [92], usually describe the salient objects’ overall content.\\nWhile the texts in the instructing tuning stage, such as LLaVA-150k [75], consist of more detailed descriptions.\\nThis LLaVA-150k dataset is generated by GPT-4 based on objects recognized by vision models.\\nOne recent work [16] argues that within the training data, detailed descriptions related to object position, attributes, and non-salient objects are usually absent.\\nThis property results in incomplete cross-modal alignment and deprives the model of grounding ability [62, 126].\\nHowever, another work [120] hypothesizes that the text descriptions in the instruction tuning data contain too much details, exceeding the perception limit of MLLMs.\\nWhen trained with such detailed data, in an attempt to fit the detail level and length distribution of ground truth captions, the model may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThe detail level of the training data remains an open question.\\n3.1.3 Statistic bias.\\nNeural networks, especially large language models, possess an intrinsic tendency to memorize training data, as noted in [23].\\nThe nous (e.g., objects) distribution in the training dataset has strong effects on the behavior of the model.\\nFrequently appeared objects and object co-occurrence are two prominent types of statistical bias, as discussed in [69, 90, 137].\\nFor example, ‘person’ might be one of the most frequently appearing objects in the training data.\\nDuring inference, even if the given image does not contain a person, the model still tends to predict the presence of a person.\\nOn the other hand, object co-occurrence refers to the phenomenon that the model will remember which two objects usually ‘go together’ [90].\\nFor instance, given an image of a kitchen with a refrigerator, MLLMs are prone to answer ‘Yes’ when asked about a microwave, as refrigerators and microwaves frequently appear together in kitchen scenes.\\nBias exists in most datasets.\\nIncreasing the scale of data may alleviate the effect, but cannot fully resolve it, given the long-tail distribution of the real world.\\n3.2 Model\\nCurrently, the architecture of popular MLLMs is composed of several components, usually including pre-trained vision model, pre-trained LLM, and alignment module as we discussed above.\\nSince these models are connected together, instead of end-to-end training from scratch, the error of each module can be accumulated.\\nInferior and problematic output from each module may lead to hallucinations.\\n• Weak vision model.\\nAs mentioned in related works [31, 90, 103], a primary potential reason for hallucination is a weak vision model, which can lead to misclassification or misinterpretation of visual concepts.\\nEven themost powerful visionmodelmay still experience information loss during the encoding process.\\nWeak vision model implies weak perception, which fundamentally undermines the multimodal understanding.\\n• Language model prior.\\nThe modern architecture of MLLMs is imbalanced.\\nUsually, the language model is much larger and stronger than the vision model, leading to a tendency to prioritize language-based information [31, 63, 64, 73, 90].\\nA typical phenomenon is that the knowledge entailed in the language model, also termed as parametric knowledge, can override the visual content.\\nFor example, given an image showing a red banana, which is counter-intuitive in the real world, an MLLM may still respond with \"yellow banana\", as \"banana is yellow\" is a deep-rooted knowledge in the LLM.\\nSuch language/knowledge prior makes the model overlook the visual content and response with hallucination.\\n• Weak alignment interface.\\nThe alignment interface plays an essential role in MLLMs, as it serves as the bridge between the two modalities.\\nA weak alignment interface can easily cause hallucinations.\\nOne potential cause of a weak alignment interface is data, as discussed in earlier sections.\\nApart from that, the interface architecture itself and training loss design also matter [52, 77, 123].\\nRecent work [52] argues that the LLaVA-like linear projection interface preserves most of the information, but lacks supervision on the projected feature.\\nVisualization in [52] reveals that the features after the projection layer remain distinct from the language embeddings.\\nThe distribution gap causes trouble in cross-modal interaction, leading to hallucination.\\nOn the other hand, Q-former-like [66] architecture has diverse supervision on the extracted visual feature, aligning it to the language embedding space.\\nHowever, the use of learnable queries inevitably results in the loss of fine-grained visual information.\\n3.3 Training\\nThe training objective of MLLMs is basically the same as LLMs, i.e, auto-regressive next token prediction loss.\\nThis loss is straightforward yet effective and easy to scale up, showing promising performance in language modeling.\\nHowever, some studies in the field of MLLMs have suggested that the next-token prediction loss might not be suitable for learning visual content due to its complex spatial structure [5, 16].\\nAdditionally, the loss optimizes at the token level, while lacking supervision at the sequence level [5].\\nAnother perspective is that, unlike training LLMs, the RLHF stage is absent in training procedure of MLLMs [96, 119], becoming a potential cause of hallucination.\\n3.4 Inference\\nAs for inference, some works also argues a potential issue in the auto-regressive generation.\\nDuring generation, as the sequence length grows, the self-attention will focus more on the previously generated text tokens, i.e., the attention on the visual content is diluted [45, 102–104].\\nThrough visualizing the attention map during generation [45, 104], it can be observed that the generated content focuses more on previous special tokens, such as punctuation, rather than visual content tokens.\\nThe issue of ’losing attention’ would also lead to the model’s output response being irrelevant to the visual content.\\n4 HALLUCINATION METRICS AND BENCHMARKS\\nIn this section, we present a comprehensive overview of existing hallucination metrics and benchmarks, which are designed to assess the extent of hallucinations generated by existing cutting-edge MLLMs.\\nCurrently, the primary focus of these benchmarks is on evaluating the object hallucination of MLLM-generated content.\\nTab.\\n1 illustrates a summary of related benchmarks.\\nCHAIR [90].\\nAs one of the early works, the metric of CHAIR was proposed to evaluate object hallucination in the traditional image captioning task.\\nThis is achieved by computing what proportion of words generated are actually in the image according to the ground truth sentences and object segmentations.\\nThe computation of the CHAIR metric is straightforward and easy to understand.\\nThe metric has two variants: per-instance (denoted as CHAIR𝑖) and per-sentence\\nTable 1.\\nSummary of most relevant benchmarks and metrics of object hallucination in MLLMs.\\nThe order is based on chronological order on arxiv.\\nIn the metric column, Acc/P/R/F1 denotes Accuracy/Precision/Recall/F1Score.\\n | Benchmark | Venue Underlying Data Source | Size | Task | Type | Metric | Hallucination | Type\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | CHAIR [90] | EMNLP’18 MSCOCO [70] | 5,000 | Gen | CHAIR | ✓ | ✗ | ✗ | ✗\\n | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | Category Attribute Relation Others\\n | POPE [69] | EMNLP’23 MSCOCO [70] | 3,000 | Dis | Acc/P/R/F1 | ✓ | ✗ | ✗ | ✗\\n | MME [113] | arXiv’23 Jun MSCOCO [70] | 1457 | Dis | Acc/Score | ✓ | ✓ | ✗ | ✓\\n | CIEM [42] | NeurIPS-W’23 MSCOCO [70] | 78120 | Dis | Acc | ✓ | ✗ | ✗ | ✗\\n | M-HalDetect [32] | arXiv’23 Aug. MSCOCO [70] | 4,000 | Dis | Reward Model Score | ✓ | ✗ | ✗ | ✗\\n | MMHal-Bench [96] arXiv’23 Sep. Open-Images [61] |  | 96 | Gen | LLM Assessment | ✓ | ✗ | ✗ | ✓\\n | GAVIE [73] | ICLR’24 Visual-Genome [59] | 1,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | NOPE [77] | arXiv’23 Oct. Open-Images [61] | 36,000 | Dis | Acc/METEOR [3] | ✓ | ✗ | ✗ | ✗\\n | HaELM [104] | arXiv’23 Oct. MSCOCO [70] | 5,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | FaithScore [55] | arXiv’23 Nov. MSCOCO [70] | 2,000 | Gen | FaithScore | ✓ | ✓ | ✓ | Obj. Counting\\n | Bingo [21] | arXiv’23 Nov. Unknown | 370 | Gen | Human Assessment | ✗ | ✗ | ✗ | Model Bias\\n | AMBER [103] | arXiv’23 Nov. Web | 15,202 | Dis & Gen | AMBER Score | ✓ | ✓ | ✓ | ✗\\n | RAH-Bench [16] | arXiv’23 Nov. MSCOCO [70] | 3,000 | Dis | False Positive Rate | ✓ | ✓ | ✓ | ✗\\n | HallusionBench [72] | CVPR’24 Unknown | 1,129 | Gen | LLM Assessment | ✗ | ✗ | ✗ | Model Diagnose\\n | CCEval [123] | arXiv’23 Dec. Visual-Genome [59] | 100 | Gen | LLM-based CHAIR | ✓ | ✗ | ✗ | ✗\\n | MERLIM [100] | arXiv’23 Dec. MSCOCO [70] | 31,373 | Dis | Accuracy | ✓ | ✗ | ✓ | Obj. Counting\\n | FGHE [105] | arXiv’23 Dec. MSCOCO [70] | 200 | Dis | Acc/P/R/F | ✓ | ✓ | ✓ | Obj. Behavior\\n | MOCHa [5] | arXiv’23 Dec. Synthetic | 2,000 | Gen | OpenCHAIR [5] | ✓ | ✓ | ✗ | ✗\\n | CorrelationQA [35] | arXiv’24 Feb. Synthetic | 7,308 | Dis | Acc/AccDrop | ✗ | ✗ | ✗ | Model Bias\\n | VQAv2-IDK [11] | arXiv’24 Feb. VQAv2 [30] | 6,624 | Dis | Acc | ✗ | ✗ | ✗ | IK [11]\\n | MHaluBench [13] | arXiv’24 Feb. MSCOCO [70] | 1,860 | Gen | Acc/P/R/F | ✓ | ✓ | ✗ | T2I\\n | VHTest [46] | arXiv’24 Feb. MSCOCO [70] | 1,200 | Dis & Gen | Acc | ✓ | ✓ | ✗ | ✓\\n | Hal-Eavl [53] | arXiv’24 Feb. MSCOCO [70] & LAION [92] | 10,000 Dis & Gen |  | Acc/P/R/F & LLM Assessment | ✓ | ✓ | ✓ | Obj. Event\\n\\n(denoted as CHAIR𝑠):\\n | CHAIR𝑖 = | |{hallucinated objects}| |{all objects mentioned}| ,\\n | CHAIR𝑠 = | |{sentences with hallucinated object}| |{all sentences}|\\n\\nn the paper of CHAIR [90], the range of objects is restricted to the 80 MSCOCO objects. \\nSentence tokenization and synonyms mapping are applied to determine whether a generated sentence contains hallucinated objects.\\n Ground-truth caption and object segmentations both serve as groundtruth objects in the computation\\n. In the MLLM era, this metric is still widely used for assessing the response of MLLM\\nPOPE [69].\\nWhen used in MLLMs, the work of [69] argues that the CHAIR metric can be affected by the instruction designs and the length of generated captions.\\nTherefore, it proposes a new evaluation metric as well as a benchmark, called Pooling-based Object Probing Evaluation (POPE).\\nThe basic idea is to convert the evaluation of hallucination into a binary classification task by prompting MLLMs with simple Yes-or-No short questions about the probing objects (e.g., Is there a car in the image?) Compared to CHAIR, POPE offers increased stability and flexibility.\\nBased on this metric design, it further proposed an evaluation benchmark, drawing 500 images from the MSCOCO dataset.\\nThe questions in the benchmark consist of both positive and negative questions.\\nThe positive questions are formed based on the ground-truth objects, while the negative questions are built from sampling nonexistent objects.\\nThe benchmark is divided into three subsets according to different negative sampling strategy: random, popular, and adversarial.\\nPopular and adversarial sampling are specifically designed to assess frequently appeared objects and object co-occurrence.\\nAs an early representative work, POPE serves as a foundation of object hallucination evaluation.\\nMME [113].\\nMME is a comprehensive evaluation benchmark for MLLMs.\\nIt covers the examination of perception and cognition abilities, encompassing 14 subtasks.\\nRegarding object hallucination, there are four popular object related subtasks in its perception evaluation, including object existence, count, position, color.\\nSimilar to POPE, these tasks are formulated as Yes-or-No tasks.\\nCIEM [42] CIEM is a benchmark to evaluate hallucination of MLLMs.\\nUnlike previous works utilize human annotated objects, CIEM is generated using an automatic pipeline.\\nThe pipeline takes the text description of a specific image as input and utilize advanced LLMs to generate QA pairs.\\nAlthough the LLM-based data generation pipeline is not completely reliable, empirical result shows that the generated data has low error rate, around 5%.\\nMMHal-Bench [96] Comprising 96 image-question pairs, ranging in 8 question categories × 12 object topics, MMHal-Bench is a dedicated benchmark for evaluating hallucination in MLLMs.\\nThe 8 question categories cover various types of hallucination, including object attributes, counting, spatial relations, etc.\\nDuring the evaluation of MMHal-Bench, the GPT-4 model is employed to analyze and rate the responses.\\nGAVIE [73]GPT4-Assisted Visual Instruction Evaluation (GAVIE) is proposed to assess the LMM output in two different aspects: Relevancy to evaluate the instruction-following performance and Accuracy to measure the visual hallucination in the LMM output.\\nIt comprises a benchmark with 1,000 samples and an evaluation approach.\\nGAVIE evaluates the output of MLLMs in an open-ended manner and does not require human-annotated ground-truth answers.\\nThe core idea is to ask the advanced GPT-4 to work as a smart teacher and score the answer by taking image content, human instruction, and model response as input.\\nNOPE [77] This paper proposes to establish a distinction between object hallucination and incorrectness.\\na) Object hallucination refers to a phenomenon in VQA where a VL model’s response includes a non-existent object, despite the ground truth answer being a negative indefinite pronoun (e.g., \"none\", \"no one\", etc).\\nThis is denoted as NegP.\\nb) Incorrectness occurs when a VL model fails to accurately respond to a question with a ground truth answer that is anything other than NegP, denoted as Others.\\nThis paper argues that the existing VQA datasets have a significantly imbalanced distribution, containing too littleNegP data.\\nTherefore, NOPE (Negative Object Presence Evaluation) is proposed in this paper to complement the absent NegP data.\\nDuring evaluation, traditional metrics, including Accuracy and METEOR, are employed.\\nHaELM [104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4 models to assess the quality of theMLLM response.\\nIn contrast, the work of Hallucination Evaluation based on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination detection.\\nIt collects a set of hallucination data generated by a wide range of MLLMs, simulates data using ChatGPT, and trains an LLM based on LLaMA [99].\\nAfter that, the HaELM model becomes proficient in hallucination evaluation, leveraging reference descriptions of images as the basis of assessment.\\nFaithScore [55] Considering the natural forms of interaction between humans and MLLMs, FaithScore aims to evaluate free-form responses to open-ended questions.\\nDifferent from LLM-based overall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate, and analyze the elements in detail.\\nSpecifically, it includes three steps: descriptive sub-sentence identification, atomic fact generation, and fact verification.\\nThe evaluation metric involves finegrained object hallucination categories, including entity, count, color, relation, and other attributes.\\nThe final computation of FaithScore is the ratio of hallucinated content.\\nBingo [21] Bingo (Bias and Interference Challenges in Visual Language Models) is a benchmark specifically designed for assessing and analyzing the limitations of current popular MLLMs, such as GPT-4V [83].\\nIt comprises 190 failure instances, along with 131 success instances as a comparison.\\nThis benchmark reveals that state-of-the-art MLLMs show the phenomenon of bias and interference.\\nBias refers to the model’s susceptibility to generating hallucinatory outputs on specific types of examples, such as OCR bias, region bias, etc.\\nInterference refers to scenarios in which the judgment model can be disrupted, making it more susceptible to hallucination.\\nDue to the small amount of data in this benchmark, the assessment and analysis are mostly conducted by humans.\\nAMBER [103] Upon the application and evaluation of MLLMs, the tasks can be roughly divided into generative tasks and discriminative tasks.\\nFor generative tasks, this paper argues that most existing works rely on additional LLMs, suffering from computational cost.\\nAs for discriminative tasks, the most popular evaluation suite is POPE [69].\\nHowever, POPE lacks fine-grained hallucination types such as attributes and relations.\\nAMBER (An LLM-free Multi-dimensional Benchmark) is proposed to support the evaluation of generative tasks and discriminative tasks, including object existence hallucination, attribute hallucination, and relation hallucination.\\nIt further combines the CHAIR [90] metric in generative tasks and F1 in discriminative tasks to form the AMBER Score as follows:\\nAMBER Score = 𝐴𝑣𝑔(1 − CHAIR, F1).\\n(1)\\nRAH-Bench [16] Relation-Associated Hallucination Benchmark (RAH-Bench) can be regarded as an upgraded version of POPE, containing 3,000 yes-or-no questions with their corresponding images.\\nDifferent from POPE, RAH-Bench further divides the negative questions into three subsets.\\nEach subset contains 500 questions with misleading statements in the different aspects, including:\\n1) categorical hallucination, 2) attribute hallucination, 3) relation hallucination.\\nHallusionBench [72] To diagnose and analyze the potential failure modes of MLLMs, HallusionBench evaluates hallucination from a different perspective.\\nIt consists of 455 visual-question control pairs, with 346 different figures and a total of 1129 questions covering diverse topics and formats.\\nThe questions are divided into two categories: Visual Dependent and Visual Supplement.\\nThe Visual Dependent questions are defined as questions that do not have an affirmative answer without the visual context.\\nThis setting aims to evaluate visual commonsense knowledge and visual reasoning skills.\\nThe Visual Supplement questions can be answered without the visual input; the visual component merely provides supplemental information or corrections.\\nThis setting is designed to evaluate visual reasoning ability and the balance between parametric memory (language prior) and image context.\\nThis division provides a new perspective for understanding and diagnosing MLLMs.\\nCCEval [123] CCEval focuses on the hallucination evaluation of detailed captions.\\nTraditional caption-based evaluation benchmarks and metrics, like CHAIR, are known to favor short captions.\\nHowever, short captions often lack detail and contain less information.\\nTo address this issue, CCEval randomly samples 100 images from Visual Genome to form a benchmark.\\nIn evaluation, GPT-4 is utilized to parse the captions generated by MLLMs and extract objects.\\nAdditionally, this work introduces the \"coverage\" metric on top of CHAIR to ensure that the captions are detailed enough.\\nThis metric computes the ratio of objects in the caption that match the ground truth to the total number of ground truth objects.\\nIt additionally records the average number of objects as well as the average length of captions as auxiliary metric.\\nCompared with CHAIR, CCEval employs more diverse objects, as reflected in the source of ground truth (Visual Genome vs. COCO) and caption parsing (GPT-4 vs. rule-based tool).\\nMERLIM [100] MERLIM (Multi-modal Evaluation benchmaRk for Large Image-language Models) is a test-bed aimed at empirically evaluating MLLMs on core computer vision tasks, including object recognition, instance counting, and identifying object-to-object relationships.\\nMERLIM contains over 279K image-question pairs, and has a strong focus on detecting cross-modal hallucinations.\\nInterestingly, when organizing the data, a set of edited images is intentionally added.\\nBased on the original image, an inpainting strategy is employed to remove one object instance in the image.\\nWith this original-edited image pair, one can compare the output of the target MLLM and identify the hallucinated objects that lack visual grounding.\\nFGHE [105] Fine-Grained Object Hallucination Evaluation (FGHE) follows a binary classification approach similar to POPE to evaluate MLLMs.\\nHowever, unlike POPE, FGHE requires a different set of binary questions to measure fine-grained hallucination.\\nThe FGHE dataset consists of 50 images and 200 binary questions divided into three categories: (a) multiple-object questions, which verify the relationships between multiple objects in the image; (b) attribute questions, which verify attributes of objects in the image; and (c) behavior questions, which verify behaviors or objects in the image.\\nThe questions are manually defined by human annotators on a subset of 50 images from the validation set of the MSCOCO dataset.\\nSimilar to POPE, the Accuracy, Precision, Recall, and F1 score are employed as the evaluation metrics.\\nOpenCHAIR [5] The traditional CHAIR metric relies on the closed list of 80 objects in the MS-COCO dataset, limiting its application.\\nTo measure object hallucination in the open-vocabulary settings, OpenCHAIR expands CHAIR by relaxing the strong reliance on the closed vocabulary.\\nThe ’open-vocabulary’ manifests in two ways.\\nFirstly, when building the benchmark, it organizes a dataset consisting of synthetic images with corresponding captions, which include diverse, openvocabulary objects using a text-to-image diffusion model.\\nSecondly, during computing the metric, CHAIR checks if words or their synonyms (as given by fixed vocabulary lists) are found in groundtruth annotations.\\nIn contrast, OpenCHAIR extracts concrete objects from a predicted caption and identifies hallucinated objects from this list by querying an LLM.\\nSimilar to CHAIR, the final metric computation is based on the hallucination rate.\\nHal-Eval [53] The work of Hal-Eval [53] identifies another type of object hallucination: event hallucination.\\nThis type of hallucination fabricates a fictional target and constructing an entire narrative around it, including its attributes, relationships, and actions.\\nThis effort further completes the definition of hallucination types.\\nIn addition, this work proposes an evaluation benchmark, which encompasses both discriminative and generative evaluation methods.\\nThis is achieved by collecting two evaluation subsets, each tailored to the discriminative and generative evaluation methods, respectively.\\nCorrelationQA [35] CorrelationQA is a dedicated benchmark to quantify the effect of hallucination induced by the spurious visual input.\\nThis type of hallucination usually occurs when providing the MLLM with images that are highly relevant but inconsistent with the answers, causing MLLMs to suffer from hallucination.\\nSuch visual inputs are defined as ’spurious visual inputs’.\\nThis benchmark reveals that most of mainstream MLLMs, including GPT-4V, suffer from hallucination when presented with such spurious visual inputs.\\nThis phenomenon indicates that an image can induce MLLMs to instinctively focus on visual content, resulting in responses that are predominantly based on visual information without proper reasoning and thinking.\\nVQAv2-IDK [11] It has been widely discussed that in the binary QA scenario, MLLMs generally have a bias on answering ’Yes-or-No,’ leading to hallucination.\\nIn a more detailed question and answer scenario, MLLMs generally tend to respond to the user’s question plausibly, even if the desired answer is ’I don’t know’.\\nThe concept is defined as ’I Know (IK)’ hallucination in the work of [11].\\nAccordingly, a new benchmark, VQAv2-IDK, is proposed to specifically evaluate this type of hallucination.\\nVQAv2-IDK is a subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators.\\nIn this benchmark, ’I Know (IK)’ hallucination has been further categorized into four types:\\n• Unanswerable: no one can know.\\n• Don’t know: human may not know, but robot might.\\n• False questions: refers non-existing.\\n• Not sure: ambiguous to answer.\\nThis benchmark opens a new track for the study of hallucination in MLLMs.\\nMHaluBench [13] This benchmark does not aim to evaluate the MLLMs themselves.\\nInstead, it is intentionally designed to evaluate the hallucination detection tools of MLLMs, i.e., judge whether a tool can successfully detect the hallucination produced by an MLLM.\\nThus, the benchmark consists of hallucinatory examples.\\nSpecifically, the benchmark unifies image-to-text tasks and the text-to-image tasks into one evaluation suite: cross-modal consistency checking.\\nThe hallucinatory examples are generated using leading MLLMs and image generation models, such as LLaVA [75], MiniGPT-4 [138], DALL-E2 [89], and DALL-E3 [6].\\nDuring evaluation, the benchmark can be used to compare different hallucination detection methods based on their performance.\\nSo far, there are not many dedicated hallucination detection methods.\\nThis work serves as a basis for this direction.\\nVHTest [46] VHTest categorizes visual properties of objects in an image into 1) individual properties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which emerge from comparisons across multiple objects, such as relative size, relative position, and counting.\\nBased on such categorization, the authors further defined 8 visual hallucination modes, providing a very detailed evaluation of hallucination in MLLMs.\\nFurthermore, the collected 1,200 evaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no question\" (YNQ).\\nSuch design enables this benchmark to evaluate both generative and discriminative tasks.\\nComparison of mainstream models We compare the mainstream MLLMs on some representative benchmarks, providing a holistic overview of their performance from different dimensions.\\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks.\\nWe observe that the MLLMs’ performance is not always consistent across different benchmarks.\\nIt indicates that different benchmarks have different evaluation dimensions and emphases.\\nTable 2.\\nComparison of mainstreamMLLMs on generative benchmarks.\\nThe numbers come from the original papers of these benchmarks.\\n | Model | LLM Size CHAIR (On AMBER) ↓ | AMBER Score ↑ | HallusionBench All-Acc ↑ | FaithScore (LLaVA-1k) ↑ | FaithScore (COCO-Cap) ↑ | Hal-Eval In-domain Gen. Acc ↑ | Hal-Eval Out-of-domain Gen. Acc ↑\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B 23.1 | 54.1 | 43.93 | 0.7167 | 0.8546 | 27.3 | 29.5\\n | Multimodal-GPT [28] | 7B - | - | - | 0.5335 | 0.5440 | - | -\\n | InstructBLIP [22] | 7B 10.3 | 86.2 | 45.26 | 0.8091 | 0.9392 | 35.5 | 41.3\\n | GPT-4V [83] | - 4.3 | 92.7 | 65.28 | - | - | - | -\\n | LLaVA (7B) [75] | 7B 13.5 | 69.3 | - | - | - | 23.3 | 26.3\\n | LLaVA (13B) [75] | 13B - | - | - | 0.8360 | 0.8729 | - | -\\n | MiniGPT-4 (7B) [138] | 7B - | - | 35.78 | 0.5713 | 0.6359 | 61.4 | 50.1\\n | MiniGPT-4 (13B) [138] | 13B 15.9 | 76.7 | - | - | - | - | -\\n | mPLUG-Owl2 [112] | 7B 10.6 | 84.0 | 47.30 | - | - | - | -\\n | LLaVA-1.5 (7B) [74] | 7B 8.6 | 82.9 | - | - | - | 44.6 | 46.4\\n | LLaVA-1.5 (13B) [74] | 13B - | - | 46.94 | 0.8566 | 0.9425 | - | -\\n | CogVLM [106] | 7B 7.9 | 86.1 | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B - | - | 39.15 | - | - | - | -\\n | Open-Flamingo [1] | 9B - | - | 38.44 | - | - | - | -\\n | LRV-Instruction [73] | - - | - | 42.78 | - | - | - | -\\n\\n5 HALLUCINATION MITIGATION\\nIn this section, we present a comprehensive review of contemporary methods aimed at mitigating hallucinations in MLLMs.\\nBased on the properties and perspectives of these methods, we systematically categorize them into four groups.\\nSpecifically, we investigate approaches addressing hallucination from Data, Model, Training, and Inference.\\nTable 3.\\nComparison of mainstream MLLMs on discriminative benchmarks.\\nThe numbers come from the original papers of these benchmarks.\\n | Model | LLM Size | MME Existence Score ↑ | MME Count Score ↑ | MME Position Score ↑ | MME Color Score ↑ | POPE Random F1-Score ↑ | POPE Random F1-Score ↑ | POPE Adversarial F1-Score ↑ | RAH-Bench F1 Score ↑ | AMBER Dis. F1-Score ↑ | AMBER Score ↑ | Hal-Eval In-domain Event. F1 ↑ | Hal-Eval Out-of-domain Event. F1 ↑\\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B | 120.00 | 50.00 | 50.00 | 55.00 | 68.06 | 66.79 | 66.82 | 69.3 | 31.2 | 54.1 | 47 | 46.6\\n | ImageBind-LLM [34] | 7B | 128.33 | 60.00 | 46.67 | 73.33 | - | - | - | - | - | - | - | -\\n | InstructBLIP [22] (7B) | 7B | - | - | - | - | - | - | - | 89.1 | 82.6 | 86.2 | 66.2 | 66.6\\n | InstructBLIP [22] (13B) | 13B | 185.00 | 143.33 | 66.67 | 153.33 | 89.29 | 83.45 | 78.45 | 84.7 | - | - | - | -\\n | VisualGLM-6B [25] | 6B | 85.00 | 50.00 | 48.33 | 55.00 | - | - | - | - | - | - | - | -\\n | Multimodal-GPT [28] | 7B | 61.67 | 55.00 | 58.33 | 68.33 | 66.68 | 66.67 | 66.67 | - | - | - | - | -\\n | PandaGPT [95] | 7B | 70.00 | 50.00 | 50.00 | 50.00 | - | - | - | - | - | - | - | -\\n | LaVIN [78] | 13B | 185.00 | 88.33 | 63.33 | 75.00 | - | - | - | - | - | - | - | -\\n | Cheetor [67] | 7B | 180.00 | 96.67 | 80.00 | 116.67 | - | - | - | - | - | - | - | -\\n | GPT-4V [83] | - | 190.00 | 160.00 | 95.00 | 150.00 | - | - | - | - | 89.6 | 92.7 | - | -\\n | LLaVA [75] (7B) | 7B | - | - | - | - | - | - | - | 73.3 | 32.0 | 69.3 | 35.1 | 14.0\\n | LLaVA [75] (13B) | 13B | 185.00 | 155.00 | 133.33 | 170.00 | 68.65 | 67.72 | 66.98 | 71.8 | - | - | - | -\\n | LRV-Instruction [73] | 7B | 165.00 | 111.67 | 86.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Lynx [122] | 7B | 195.00 | 151.67 | 90.00 | 170.00 | - | - | - | - | - | - | - | -\\n | MMICL [130] | 11B | 170.00 | 160.00 | 81.67 | 156.67 | - | - | - | - | - | - | - | -\\n | Muffin [118] | 13B | 195.00 | 163.33 | 66.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Otter [65] | 7B | 195.00 | 88.33 | 86.67 | 113.33 | - | - | - | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B | 158.33 | 150.00 | 128.33 | 170.00 | - | - | - | - | - | - | - | -\\n | SPHINX [71] | 13B | 195.00 | 160.00 | 153.33 | 160.00 | - | - | - | - | - | - | - | -\\n | VPGTrans [124] | 7B | 70.00 | 85.00 | 63.33 | 73.33 | - | - | - | - | - | - | - | -\\n | BLIVA [43] | 11B | 180.00 | 138.33 | 81.67 | 180.00 | - | - | - | - | - | - | - | -\\n | InfMLLM [135] | 13B | 195.00 | 145.00 | 170.00 | 195.00 | - | - | - | - | - | - | - | -\\n | LLaMA-Adapter V2 [26] | 7B | 185.00 | 133.33 | 56.67 | 118.33 | - | - | - | - | - | - | - | -\\n | MiniGPT-4 [138] | 13B | 68.33 | 55.00 | 43.33 | 75.00 | 78.86 | 72.21 | 71.37 | - | 69.3 | 76.7 | 48.2 | 53.0\\n | mPLUG-Owl2 [112] | 7B | 185.00 | 155.00 | 88.33 | 150.00 | - | - | - | - | 78.5 | 84.0 | - | -\\n | LLaVA-1.5 [75] | 7B | - | - | - | - | - | - | - | - | 74.4 | 82.9 | 48.9 | 34.2\\n | CogVLM [106] | 7B | 195.00 | 165.00 | 103.33 | 160.00 | - | - | - | - | 80 | 86.1 | - | -\\n\\n5.1 Data\\nAs discussed in the section on hallucination causes 3, data is one of the primary factors inducing hallucination in MLLMs.\\nFor mitigating hallucination, recent works make attempts on data, including introducing negative data [73], introducing counterfactual data [117], and reducing noise and errors in existing dataset [105, 120].\\nLRV-Instruction [73] LRV-Instruction is proposed to address the issue that existing instruction tuning data primarily focus on positive instruction samples, leading the model to consistently answer ’Yes’.\\nLRV-Instruction is designed to include both positive and negative instructions for more robust visual instruction tuning, where the negative instructions include: 1) ’Nonexistent Object Manipulation’: introducing nonexistent objects, activities, attributes, and interactions; 2) ’Existent Object Manipulation’: manipulating existent objects with inconsistent attributes; 3) ’Knowledge Manipulation’: manipulating knowledge in instructions.\\nHalluciDoctor [117] This paper addresses the object hallucination problem in MLLMs by calibrating the instruction-tuning dataset.\\nThe calibration is conducted from two perspectives.\\nFirstly, it develops a hallucination detection pipeline via consistency cross-checking of multiple MLLMs.\\nBased on the detection result, the hallucinated content can be eliminated.\\nSecondly, this work observes that long-tail distribution and object co-occurrence in the training data are two primary factors of hallucination.\\nThus, a counterfactual visual instruction generation strategy is proposed to expand the dataset.\\nUsing the proposed methods, the instruction tuning data can be balanced and experience reduced hallucination.\\nMLLMs trained on the calibrated dataset are shown to be less prone to hallucination.\\nReCaption [105] This work proposes a framework called ReCaption to rewrite the text captions of existing image-text pairs in datasets.\\nThe framework comprises two steps: 1) keyword extraction, which extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which employs an LLM to generate sentences based on the extracted keywords.\\nUltimately, the framework produces a set of high-quality image-caption pairs.\\nExperiment results show that the model trained on the rewritten caption dataset has higher accuracy on certain benchmarks, such as the POPE benchmark [69].\\nDespite the performance improvement, the question of why rewritten captions can reduce hallucination remains an open problem.\\nEOS Decision [120] Previous work [137] provides an observation that hallucination tends to occur with objects positioned later in the generated descriptions.\\nIntuitively, an ideal scenario is that the MLLM can terminate the generation process in a timely manner.\\nThis idea is thoroughly explored in the work of [120] from the perspective of end-of-sequence (EOS) decision.\\nThe key insight is that the training data may exceed the perception limit of the MLLM.\\nWhen trained with such data, the model may attempt to fit the detail level and length distribution of ground truth captions.\\nHowever, it may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThus, the authors explored approaches to enhance the model’s end-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the perception limit.\\nRegarding data, this work proposes a data filtering strategy to eliminate harmful training data that could impair the model’s ability to end sequences.\\n5.2 Model\\n5.2.1 Scale-up Resolution\\nEnhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.\\n5.2.2 Versatile Vision Encoders\\nSeveral studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.\\n5.2.3 Dedicated Module\\nFollowing our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.\\n5.3 Training\\n5.3.1 Auxiliary supervision.\\nThe primary supervision signal of training MLLMs is language modeling loss (implemented as CrossEntropyLoss) in both pre-training and finetuning stage.\\nHowever, such supervision may not be sufficient to process the rich information encoded in the visual content.\\nAccordingly, the work of [16] constructs a fine-grained vision instruction dataset based on Panoptic Scene Graph (PSG), called Relation-Associated Instruction (RAI-30k).\\nIn addition to standard dialogues, each instruction in RAI-30k is associated with a relation annotation in PSG, which includes mask annotations for related instances.\\nWith these additional annotations, it further supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [57], guiding MLLMs to focus on highly-related image content.\\nWith the additional supervision from the mask prediction loss, MLLMs are encouraged to extract features that can better represent these crucial instances, thus generating more accurate responses and mitigating vision hallucination.\\nThe intuitive idea of supervising MLLMs with grounding shows promising performance in mitigating hallucination.\\nAnother line of work analyzes the training loss from the perspective of embedding space distribution.\\nAs introduced earlier, popular MLLMs typically project the encoded vision features into the input space of a specific LLM.\\nA recent work, HACL [52], argues that an ideal projection should blend the distribution of visual and textual embeddings.\\nHowever, despite visual projection, a significant modality gap exists between textual and visual tokens, suggesting that the current learned interfaces are not effective in mapping visual representations into the textual representation space of LLMs.\\nThis issue potentially exacerbates the tendency for MLLMs to generate more hallucinations.\\nTherefore, HACL proposes enhancing the alignment between visual and textual representations through contrastive loss.\\nTexts with hallucinations are used as hard negative examples for image anchors.\\nThe loss pulls representations of non-hallucinating text and visual samples closer while pushing representations of non-hallucinating and hallucinative text apart.\\nExperiment results show that this method not only reduces hallucination but also enhances performance on other popular benchmarks.\\nRecalling the work of EOS Decision [120], to teach the model to terminate the generation process properly, this work also designs a learning objective, termed Selective EOS Supervision, in addition to the data filtering strategy.\\nThis is achieved by simply modifying the Maximum Likelihood Estimation (MLE), enabling the model to mitigate hallucination through learning from regular instruction data.\\n5.3.2 Reinforcement Learning\\nReinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.\\nAutomatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.\\nReinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.\\nA concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.\\nReinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct\\nPreference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.\\nLLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.\\nSimilarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.\\nAnother similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.\\n5.3.3 Unlearning\\nUnlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.\\n5.4 Inference\\n5.4.1 Generation Intervention.\\nContrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.\\nGuided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.\\nSimilarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.\\nHALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.\\nOthers.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.\\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.\\n5.4.2 Post-hoc Correction\\nPost-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].\\nWoodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.\\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.\\nSimilar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.\\nLogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.\\n6 CHALLENGES AND FUTURE DIRECTIONS\\nThe research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.\\n6.1 Data-centric Challenges and Innovations\\nThe reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.\\n6.2 Cross-modal Alignment and Consistency\\nThe key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.\\n6.3 Advancements in Model Architecture\\nDespite recent advancements in model architectures of LLMs and MLLMs, designing effective architectures specifically tailored to hallucination remains a challenge.\\nDeveloping advanced model architectures capable of capturing complex linguistic structures and generating coherent and contextually relevant output based on input visual content is essential for improving the performance of MLLMs.\\nFuture research can explore innovative architectural designs based on identified causes of hallucination.\\nThis includes developing stronger visual perception models, innovative cross-modal interaction modules capable of transferring cross-modal information seamlessly, and novel large language model architectures faithful to input visual content and text instructions, etc.\\n6.4 Establishing Standardized Benchmarks\\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in assessing the degree of hallucination in MLLMs.\\nIn Table 1, it can be observed that there is a variety of evaluation benchmarks, but a lack of unified standards.\\nAmong them, one of the most popular benchmarks might be POPE [69], which employs a ’Yes-or-No’ evaluation protocol.\\nHowever, this binary-QA manner does not align with how humans use MLLMs.\\nAccordingly, some benchmarks specifically evaluate the hallucination of MLLMs in the (free-form) generative context.\\nYet, they often rely on external models, such as vision expert models or other LLMs, which limits their widespread application.\\nMoving forward, future research can investigate standardized benchmarks that are theoretically sound and easy to use.\\nOtherwise, research on methods to mitigate hallucinations may be built on an incorrect foundation.\\n6.5 Reframing Hallucination as a Feature\\nRecently, discussions on social media [56] have suggested that hallucination can be regarded as an inherent feature of LLMs and MLLMs.\\nThe models are like dream machines.\\nHuman users direct their dreams with prompts.\\nThe prompts start the dream, and based on the model’s hazy recollection of its training documents, most of the time the result goes someplace useful.\\nIt’s only when the dreams enter deemed factually incorrect territory that we label them as ’hallucinations’.\\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications presents exciting opportunities for enhancing user experiences and enabling new use cases.\\nAs humans are the end-users of these models, the primary goal is to enrich human user experiences.\\nFuture research may switch the optimization objective from specific cross-modal benchmarks to human experience.\\nFor example, Some content may cause hallucinations but will not affect the user experience, while some content may.\\nAlternatively, integrating hallucination to inspire more creative ideas in real-world applications could also be intriguing.\\n6.6 Enhancing Interpretability and Trust\\nExisting methods for hallucination mitigation are primarily based on empirical observations of specific patterns, such as skipping the ‘\\\\n’ token and penalizing over-trust tokens.\\nHowever, despite the impressive improvements achieved on specific benchmarks, understanding the underlying mechanisms and decision-making processes remains challenging.\\nFuture research should focus on developing techniques for interpreting and explaining the generation process of MLLMs, thereby providing insights into the factors influencing hallucinated content.\\nThis includes investigating methods for visualizing model internals, identifying salient features and linguistic patterns, and tracing the generation process from input to output.\\nEnhancing the interpretability of MLLMs will not only improve our understanding of model behavior but also enable users to better assess hallucinated content in practical applications.\\n6.7 Navigating the Ethical Landscape\\nAs MLLMs become increasingly proficient at generating realistic text, ethical considerations surrounding the use of generated content become paramount.\\nEspecially in the context of hallucination, the generated response may contain severely concerning ethical content, amplifying the importance of the problem.\\nAddressing ethical concerns related to misinformation, bias, privacy, and societal impact is crucial for promoting responsible AI practices in the development and deployment of MLLMs.\\nIn addition to addressing typical object hallucination, future research on MLLM hallucinations should prioritize ethical considerations throughout the entire lifecycle of MLLM development, from data collection and model training to deployment and evaluation.\\n7 CONCLUSION\\nBased on powerful large language models, multimodal large language models demonstrate remarkable performance across various multimodal tasks.\\nHowever, the phenomenon of hallucination presents a significant challenge to the practical applications of MLLMs, giving rise to undeniable concerns about safety, reliability, and trustworthiness.\\nIn this comprehensive survey, we conducted a thorough examination of hallucinations within multimodal large language models, focusing on their underlying causes, evaluation metrics, benchmarks, and mitigation methods.\\nDespite considerable progress, hallucination remains a complex and persistent concern that warrants ongoing investigation.\\nThe challenge of hallucination in multimodal large language models remains compelling, requiring continuous scrutiny and innovation.\\nIn light of these challenges, we have outlined several promising future directions in this burgeoning domain.\\nThrough navigating the intricate landscape of hallucinations, we aim for this survey to serve as a foundational resource for addressing the complexities of hallucination phenomena in MLLMs.\\nWe envision this survey empowering researchers and practitioners to dedicate efforts to advancing research and developing robust solutions in this vital area of study.',\n",
       "   'title': 'INTRODUCTION',\n",
       "   'chunks': [{'text': 'Recently, the emergence of large language models (LLMs) [29, 81, 85, 99, 132] has dominated a wide range of tasks in natural language processing (NLP), achieving unprecedented progress in language understanding [39, 47], generation [128, 140] and reasoning [20, 58, 87, 107, 115].\\nLeveraging the capabilities of robust LLMs, multimodal large language models (MLLMs) [22, 75, 111, 138], sometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\\nMLLMs show promising ability in multimodal tasks, such as image captioning [66], visual question answering [22, 75], etc.\\nHowever, there is a concerning trend associated with the rapid advancement in MLLMs.\\nThese models exhibit an inclination to generate hallucinations [69, 76, 137], resulting in seemingly plausible yet factually spurious content.',\n",
       "     'title': 'INTRODUCTION',\n",
       "     'page': 1,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'The problem of hallucination originates from LLMs themselves.\\nIn the NLP community, the hallucination problem is empirically categorized into two types [44]: 1) factuality hallucination emphasizes the discrepancy between generated content and verifiable real-world facts, typically manifesting as factual inconsistency or fabrication; 2) faithfulness hallucination refers to the divergence of generated content from user instructions or the context provided by the input, as well as self-consistency within generated content.\\nIn contrast to pure LLMs, research efforts of hallucination in MLLMs mainly focus on the discrepancy between generated text response and provided visual content [69, 76, 137], i.e., cross-modal inconsistency.\\nThis difference suggests that studies in LLMs cannot be seemingly transferred to MLLMs.\\nTherefore, there is a growing need to comprehensively survey recent advancements in MLLMs’ hallucination phenomena to inspire new ideas and foster the field’s development.',\n",
       "     'title': 'INTRODUCTION',\n",
       "     'page': 1,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'In the realm of computer vision, object recognition is the core task, including sub-tasks such as object classification [60], detection [27], and segmentation [37], etc.\\nSimilarly, studies on hallucination in MLLMs primarily focus on object hallucination.\\nIn pre-MLLM era, there is a pioneering work on object hallucination in image captioning [90], evaluating object existence by comparing captions and image content.\\nIn MLLMs, object hallucination has been empirically categorized into three categories: 1) category, which identifies nonexistent or incorrect object categories in the given image; 2) attribute, which emphasizes descriptions of the objects’ attributes, such as color, shape, material, etc; and 3) relation, which assesses the relationships among objects, such as human-object interactions or relative positions.\\nNote that some literature may consider objects counting, objects event, etc., as independent hallucination categories; however, in this work, we include them into attribute category.\\nAs numerous studies exist on the underlying causes of hallucinations in LLMs, the unique challenges posed by cutting-edge MLLMs warrant an in-depth investigation.\\nOur analysis specifically targets the unique origins of hallucinations in MLLMs, spanning a spectrum of contributing factors from data, model, training, to the inference stage.\\nIn addition, we provide a comprehensive overview of benchmarks and metrics designed specifically for evaluating hallucinations in MLLMs.\\nThen, we review and discuss recent works tailored to mitigate the problem of hallucination from the viewpoints of the identified causes.',\n",
       "     'title': 'INTRODUCTION',\n",
       "     'page': 1,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Through our comprehensive survey, we aim to contribute to advancing the field of MLLMs and offer valuable insights that deepen understanding of the opportunities and challenges associated with hallucinations in MLLMs.\\nThis exploration not only enhances our understanding of the limitations of current MLLMs but also offers essential guidance for future research and the development of more robust and trustworthy MLLMs.',\n",
       "     'title': 'INTRODUCTION',\n",
       "     'page': 1,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Comparison with existing surveys.\\nIn pursuit of reliable generative AI, hallucination stands out as a major challenge, leading to a series of survey papers on its recent advancements.\\nFor pure LLMs, there are several surveys [44, 129], describing the landscape of hallucination in LLMs.\\nIn contrast, there are very few surveys on hallucination in the field of MLLMs.\\nTo the best of our knowledge, there is only one concurrent work [76], a short survey on the hallucination problem of LVLMs.\\nHowever, our survey distinguishes itself in terms of both taxonomy and scope.\\nWe present a layered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape of this field.\\nAdditionally, our approach does not limit itself to specific model architectures as prescribed in the work of [76], but rather dissects the causes of hallucinations by tracing back to various affecting factors.\\nWe cover a larger range of literature both in terms of paper number and taxonomy structure.\\nFurthermore, our mitigation strategies are intricately linked to the underlying causes, ensuring a cohesive and targeted approach.',\n",
       "     'title': 'INTRODUCTION',\n",
       "     'page': 1,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Organization of this survey.\\nIn this paper, we present a comprehensive survey of the latest developments regarding hallucinations in MLLMs.\\nThe survey is organized as follows: We begin by providing sufficient context and defining concepts related to LLMs, MLLMs, hallucination, etc.\\nNext, we delve into an in-depth analysis of the factors contributing to hallucinations in MLLMs.\\nFollowing this, we present a set of metrics and benchmarks employed for evaluating hallucinations in MLLMs.\\nWe then elaborate on a range of approaches designed to mitigate hallucinations in MLLMs.\\nFinally, we delve into the challenges and open questions that frame the current limitations and future prospects of this field, offering insights and delineating potential pathways for forthcoming research.',\n",
       "     'title': 'INTRODUCTION',\n",
       "     'page': 2,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '2 DEFINITIONS',\n",
       "     'title': 'INTRODUCTION',\n",
       "     'page': 2,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Before moving to multimodal large language models, it is essential to introduce the concept of large language models.\\nTypically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets.\\nProminent examples include GPT-3 [8], PaLM [18], LLaMA [99], and GPT-4 [82].\\nThrough scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning[8], Chain-of-Thought prompting[107] and instruction following[86], among others.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models',\n",
       "     'page': 2,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'The characteristics and behaviors of LLMs are intricately linked to their training processes.\\nLLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF).\\nBelow, we provide a concise overview of each stage to facilitate comprehension.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models',\n",
       "     'page': 2,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Pre-trianing.\\nPre-training serves as a fundamental phase in the learning process of LLMs [134].\\nDuring this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence.\\nBy undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities.\\nThis pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models',\n",
       "     'page': 2,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Supervised Fine-Tuning.\\nAlthough pre-training equips LLMs with substantial knowledge and skills, it’s important to acknowledge that its primary focus is on optimizing for completion.\\nConsequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user’s objective of obtaining desired responses.\\nTo address this disparity, the concept of Supervised FineTuning (SFT) [125] has been introduced.\\nSFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models',\n",
       "     'page': 2,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Reinforcement Learning from Human Feedback.\\nAlthough SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.\\nAmong the various methods, Reinforcement Learning from Human Feedback',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models',\n",
       "     'page': 2,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': ' | Language Model | Parametric Knowledge | e.g. | VCD [64], Volcano [63]\\n | --- | --- | --- | ---\\n | Cross-modal Interface | Inferior Alignment | e.g. | HACL [52], Halle-Switch [123]\\n | Sequence Supervision | e.g. MOCHa [5], OPERA [45] |  | \\n | Visual Supervision | e.g. Chen et al. [16] |  | \\n | Human Feedback | e.g. RLHF-V [119] |  | \\n | Lose Visual Attention | e.g. OPERA [45], HaELM [104] |  | \\n | CHAIR | CHAIR [90] |  | \\n | POPE | POPE [69] |  | \\n | LLM-based | e.g. GAVIE [73], HaELM [104], HallusionBench [72] |  | \\n | Others | e.g. Faith-Score [55], AMBER [103] |  | \\n | Discriminative Task | e.g. POPE [69], RAH-Bench [16], FGHE [105] |  | \\n | Generative Task | e.g. GAVIE [73], Faith-Score [55] |  | \\n | Introducing | Negative Data | e.g. | LRV-Instruction [73]\\n | Introducing | Counterfactual Data | e.g. | HalluciDoctor [117]\\n | Mitigating Noises and Errors | e.g. ReCaption [105], EOS [120] |  | \\n | Scale-up Resolution | e.g. LLaVA-1.5 [74], InternVL [14], HallE-Switch [123] |  | \\n | Versatile | Vision Encoders | e.g. | VCoder [49], IVE [38]\\n | Dedicated Module | e.g. HallE-Switch [123] |  | \\n | Auxiliary Supervision\\n |  | Noisy Data | e.g. | HalluciDoctor [117], LLaVA-1.5 [74]\\n |  | Lack of Diversity | e.g. | LRV-Instruction [73], HalluciDoctor [117]\\n |  | Detailed descriptions (open question) | e.g. | Chen et al. [16], EOS [120]\\n |  | Frequent Objects | e.g. | POPE [69]\\n |  | Objects Occurrence | e.g. | LURE [137], VCD [64]\\n |  | Information Loss | e.g. | HallusionBench [72], AMBER [103]\\n |  | Feature Bias | e.g. | Tong et al. [98]\\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Vision Model',\n",
       "     'page': 3,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': ' | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Metrics and Benchmarks(§4) > Hallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)',\n",
       "     'page': 3,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Fig. 1.\\nThe main content flow and categorization of this survey.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 3,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 3,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': ' | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 4,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Fig. 2.\\nPopular architecture of multimodal large language model.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 4,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'MLLMs [22, 75, 111, 138] typically refers to a series of models that enable LLMs to perceive and comprehend data from various modalities.\\nAmong them, vision+LLM is particularly prominent, owing to the extensive research on vision-language models (VLMs) [51, 88, 116] prior to LLMs.\\nAs a result, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models (LVLMs).\\nThe goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\" the world via images or videos.\\nCombined with strong reasoning and language generation abilities, MLLMs trigger a series of downstream tasks in multimodal domains, such as image/video captioning and visual question answering.\\nAdditionally, MLLMs serve as the foundation for applications in other fields, such as AI assistants, embodied agents, and robotics.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 4,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Integrating the two modalities of vision and language involves primarily two types of approaches.\\nThe first line of work is built upon off-the-shelf pre-trained uni-modal models.\\nSpecifically, these MLLMs usually incorporate a learnable interface between pre-trained visual encoders and LLMs.\\nThe interface extracts and integrates information from visual modalities.\\nSuch interfaces can be further categorized into 1) learnable query-based and 2) projection layer based.\\nLearnable querybased methods, exemplified by Q-Former [66], as used in MiniGPT-4 [138] and Instruct-BLIP [22], utilize a set of learnable query tokens to capture visual signals via cross-attention.\\nProjection layerbased methods, as widely applied in LLaVA [75], Shikra [12], etc., involve training a linear projection layer or a Multi-Layer Perceptron (MLP) module to transform extracted visual features.\\nBoth types of interfaces aim to transform pre-trained visual features into the input space of pre-trained LLMs.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 4,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Another line of work is represented by Fuyu-8B [4] and Gemini [97].\\nUnlike previous methods that leverage pre-trained uni-modal models, these works employ end-to-end training from scratch.\\nTaking Fuyu-8B as an example, it does not employ any pre-trained vision encoder.\\nInstead, it directly inputs image patches and employs a linear projection to transform the raw pixels of each patch into embeddings.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 4,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'The abstracted pipeline is depicted in Fig. 2.\\nMLLMs take input from both visual and textual modalities, learning from multimodal instructions and responses, which leads to remarkable performance across various multimodal tasks.\\nRegarding the training of MLLMs, we provide a concise overview of the training process for interface-based MLLMs.\\nGiven that end-to-end models are closed-source, the training details are unknown.\\nTypically, the training of interface-based MLLMs consists of two stages: 1) pre-training, 2) instruction tuning.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 4,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Please describe this image in detail.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'The image shows a group of people standing in a park.\\nThere is a large tree in the background with pink blossoms.\\nThe people are all wearing different clothing, including a man in a brown jacket and a woman in a white shirt and jeans.\\nThere are also some people in the background who are not in the foreground of the image.\\nThe people are standing in a grassy area and there are some benches and a fence in the background.\\nThe sky is clear and there are some clouds in the distance.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'What is the girl in the picture doing?\\nThe girl in the picture is standing with her arms outstretched, as if she is trying to catch something.\\nIt is not clear what she is trying to catch, but it could be a kite or a frisbee.\\nThe other people in the picture are standing around her, watching what she is doing.\\nIt appears that they are all having a good time together in the park.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Fig. 3.\\nThree types of typical hallucination.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Pre-training.\\nGiven that models from each modality are pre-trained on their respective data, the objective of this pre-training phase is to achieve cross-modal feature alignment.\\nDuring training, both the pre-trained visual encoder and LLM remain frozen, with only the cross-modal interface being trained.\\nSimilar to traditional VLMs training, as exemplified by CLIP [88], web-scale imagetext pairs [92] are utilized for training.\\nGiven that the final output is at the LLM side, the most widely used loss function in this stage is the text generation loss, typically cross-entropy loss, which aligns with the pre-training of LLMs.\\nCertain studies (e.g., [22, 66]) explore the incorporation of contrastive loss and image-text matching loss to further enhance alignment.\\nAfter training, the interface module maps the visual features into the input embedding space of the LLM.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Instruction Tuning.\\nSimilar to LLMs, after pre-training, the current model still lacks instruction following ability in the multimodal context.\\nDuring the instruction tuning stage, both machinegenerated datasets [75] and human-annotated QA datasets [48, 59, 80] are utilized to enhance the model’s ability to comprehend and follow multimodal instructions.\\nUnlike pre-training data, the format and quality of instruction tuning data significantly impact the model’s performance.\\nIt is usually in the format of visual content - instruction - response.\\nEmpirical studies also demonstrate that high-quality data significantly enhances the performance of MLLMs.\\nDuring this stage, there are various options for training, such as fine-tuning LLM parameters in full [75], or using techniques like LoRA [41] to tune specific LLM parameters.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Hallucination of MLLM generally refers to the phenomenon where the generated text response does not align with the corresponding visual content.\\nState-of-the-art studies in this field primarily focus on object hallucination, given that objects are central to research in computer vision and multimodal contexts.\\nRegarding inconsistency, two typical failure modes are: 1) missing objects, and 2) describing objects that are not present in the image or with incorrect statements.\\nEmpirically, the second mode has been shown to be less preferable to humans.\\nFor example, the LSMDC challenge [91] shows that correctness is more important to human judges than specificity.\\nIn contrast, the coverage of objects is less perceptible to humans.\\nThus, object coverage is not a primary focus in studies of object hallucination.\\nEmpirically, object hallucination can be categorized into three types: object category, object attribute, and object relation.\\nAn example of the three types of hallucination is shown in Fig. 3.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Category.\\nMLLMs identify nonexistent object categories or incorrect categories in the given image.\\nFor example, in Fig. 3, \"some benches and a fence\", \"some clouds\", described in the text response do not exist in the given image.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Attribute.\\nThe object categories identified by MLLMs are accurate, while the descriptions of these objects’ attributes (such as color, shape, material, content, counting, action, etc.) are wrong.\\nIn Fig. 3, \"pink blossoms\" is hallucinated by the MLLM as the color is inaccurate.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 6,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Relation.\\nAll objects and their attributes are described correctly, but the relationships among them (such as human-object interactions or relative positions) do not align with the actual image content.\\nIn Fig. 3, \"standing around her, watching\" is a typical example of relation hallucination, as the objects are presented in the image but the relation is inaccurate.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 6,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'It’s worth noting that some literature may categorize objects counting, objects event, etc., as independent hallucination categories.\\nIn this work, we classify them under the attribute category.\\nThe definition of hallucination types aligns well with the domain of compositional generalization [79, 121] of VLMs, which investigates visio-linguistic generalization and reasoning abilities.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 6,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '3 HALLUCINATION CAUSES',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 6,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Hallucinations have multifaceted origins, spanning the entire spectrum of MLLMs’ capability acquisition process.\\nIn this section, we delve into the root causes of hallucinations in MLLMs, primarily categorized into four aspects: Data, Model, Training, and Inference.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 6,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Data stands as the bedrock for MLLMs, enabling them to gain cross-modal understanding and instruction-following capabilities.\\nHowever, it can inadvertently become the source of MLLM hallucinations.\\nThis mainly manifests in three aspects: quantity, quality, and statistical bias.',\n",
       "     'title': 'INTRODUCTION > 3.1 Data',\n",
       "     'page': 6,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Deep learning models are data-hungry, especially large models like MLLMs.\\nThe amount of data plays an important role in building robust and reliable MLLMs.\\nCurrently, image-text pair datasets [92] and visual QA [48, 80] data are used for training MLLMs.\\nAlthough these datasets are usually larger than typical datasets in computer vision, they are still far less abundant than the text-only data used for training LLMs in terms of quantity.\\nInsufficient data could potentially lead to problematic cross-modal alignment, resulting in hallucinations [96, 103].',\n",
       "     'title': 'INTRODUCTION > 3.1.1 Quantity',\n",
       "     'page': 6,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Given the increasing demand for large-scale training data, heuristic data collection methods are employed to efficiently gather vast volumes of data.\\nWhile these methods provide extensive data, they offer no guarantee of quality, thereby increasing the risk of hallucinations.\\nData quality relevant to hallucinations can be further categorized into the following three facets.',\n",
       "     'title': 'INTRODUCTION > 3.1.2 Quality',\n",
       "     'page': 6,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Noisy data.\\nAs mentioned in the definition section, training MLLMs involves two stages.\\nThe pre-training stage employs image-text pairs crawled from the web, which contain inaccurate, misaligned, or corrupted data samples.\\nThe noisy data would limit the cross-modal feature alignment [117, 120], which serves as the foundation of MLLMs.\\nAs for the instruction tuning data, prevalent methods, such as LLaVA [75], utilize the advanced GPT-4 [82] model to generate instructions.\\nHowever, ChatGPT is a language model that cannot interpret visual content, leading to the risk of noisy data.\\nMoreover, language models themselves suffer from the issue of hallucination [44], further increasing the risk.\\nLLaVA-1.5 [74] adds human annotated QA data into instruction following and shows improved results, revealing the effect of noisy data.',\n",
       "     'title': 'INTRODUCTION > 3.1.2 Quality',\n",
       "     'page': 6,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Lack of diversity.\\nRecent works [73, 117] reveal that the diversity of data also plays a crucial role.\\nFor the data used in the two training stages, instruction tuning data are more likely to have this issue since it is usually in a relatively small amount.\\nOne prominent property is that most instruction following data samples are composed of conversations regarding the image content.\\nWe regard this type of data as positive instruction, as it always faithfully reflects the image content.\\nIn contrast, negative instruction data [73] and reject answering responses [11] are rare in the datasets.\\nGiven such training data, one potential drawback observed by recent studies [69, 73] is that current models tend to answer \"Yes\" for any instructions presented to the model, even when a proper answer should be \"No\", leading to hallucination.\\nThis phenomenon indicates the effect of data diversity.',\n",
       "     'title': 'INTRODUCTION > 3.1.2 Quality',\n",
       "     'page': 6,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Detailed descriptions (open question) The impact of the level of detail in textual descriptions on this matter remains an open question.\\nAs discussed in Sec. 2.2, the texts in pre-training data, such as LAION [92], usually describe the salient objects’ overall content.\\nWhile the texts in the instructing tuning stage, such as LLaVA-150k [75], consist of more detailed descriptions.\\nThis LLaVA-150k dataset is generated by GPT-4 based on objects recognized by vision models.\\nOne recent work [16] argues that within the training data, detailed descriptions related to object position, attributes, and non-salient objects are usually absent.\\nThis property results in incomplete cross-modal alignment and deprives the model of grounding ability [62, 126].\\nHowever, another work [120] hypothesizes that the text descriptions in the instruction tuning data contain too much details, exceeding the perception limit of MLLMs.\\nWhen trained with such detailed data, in an attempt to fit the detail level and length distribution of ground truth captions, the model may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThe detail level of the training data remains an open question.',\n",
       "     'title': 'INTRODUCTION > 3.1.2 Quality',\n",
       "     'page': 7,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '3.1.3 Statistic bias.\\nNeural networks, especially large language models, possess an intrinsic tendency to memorize training data, as noted in [23].\\nThe nous (e.g., objects) distribution in the training dataset has strong effects on the behavior of the model.\\nFrequently appeared objects and object co-occurrence are two prominent types of statistical bias, as discussed in [69, 90, 137].\\nFor example, ‘person’ might be one of the most frequently appearing objects in the training data.\\nDuring inference, even if the given image does not contain a person, the model still tends to predict the presence of a person.\\nOn the other hand, object co-occurrence refers to the phenomenon that the model will remember which two objects usually ‘go together’ [90].\\nFor instance, given an image of a kitchen with a refrigerator, MLLMs are prone to answer ‘Yes’ when asked about a microwave, as refrigerators and microwaves frequently appear together in kitchen scenes.\\nBias exists in most datasets.\\nIncreasing the scale of data may alleviate the effect, but cannot fully resolve it, given the long-tail distribution of the real world.',\n",
       "     'title': 'INTRODUCTION > 3.1.2 Quality',\n",
       "     'page': 7,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Currently, the architecture of popular MLLMs is composed of several components, usually including pre-trained vision model, pre-trained LLM, and alignment module as we discussed above.\\nSince these models are connected together, instead of end-to-end training from scratch, the error of each module can be accumulated.\\nInferior and problematic output from each module may lead to hallucinations.',\n",
       "     'title': 'INTRODUCTION > 3.2 Model',\n",
       "     'page': 7,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Weak vision model.\\nAs mentioned in related works [31, 90, 103], a primary potential reason for hallucination is a weak vision model, which can lead to misclassification or misinterpretation of visual concepts.\\nEven themost powerful visionmodelmay still experience information loss during the encoding process.\\nWeak vision model implies weak perception, which fundamentally undermines the multimodal understanding.',\n",
       "     'title': 'INTRODUCTION > 3.2 Model',\n",
       "     'page': 7,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Language model prior.\\nThe modern architecture of MLLMs is imbalanced.\\nUsually, the language model is much larger and stronger than the vision model, leading to a tendency to prioritize language-based information [31, 63, 64, 73, 90].\\nA typical phenomenon is that the knowledge entailed in the language model, also termed as parametric knowledge, can override the visual content.\\nFor example, given an image showing a red banana, which is counter-intuitive in the real world, an MLLM may still respond with \"yellow banana\", as \"banana is yellow\" is a deep-rooted knowledge in the LLM.\\nSuch language/knowledge prior makes the model overlook the visual content and response with hallucination.',\n",
       "     'title': 'INTRODUCTION > 3.2 Model',\n",
       "     'page': 7,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Weak alignment interface.\\nThe alignment interface plays an essential role in MLLMs, as it serves as the bridge between the two modalities.\\nA weak alignment interface can easily cause hallucinations.\\nOne potential cause of a weak alignment interface is data, as discussed in earlier sections.\\nApart from that, the interface architecture itself and training loss design also matter [52, 77, 123].\\nRecent work [52] argues that the LLaVA-like linear projection interface preserves most of the information, but lacks supervision on the projected feature.\\nVisualization in [52] reveals that the features after the projection layer remain distinct from the language embeddings.\\nThe distribution gap causes trouble in cross-modal interaction, leading to hallucination.\\nOn the other hand, Q-former-like [66] architecture has diverse supervision on the extracted visual feature, aligning it to the language embedding space.\\nHowever, the use of learnable queries inevitably results in the loss of fine-grained visual information.',\n",
       "     'title': 'INTRODUCTION > 3.2 Model',\n",
       "     'page': 8,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'The training objective of MLLMs is basically the same as LLMs, i.e, auto-regressive next token prediction loss.\\nThis loss is straightforward yet effective and easy to scale up, showing promising performance in language modeling.\\nHowever, some studies in the field of MLLMs have suggested that the next-token prediction loss might not be suitable for learning visual content due to its complex spatial structure [5, 16].\\nAdditionally, the loss optimizes at the token level, while lacking supervision at the sequence level [5].\\nAnother perspective is that, unlike training LLMs, the RLHF stage is absent in training procedure of MLLMs [96, 119], becoming a potential cause of hallucination.',\n",
       "     'title': 'INTRODUCTION > 3.3 Training',\n",
       "     'page': 8,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'As for inference, some works also argues a potential issue in the auto-regressive generation.\\nDuring generation, as the sequence length grows, the self-attention will focus more on the previously generated text tokens, i.e., the attention on the visual content is diluted [45, 102–104].\\nThrough visualizing the attention map during generation [45, 104], it can be observed that the generated content focuses more on previous special tokens, such as punctuation, rather than visual content tokens.\\nThe issue of ’losing attention’ would also lead to the model’s output response being irrelevant to the visual content.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 8,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '4 HALLUCINATION METRICS AND BENCHMARKS',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 8,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'In this section, we present a comprehensive overview of existing hallucination metrics and benchmarks, which are designed to assess the extent of hallucinations generated by existing cutting-edge MLLMs.\\nCurrently, the primary focus of these benchmarks is on evaluating the object hallucination of MLLM-generated content.\\nTab.\\n1 illustrates a summary of related benchmarks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 8,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'CHAIR [90].\\nAs one of the early works, the metric of CHAIR was proposed to evaluate object hallucination in the traditional image captioning task.\\nThis is achieved by computing what proportion of words generated are actually in the image according to the ground truth sentences and object segmentations.\\nThe computation of the CHAIR metric is straightforward and easy to understand.\\nThe metric has two variants: per-instance (denoted as CHAIR𝑖) and per-sentence',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 8,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Table 1.\\nSummary of most relevant benchmarks and metrics of object hallucination in MLLMs.\\nThe order is based on chronological order on arxiv.\\nIn the metric column, Acc/P/R/F1 denotes Accuracy/Precision/Recall/F1Score.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': ' | Benchmark | Venue Underlying Data Source | Size | Task | Type | Metric | Hallucination | Type\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | CHAIR [90] | EMNLP’18 MSCOCO [70] | 5,000 | Gen | CHAIR | ✓ | ✗ | ✗ | ✗\\n | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | Category Attribute Relation Others\\n | POPE [69] | EMNLP’23 MSCOCO [70] | 3,000 | Dis | Acc/P/R/F1 | ✓ | ✗ | ✗ | ✗\\n | MME [113] | arXiv’23 Jun MSCOCO [70] | 1457 | Dis | Acc/Score | ✓ | ✓ | ✗ | ✓\\n | CIEM [42] | NeurIPS-W’23 MSCOCO [70] | 78120 | Dis | Acc | ✓ | ✗ | ✗ | ✗\\n | M-HalDetect [32] | arXiv’23 Aug. MSCOCO [70] | 4,000 | Dis | Reward Model Score | ✓ | ✗ | ✗ | ✗\\n | MMHal-Bench [96] arXiv’23 Sep. Open-Images [61] |  | 96 | Gen | LLM Assessment | ✓ | ✗ | ✗ | ✓\\n | GAVIE [73] | ICLR’24 Visual-Genome [59] | 1,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | NOPE [77] | arXiv’23 Oct. Open-Images [61] | 36,000 | Dis | Acc/METEOR [3] | ✓ | ✗ | ✗ | ✗\\n | HaELM [104] | arXiv’23 Oct. MSCOCO [70] | 5,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | FaithScore [55] | arXiv’23 Nov. MSCOCO [70] | 2,000 | Gen | FaithScore | ✓ | ✓ | ✓ | Obj. Counting\\n | Bingo [21] | arXiv’23 Nov. Unknown | 370 | Gen | Human Assessment | ✗ | ✗ | ✗ | Model Bias\\n | AMBER [103] | arXiv’23 Nov. Web | 15,202 | Dis & Gen | AMBER Score | ✓ | ✓ | ✓ | ✗\\n | RAH-Bench [16] | arXiv’23 Nov. MSCOCO [70] | 3,000 | Dis | False Positive Rate | ✓ | ✓ | ✓ | ✗\\n | HallusionBench [72] | CVPR’24 Unknown | 1,129 | Gen | LLM Assessment | ✗ | ✗ | ✗ | Model Diagnose\\n | CCEval [123] | arXiv’23 Dec. Visual-Genome [59] | 100 | Gen | LLM-based CHAIR | ✓ | ✗ | ✗ | ✗\\n | MERLIM [100] | arXiv’23 Dec. MSCOCO [70] | 31,373 | Dis | Accuracy | ✓ | ✗ | ✓ | Obj. Counting\\n | FGHE [105] | arXiv’23 Dec. MSCOCO [70] | 200 | Dis | Acc/P/R/F | ✓ | ✓ | ✓ | Obj. Behavior\\n | MOCHa [5] | arXiv’23 Dec. Synthetic | 2,000 | Gen | OpenCHAIR [5] | ✓ | ✓ | ✗ | ✗\\n | CorrelationQA [35] | arXiv’24 Feb. Synthetic | 7,308 | Dis | Acc/AccDrop | ✗ | ✗ | ✗ | Model Bias\\n | VQAv2-IDK [11] | arXiv’24 Feb. VQAv2 [30] | 6,624 | Dis | Acc | ✗ | ✗ | ✗ | IK [11]\\n | MHaluBench [13] | arXiv’24 Feb. MSCOCO [70] | 1,860 | Gen | Acc/P/R/F | ✓ | ✓ | ✗ | T2I\\n | VHTest [46] | arXiv’24 Feb. MSCOCO [70] | 1,200 | Dis & Gen | Acc | ✓ | ✓ | ✗ | ✓\\n | Hal-Eavl [53] | arXiv’24 Feb. MSCOCO [70] & LAION [92] | 10,000 Dis & Gen |  | Acc/P/R/F & LLM Assessment | ✓ | ✓ | ✓ | Obj. Event\\n',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '(denoted as CHAIR𝑠):',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': ' | CHAIR𝑖 = | |{hallucinated objects}| |{all objects mentioned}| ,\\n | CHAIR𝑠 = | |{sentences with hallucinated object}| |{all sentences}|\\n',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'n the paper of CHAIR [90], the range of objects is restricted to the 80 MSCOCO objects. \\nSentence tokenization and synonyms mapping are applied to determine whether a generated sentence contains hallucinated objects.\\n Ground-truth caption and object segmentations both serve as groundtruth objects in the computation\\n. In the MLLM era, this metric is still widely used for assessing the response of MLLM',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'POPE [69].\\nWhen used in MLLMs, the work of [69] argues that the CHAIR metric can be affected by the instruction designs and the length of generated captions.\\nTherefore, it proposes a new evaluation metric as well as a benchmark, called Pooling-based Object Probing Evaluation (POPE).\\nThe basic idea is to convert the evaluation of hallucination into a binary classification task by prompting MLLMs with simple Yes-or-No short questions about the probing objects (e.g., Is there a car in the image?) Compared to CHAIR, POPE offers increased stability and flexibility.\\nBased on this metric design, it further proposed an evaluation benchmark, drawing 500 images from the MSCOCO dataset.\\nThe questions in the benchmark consist of both positive and negative questions.\\nThe positive questions are formed based on the ground-truth objects, while the negative questions are built from sampling nonexistent objects.\\nThe benchmark is divided into three subsets according to different negative sampling strategy: random, popular, and adversarial.\\nPopular and adversarial sampling are specifically designed to assess frequently appeared objects and object co-occurrence.\\nAs an early representative work, POPE serves as a foundation of object hallucination evaluation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'MME [113].\\nMME is a comprehensive evaluation benchmark for MLLMs.\\nIt covers the examination of perception and cognition abilities, encompassing 14 subtasks.\\nRegarding object hallucination, there are four popular object related subtasks in its perception evaluation, including object existence, count, position, color.\\nSimilar to POPE, these tasks are formulated as Yes-or-No tasks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'CIEM [42] CIEM is a benchmark to evaluate hallucination of MLLMs.\\nUnlike previous works utilize human annotated objects, CIEM is generated using an automatic pipeline.\\nThe pipeline takes the text description of a specific image as input and utilize advanced LLMs to generate QA pairs.\\nAlthough the LLM-based data generation pipeline is not completely reliable, empirical result shows that the generated data has low error rate, around 5%.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'MMHal-Bench [96] Comprising 96 image-question pairs, ranging in 8 question categories × 12 object topics, MMHal-Bench is a dedicated benchmark for evaluating hallucination in MLLMs.\\nThe 8 question categories cover various types of hallucination, including object attributes, counting, spatial relations, etc.\\nDuring the evaluation of MMHal-Bench, the GPT-4 model is employed to analyze and rate the responses.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'GAVIE [73]GPT4-Assisted Visual Instruction Evaluation (GAVIE) is proposed to assess the LMM output in two different aspects: Relevancy to evaluate the instruction-following performance and Accuracy to measure the visual hallucination in the LMM output.\\nIt comprises a benchmark with 1,000 samples and an evaluation approach.\\nGAVIE evaluates the output of MLLMs in an open-ended manner and does not require human-annotated ground-truth answers.\\nThe core idea is to ask the advanced GPT-4 to work as a smart teacher and score the answer by taking image content, human instruction, and model response as input.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'NOPE [77] This paper proposes to establish a distinction between object hallucination and incorrectness.\\na) Object hallucination refers to a phenomenon in VQA where a VL model’s response includes a non-existent object, despite the ground truth answer being a negative indefinite pronoun (e.g., \"none\", \"no one\", etc).\\nThis is denoted as NegP.\\nb) Incorrectness occurs when a VL model fails to accurately respond to a question with a ground truth answer that is anything other than NegP, denoted as Others.\\nThis paper argues that the existing VQA datasets have a significantly imbalanced distribution, containing too littleNegP data.\\nTherefore, NOPE (Negative Object Presence Evaluation) is proposed in this paper to complement the absent NegP data.\\nDuring evaluation, traditional metrics, including Accuracy and METEOR, are employed.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'HaELM [104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4 models to assess the quality of theMLLM response.\\nIn contrast, the work of Hallucination Evaluation based on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination detection.\\nIt collects a set of hallucination data generated by a wide range of MLLMs, simulates data using ChatGPT, and trains an LLM based on LLaMA [99].\\nAfter that, the HaELM model becomes proficient in hallucination evaluation, leveraging reference descriptions of images as the basis of assessment.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'FaithScore [55] Considering the natural forms of interaction between humans and MLLMs, FaithScore aims to evaluate free-form responses to open-ended questions.\\nDifferent from LLM-based overall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate, and analyze the elements in detail.\\nSpecifically, it includes three steps: descriptive sub-sentence identification, atomic fact generation, and fact verification.\\nThe evaluation metric involves finegrained object hallucination categories, including entity, count, color, relation, and other attributes.\\nThe final computation of FaithScore is the ratio of hallucinated content.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Bingo [21] Bingo (Bias and Interference Challenges in Visual Language Models) is a benchmark specifically designed for assessing and analyzing the limitations of current popular MLLMs, such as GPT-4V [83].\\nIt comprises 190 failure instances, along with 131 success instances as a comparison.\\nThis benchmark reveals that state-of-the-art MLLMs show the phenomenon of bias and interference.\\nBias refers to the model’s susceptibility to generating hallucinatory outputs on specific types of examples, such as OCR bias, region bias, etc.\\nInterference refers to scenarios in which the judgment model can be disrupted, making it more susceptible to hallucination.\\nDue to the small amount of data in this benchmark, the assessment and analysis are mostly conducted by humans.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'AMBER [103] Upon the application and evaluation of MLLMs, the tasks can be roughly divided into generative tasks and discriminative tasks.\\nFor generative tasks, this paper argues that most existing works rely on additional LLMs, suffering from computational cost.\\nAs for discriminative tasks, the most popular evaluation suite is POPE [69].\\nHowever, POPE lacks fine-grained hallucination types such as attributes and relations.\\nAMBER (An LLM-free Multi-dimensional Benchmark) is proposed to support the evaluation of generative tasks and discriminative tasks, including object existence hallucination, attribute hallucination, and relation hallucination.\\nIt further combines the CHAIR [90] metric in generative tasks and F1 in discriminative tasks to form the AMBER Score as follows:',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'AMBER Score = 𝐴𝑣𝑔(1 − CHAIR, F1).\\n(1)',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'RAH-Bench [16] Relation-Associated Hallucination Benchmark (RAH-Bench) can be regarded as an upgraded version of POPE, containing 3,000 yes-or-no questions with their corresponding images.\\nDifferent from POPE, RAH-Bench further divides the negative questions into three subsets.\\nEach subset contains 500 questions with misleading statements in the different aspects, including:',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '1) categorical hallucination, 2) attribute hallucination, 3) relation hallucination.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'HallusionBench [72] To diagnose and analyze the potential failure modes of MLLMs, HallusionBench evaluates hallucination from a different perspective.\\nIt consists of 455 visual-question control pairs, with 346 different figures and a total of 1129 questions covering diverse topics and formats.\\nThe questions are divided into two categories: Visual Dependent and Visual Supplement.\\nThe Visual Dependent questions are defined as questions that do not have an affirmative answer without the visual context.\\nThis setting aims to evaluate visual commonsense knowledge and visual reasoning skills.\\nThe Visual Supplement questions can be answered without the visual input; the visual component merely provides supplemental information or corrections.\\nThis setting is designed to evaluate visual reasoning ability and the balance between parametric memory (language prior) and image context.\\nThis division provides a new perspective for understanding and diagnosing MLLMs.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'CCEval [123] CCEval focuses on the hallucination evaluation of detailed captions.\\nTraditional caption-based evaluation benchmarks and metrics, like CHAIR, are known to favor short captions.\\nHowever, short captions often lack detail and contain less information.\\nTo address this issue, CCEval randomly samples 100 images from Visual Genome to form a benchmark.\\nIn evaluation, GPT-4 is utilized to parse the captions generated by MLLMs and extract objects.\\nAdditionally, this work introduces the \"coverage\" metric on top of CHAIR to ensure that the captions are detailed enough.\\nThis metric computes the ratio of objects in the caption that match the ground truth to the total number of ground truth objects.\\nIt additionally records the average number of objects as well as the average length of captions as auxiliary metric.\\nCompared with CHAIR, CCEval employs more diverse objects, as reflected in the source of ground truth (Visual Genome vs. COCO) and caption parsing (GPT-4 vs. rule-based tool).',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'MERLIM [100] MERLIM (Multi-modal Evaluation benchmaRk for Large Image-language Models) is a test-bed aimed at empirically evaluating MLLMs on core computer vision tasks, including object recognition, instance counting, and identifying object-to-object relationships.\\nMERLIM contains over 279K image-question pairs, and has a strong focus on detecting cross-modal hallucinations.\\nInterestingly, when organizing the data, a set of edited images is intentionally added.\\nBased on the original image, an inpainting strategy is employed to remove one object instance in the image.\\nWith this original-edited image pair, one can compare the output of the target MLLM and identify the hallucinated objects that lack visual grounding.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'FGHE [105] Fine-Grained Object Hallucination Evaluation (FGHE) follows a binary classification approach similar to POPE to evaluate MLLMs.\\nHowever, unlike POPE, FGHE requires a different set of binary questions to measure fine-grained hallucination.\\nThe FGHE dataset consists of 50 images and 200 binary questions divided into three categories: (a) multiple-object questions, which verify the relationships between multiple objects in the image; (b) attribute questions, which verify attributes of objects in the image; and (c) behavior questions, which verify behaviors or objects in the image.\\nThe questions are manually defined by human annotators on a subset of 50 images from the validation set of the MSCOCO dataset.\\nSimilar to POPE, the Accuracy, Precision, Recall, and F1 score are employed as the evaluation metrics.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'OpenCHAIR [5] The traditional CHAIR metric relies on the closed list of 80 objects in the MS-COCO dataset, limiting its application.\\nTo measure object hallucination in the open-vocabulary settings, OpenCHAIR expands CHAIR by relaxing the strong reliance on the closed vocabulary.\\nThe ’open-vocabulary’ manifests in two ways.\\nFirstly, when building the benchmark, it organizes a dataset consisting of synthetic images with corresponding captions, which include diverse, openvocabulary objects using a text-to-image diffusion model.\\nSecondly, during computing the metric, CHAIR checks if words or their synonyms (as given by fixed vocabulary lists) are found in groundtruth annotations.\\nIn contrast, OpenCHAIR extracts concrete objects from a predicted caption and identifies hallucinated objects from this list by querying an LLM.\\nSimilar to CHAIR, the final metric computation is based on the hallucination rate.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Hal-Eval [53] The work of Hal-Eval [53] identifies another type of object hallucination: event hallucination.\\nThis type of hallucination fabricates a fictional target and constructing an entire narrative around it, including its attributes, relationships, and actions.\\nThis effort further completes the definition of hallucination types.\\nIn addition, this work proposes an evaluation benchmark, which encompasses both discriminative and generative evaluation methods.\\nThis is achieved by collecting two evaluation subsets, each tailored to the discriminative and generative evaluation methods, respectively.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'CorrelationQA [35] CorrelationQA is a dedicated benchmark to quantify the effect of hallucination induced by the spurious visual input.\\nThis type of hallucination usually occurs when providing the MLLM with images that are highly relevant but inconsistent with the answers, causing MLLMs to suffer from hallucination.\\nSuch visual inputs are defined as ’spurious visual inputs’.\\nThis benchmark reveals that most of mainstream MLLMs, including GPT-4V, suffer from hallucination when presented with such spurious visual inputs.\\nThis phenomenon indicates that an image can induce MLLMs to instinctively focus on visual content, resulting in responses that are predominantly based on visual information without proper reasoning and thinking.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'VQAv2-IDK [11] It has been widely discussed that in the binary QA scenario, MLLMs generally have a bias on answering ’Yes-or-No,’ leading to hallucination.\\nIn a more detailed question and answer scenario, MLLMs generally tend to respond to the user’s question plausibly, even if the desired answer is ’I don’t know’.\\nThe concept is defined as ’I Know (IK)’ hallucination in the work of [11].\\nAccordingly, a new benchmark, VQAv2-IDK, is proposed to specifically evaluate this type of hallucination.\\nVQAv2-IDK is a subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators.\\nIn this benchmark, ’I Know (IK)’ hallucination has been further categorized into four types:',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Unanswerable: no one can know.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Don’t know: human may not know, but robot might.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• False questions: refers non-existing.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '• Not sure: ambiguous to answer.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'This benchmark opens a new track for the study of hallucination in MLLMs.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'MHaluBench [13] This benchmark does not aim to evaluate the MLLMs themselves.\\nInstead, it is intentionally designed to evaluate the hallucination detection tools of MLLMs, i.e., judge whether a tool can successfully detect the hallucination produced by an MLLM.\\nThus, the benchmark consists of hallucinatory examples.\\nSpecifically, the benchmark unifies image-to-text tasks and the text-to-image tasks into one evaluation suite: cross-modal consistency checking.\\nThe hallucinatory examples are generated using leading MLLMs and image generation models, such as LLaVA [75], MiniGPT-4 [138], DALL-E2 [89], and DALL-E3 [6].\\nDuring evaluation, the benchmark can be used to compare different hallucination detection methods based on their performance.\\nSo far, there are not many dedicated hallucination detection methods.\\nThis work serves as a basis for this direction.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'VHTest [46] VHTest categorizes visual properties of objects in an image into 1) individual properties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which emerge from comparisons across multiple objects, such as relative size, relative position, and counting.\\nBased on such categorization, the authors further defined 8 visual hallucination modes, providing a very detailed evaluation of hallucination in MLLMs.\\nFurthermore, the collected 1,200 evaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no question\" (YNQ).\\nSuch design enables this benchmark to evaluate both generative and discriminative tasks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Comparison of mainstream models We compare the mainstream MLLMs on some representative benchmarks, providing a holistic overview of their performance from different dimensions.\\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks.\\nWe observe that the MLLMs’ performance is not always consistent across different benchmarks.\\nIt indicates that different benchmarks have different evaluation dimensions and emphases.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Table 2.\\nComparison of mainstreamMLLMs on generative benchmarks.\\nThe numbers come from the original papers of these benchmarks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': ' | Model | LLM Size CHAIR (On AMBER) ↓ | AMBER Score ↑ | HallusionBench All-Acc ↑ | FaithScore (LLaVA-1k) ↑ | FaithScore (COCO-Cap) ↑ | Hal-Eval In-domain Gen. Acc ↑ | Hal-Eval Out-of-domain Gen. Acc ↑\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B 23.1 | 54.1 | 43.93 | 0.7167 | 0.8546 | 27.3 | 29.5\\n | Multimodal-GPT [28] | 7B - | - | - | 0.5335 | 0.5440 | - | -\\n | InstructBLIP [22] | 7B 10.3 | 86.2 | 45.26 | 0.8091 | 0.9392 | 35.5 | 41.3\\n | GPT-4V [83] | - 4.3 | 92.7 | 65.28 | - | - | - | -\\n | LLaVA (7B) [75] | 7B 13.5 | 69.3 | - | - | - | 23.3 | 26.3\\n | LLaVA (13B) [75] | 13B - | - | - | 0.8360 | 0.8729 | - | -\\n | MiniGPT-4 (7B) [138] | 7B - | - | 35.78 | 0.5713 | 0.6359 | 61.4 | 50.1\\n | MiniGPT-4 (13B) [138] | 13B 15.9 | 76.7 | - | - | - | - | -\\n | mPLUG-Owl2 [112] | 7B 10.6 | 84.0 | 47.30 | - | - | - | -\\n | LLaVA-1.5 (7B) [74] | 7B 8.6 | 82.9 | - | - | - | 44.6 | 46.4\\n | LLaVA-1.5 (13B) [74] | 13B - | - | 46.94 | 0.8566 | 0.9425 | - | -\\n | CogVLM [106] | 7B 7.9 | 86.1 | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B - | - | 39.15 | - | - | - | -\\n | Open-Flamingo [1] | 9B - | - | 38.44 | - | - | - | -\\n | LRV-Instruction [73] | - - | - | 42.78 | - | - | - | -\\n',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '5 HALLUCINATION MITIGATION',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'In this section, we present a comprehensive review of contemporary methods aimed at mitigating hallucinations in MLLMs.\\nBased on the properties and perspectives of these methods, we systematically categorize them into four groups.\\nSpecifically, we investigate approaches addressing hallucination from Data, Model, Training, and Inference.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Table 3.\\nComparison of mainstream MLLMs on discriminative benchmarks.\\nThe numbers come from the original papers of these benchmarks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 14,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': ' | Model | LLM Size | MME Existence Score ↑ | MME Count Score ↑ | MME Position Score ↑ | MME Color Score ↑ | POPE Random F1-Score ↑ | POPE Random F1-Score ↑ | POPE Adversarial F1-Score ↑ | RAH-Bench F1 Score ↑ | AMBER Dis. F1-Score ↑ | AMBER Score ↑ | Hal-Eval In-domain Event. F1 ↑ | Hal-Eval Out-of-domain Event. F1 ↑\\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B | 120.00 | 50.00 | 50.00 | 55.00 | 68.06 | 66.79 | 66.82 | 69.3 | 31.2 | 54.1 | 47 | 46.6\\n | ImageBind-LLM [34] | 7B | 128.33 | 60.00 | 46.67 | 73.33 | - | - | - | - | - | - | - | -\\n | InstructBLIP [22] (7B) | 7B | - | - | - | - | - | - | - | 89.1 | 82.6 | 86.2 | 66.2 | 66.6\\n | InstructBLIP [22] (13B) | 13B | 185.00 | 143.33 | 66.67 | 153.33 | 89.29 | 83.45 | 78.45 | 84.7 | - | - | - | -\\n | VisualGLM-6B [25] | 6B | 85.00 | 50.00 | 48.33 | 55.00 | - | - | - | - | - | - | - | -\\n | Multimodal-GPT [28] | 7B | 61.67 | 55.00 | 58.33 | 68.33 | 66.68 | 66.67 | 66.67 | - | - | - | - | -\\n | PandaGPT [95] | 7B | 70.00 | 50.00 | 50.00 | 50.00 | - | - | - | - | - | - | - | -\\n | LaVIN [78] | 13B | 185.00 | 88.33 | 63.33 | 75.00 | - | - | - | - | - | - | - | -\\n | Cheetor [67] | 7B | 180.00 | 96.67 | 80.00 | 116.67 | - | - | - | - | - | - | - | -\\n | GPT-4V [83] | - | 190.00 | 160.00 | 95.00 | 150.00 | - | - | - | - | 89.6 | 92.7 | - | -\\n | LLaVA [75] (7B) | 7B | - | - | - | - | - | - | - | 73.3 | 32.0 | 69.3 | 35.1 | 14.0\\n | LLaVA [75] (13B) | 13B | 185.00 | 155.00 | 133.33 | 170.00 | 68.65 | 67.72 | 66.98 | 71.8 | - | - | - | -\\n | LRV-Instruction [73] | 7B | 165.00 | 111.67 | 86.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Lynx [122] | 7B | 195.00 | 151.67 | 90.00 | 170.00 | - | - | - | - | - | - | - | -\\n | MMICL [130] | 11B | 170.00 | 160.00 | 81.67 | 156.67 | - | - | - | - | - | - | - | -\\n | Muffin [118] | 13B | 195.00 | 163.33 | 66.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Otter [65] | 7B | 195.00 | 88.33 | 86.67 | 113.33 | - | - | - | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B | 158.33 | 150.00 | 128.33 | 170.00 | - | - | - | - | - | - | - | -\\n | SPHINX [71] | 13B | 195.00 | 160.00 | 153.33 | 160.00 | - | - | - | - | - | - | - | -\\n | VPGTrans [124] | 7B | 70.00 | 85.00 | 63.33 | 73.33 | - | - | - | - | - | - | - | -\\n | BLIVA [43] | 11B | 180.00 | 138.33 | 81.67 | 180.00 | - | - | - | - | - | - | - | -\\n | InfMLLM [135] | 13B | 195.00 | 145.00 | 170.00 | 195.00 | - | - | - | - | - | - | - | -\\n | LLaMA-Adapter V2 [26] | 7B | 185.00 | 133.33 | 56.67 | 118.33 | - | - | - | - | - | - | - | -\\n | MiniGPT-4 [138] | 13B | 68.33 | 55.00 | 43.33 | 75.00 | 78.86 | 72.21 | 71.37 | - | 69.3 | 76.7 | 48.2 | 53.0\\n | mPLUG-Owl2 [112] | 7B | 185.00 | 155.00 | 88.33 | 150.00 | - | - | - | - | 78.5 | 84.0 | - | -\\n | LLaVA-1.5 [75] | 7B | - | - | - | - | - | - | - | - | 74.4 | 82.9 | 48.9 | 34.2\\n | CogVLM [106] | 7B | 195.00 | 165.00 | 103.33 | 160.00 | - | - | - | - | 80 | 86.1 | - | -\\n',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 14,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'As discussed in the section on hallucination causes 3, data is one of the primary factors inducing hallucination in MLLMs.\\nFor mitigating hallucination, recent works make attempts on data, including introducing negative data [73], introducing counterfactual data [117], and reducing noise and errors in existing dataset [105, 120].',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'LRV-Instruction [73] LRV-Instruction is proposed to address the issue that existing instruction tuning data primarily focus on positive instruction samples, leading the model to consistently answer ’Yes’.\\nLRV-Instruction is designed to include both positive and negative instructions for more robust visual instruction tuning, where the negative instructions include: 1) ’Nonexistent Object Manipulation’: introducing nonexistent objects, activities, attributes, and interactions; 2) ’Existent Object Manipulation’: manipulating existent objects with inconsistent attributes; 3) ’Knowledge Manipulation’: manipulating knowledge in instructions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'HalluciDoctor [117] This paper addresses the object hallucination problem in MLLMs by calibrating the instruction-tuning dataset.\\nThe calibration is conducted from two perspectives.\\nFirstly, it develops a hallucination detection pipeline via consistency cross-checking of multiple MLLMs.\\nBased on the detection result, the hallucinated content can be eliminated.\\nSecondly, this work observes that long-tail distribution and object co-occurrence in the training data are two primary factors of hallucination.\\nThus, a counterfactual visual instruction generation strategy is proposed to expand the dataset.\\nUsing the proposed methods, the instruction tuning data can be balanced and experience reduced hallucination.\\nMLLMs trained on the calibrated dataset are shown to be less prone to hallucination.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'ReCaption [105] This work proposes a framework called ReCaption to rewrite the text captions of existing image-text pairs in datasets.\\nThe framework comprises two steps: 1) keyword extraction, which extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which employs an LLM to generate sentences based on the extracted keywords.\\nUltimately, the framework produces a set of high-quality image-caption pairs.\\nExperiment results show that the model trained on the rewritten caption dataset has higher accuracy on certain benchmarks, such as the POPE benchmark [69].\\nDespite the performance improvement, the question of why rewritten captions can reduce hallucination remains an open problem.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'EOS Decision [120] Previous work [137] provides an observation that hallucination tends to occur with objects positioned later in the generated descriptions.\\nIntuitively, an ideal scenario is that the MLLM can terminate the generation process in a timely manner.\\nThis idea is thoroughly explored in the work of [120] from the perspective of end-of-sequence (EOS) decision.\\nThe key insight is that the training data may exceed the perception limit of the MLLM.\\nWhen trained with such data, the model may attempt to fit the detail level and length distribution of ground truth captions.\\nHowever, it may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThus, the authors explored approaches to enhance the model’s end-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the perception limit.\\nRegarding data, this work proposes a data filtering strategy to eliminate harmful training data that could impair the model’s ability to end sequences.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 15,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Enhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.1 Scale-up Resolution',\n",
       "     'page': 15,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Several studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.2 Versatile Vision Encoders',\n",
       "     'page': 15,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Following our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.3 Dedicated Module',\n",
       "     'page': 16,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '5.3.1 Auxiliary supervision.\\nThe primary supervision signal of training MLLMs is language modeling loss (implemented as CrossEntropyLoss) in both pre-training and finetuning stage.\\nHowever, such supervision may not be sufficient to process the rich information encoded in the visual content.\\nAccordingly, the work of [16] constructs a fine-grained vision instruction dataset based on Panoptic Scene Graph (PSG), called Relation-Associated Instruction (RAI-30k).\\nIn addition to standard dialogues, each instruction in RAI-30k is associated with a relation annotation in PSG, which includes mask annotations for related instances.\\nWith these additional annotations, it further supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [57], guiding MLLMs to focus on highly-related image content.\\nWith the additional supervision from the mask prediction loss, MLLMs are encouraged to extract features that can better represent these crucial instances, thus generating more accurate responses and mitigating vision hallucination.\\nThe intuitive idea of supervising MLLMs with grounding shows promising performance in mitigating hallucination.\\nAnother line of work analyzes the training loss from the perspective of embedding space distribution.\\nAs introduced earlier, popular MLLMs typically project the encoded vision features into the input space of a specific LLM.\\nA recent work, HACL [52], argues that an ideal projection should blend the distribution of visual and textual embeddings.\\nHowever, despite visual projection, a significant modality gap exists between textual and visual tokens, suggesting that the current learned interfaces are not effective in mapping visual representations into the textual representation space of LLMs.\\nThis issue potentially exacerbates the tendency for MLLMs to generate more hallucinations.\\nTherefore, HACL proposes enhancing the alignment between visual and textual representations through contrastive loss.\\nTexts with hallucinations are used as hard negative examples for image anchors.\\nThe loss pulls representations of non-hallucinating text and visual samples closer while pushing representations of non-hallucinating and hallucinative text apart.\\nExperiment results show that this method not only reduces hallucination but also enhances performance on other popular benchmarks.\\nRecalling the work of EOS Decision [120], to teach the model to terminate the generation process properly, this work also designs a learning objective, termed Selective EOS Supervision, in addition to the data filtering strategy.\\nThis is achieved by simply modifying the Maximum Likelihood Estimation (MLE), enabling the model to mitigate hallucination through learning from regular instruction data.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training',\n",
       "     'page': 16,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Reinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 16,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Automatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Reinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'A concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'A more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Reinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Preference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'LLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Similarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Another similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Unlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.3 Unlearning',\n",
       "     'page': 18,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Contrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 18,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Guided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Similarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'HALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Others.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Another interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Post-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Woodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Another line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Similar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'LogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '6 CHALLENGES AND FUTURE DIRECTIONS',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'The research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '6.1 Data-centric Challenges and Innovations',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'The reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '6.2 Cross-modal Alignment and Consistency',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 21,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'The key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 21,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Despite recent advancements in model architectures of LLMs and MLLMs, designing effective architectures specifically tailored to hallucination remains a challenge.\\nDeveloping advanced model architectures capable of capturing complex linguistic structures and generating coherent and contextually relevant output based on input visual content is essential for improving the performance of MLLMs.\\nFuture research can explore innovative architectural designs based on identified causes of hallucination.\\nThis includes developing stronger visual perception models, innovative cross-modal interaction modules capable of transferring cross-modal information seamlessly, and novel large language model architectures faithful to input visual content and text instructions, etc.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.3 Advancements in Model Architecture',\n",
       "     'page': 21,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'The lack of standardized benchmarks and evaluation metrics poses significant challenges in assessing the degree of hallucination in MLLMs.\\nIn Table 1, it can be observed that there is a variety of evaluation benchmarks, but a lack of unified standards.\\nAmong them, one of the most popular benchmarks might be POPE [69], which employs a ’Yes-or-No’ evaluation protocol.\\nHowever, this binary-QA manner does not align with how humans use MLLMs.\\nAccordingly, some benchmarks specifically evaluate the hallucination of MLLMs in the (free-form) generative context.\\nYet, they often rely on external models, such as vision expert models or other LLMs, which limits their widespread application.\\nMoving forward, future research can investigate standardized benchmarks that are theoretically sound and easy to use.\\nOtherwise, research on methods to mitigate hallucinations may be built on an incorrect foundation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.4 Establishing Standardized Benchmarks',\n",
       "     'page': 21,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Recently, discussions on social media [56] have suggested that hallucination can be regarded as an inherent feature of LLMs and MLLMs.\\nThe models are like dream machines.\\nHuman users direct their dreams with prompts.\\nThe prompts start the dream, and based on the model’s hazy recollection of its training documents, most of the time the result goes someplace useful.\\nIt’s only when the dreams enter deemed factually incorrect territory that we label them as ’hallucinations’.\\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications presents exciting opportunities for enhancing user experiences and enabling new use cases.\\nAs humans are the end-users of these models, the primary goal is to enrich human user experiences.\\nFuture research may switch the optimization objective from specific cross-modal benchmarks to human experience.\\nFor example, Some content may cause hallucinations but will not affect the user experience, while some content may.\\nAlternatively, integrating hallucination to inspire more creative ideas in real-world applications could also be intriguing.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.5 Reframing Hallucination as a Feature',\n",
       "     'page': 21,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Existing methods for hallucination mitigation are primarily based on empirical observations of specific patterns, such as skipping the ‘\\\\n’ token and penalizing over-trust tokens.\\nHowever, despite the impressive improvements achieved on specific benchmarks, understanding the underlying mechanisms and decision-making processes remains challenging.\\nFuture research should focus on developing techniques for interpreting and explaining the generation process of MLLMs, thereby providing insights into the factors influencing hallucinated content.\\nThis includes investigating methods for visualizing model internals, identifying salient features and linguistic patterns, and tracing the generation process from input to output.\\nEnhancing the interpretability of MLLMs will not only improve our understanding of model behavior but also enable users to better assess hallucinated content in practical applications.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.6 Enhancing Interpretability and Trust',\n",
       "     'page': 22,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'As MLLMs become increasingly proficient at generating realistic text, ethical considerations surrounding the use of generated content become paramount.\\nEspecially in the context of hallucination, the generated response may contain severely concerning ethical content, amplifying the importance of the problem.\\nAddressing ethical concerns related to misinformation, bias, privacy, and societal impact is crucial for promoting responsible AI practices in the development and deployment of MLLMs.\\nIn addition to addressing typical object hallucination, future research on MLLM hallucinations should prioritize ethical considerations throughout the entire lifecycle of MLLM development, from data collection and model training to deployment and evaluation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.7 Navigating the Ethical Landscape',\n",
       "     'page': 22,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': '7 CONCLUSION',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.7 Navigating the Ethical Landscape',\n",
       "     'page': 22,\n",
       "     'source_doc': 'INTRODUCTION'},\n",
       "    {'text': 'Based on powerful large language models, multimodal large language models demonstrate remarkable performance across various multimodal tasks.\\nHowever, the phenomenon of hallucination presents a significant challenge to the practical applications of MLLMs, giving rise to undeniable concerns about safety, reliability, and trustworthiness.\\nIn this comprehensive survey, we conducted a thorough examination of hallucinations within multimodal large language models, focusing on their underlying causes, evaluation metrics, benchmarks, and mitigation methods.\\nDespite considerable progress, hallucination remains a complex and persistent concern that warrants ongoing investigation.\\nThe challenge of hallucination in multimodal large language models remains compelling, requiring continuous scrutiny and innovation.\\nIn light of these challenges, we have outlined several promising future directions in this burgeoning domain.\\nThrough navigating the intricate landscape of hallucinations, we aim for this survey to serve as a foundational resource for addressing the complexities of hallucination phenomena in MLLMs.\\nWe envision this survey empowering researchers and practitioners to dedicate efforts to advancing research and developing robust solutions in this vital area of study.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.7 Navigating the Ethical Landscape',\n",
       "     'page': 22,\n",
       "     'source_doc': 'INTRODUCTION'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edb11e70>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edb13c40>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2ef7608e0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2ef7615d0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2ef780640>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2ef780f70>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2ef782e00>]},\n",
       "  {'text': '2.1 Large Language Models\\nBefore moving to multimodal large language models, it is essential to introduce the concept of large language models.\\nTypically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets.\\nProminent examples include GPT-3 [8], PaLM [18], LLaMA [99], and GPT-4 [82].\\nThrough scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning[8], Chain-of-Thought prompting[107] and instruction following[86], among others.\\nThe characteristics and behaviors of LLMs are intricately linked to their training processes.\\nLLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF).\\nBelow, we provide a concise overview of each stage to facilitate comprehension.\\nPre-trianing.\\nPre-training serves as a fundamental phase in the learning process of LLMs [134].\\nDuring this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence.\\nBy undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities.\\nThis pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively.\\nSupervised Fine-Tuning.\\nAlthough pre-training equips LLMs with substantial knowledge and skills, it’s important to acknowledge that its primary focus is on optimizing for completion.\\nConsequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user’s objective of obtaining desired responses.\\nTo address this disparity, the concept of Supervised FineTuning (SFT) [125] has been introduced.\\nSFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs.\\nReinforcement Learning from Human Feedback.\\nAlthough SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.\\nAmong the various methods, Reinforcement Learning from Human Feedback\\nData Quantity Insufficient Data e.g. AMBER [103], LLaVA-RLHF [96]\\nData Quality\\nStatistic Bias\\nVision Model\\n | Language Model | Parametric Knowledge | e.g. | VCD [64], Volcano [63]\\n | --- | --- | --- | ---\\n | Cross-modal Interface | Inferior Alignment | e.g. | HACL [52], Halle-Switch [123]\\n | Sequence Supervision | e.g. MOCHa [5], OPERA [45] |  | \\n | Visual Supervision | e.g. Chen et al. [16] |  | \\n | Human Feedback | e.g. RLHF-V [119] |  | \\n | Lose Visual Attention | e.g. OPERA [45], HaELM [104] |  | \\n | CHAIR | CHAIR [90] |  | \\n | POPE | POPE [69] |  | \\n | LLM-based | e.g. GAVIE [73], HaELM [104], HallusionBench [72] |  | \\n | Others | e.g. Faith-Score [55], AMBER [103] |  | \\n | Discriminative Task | e.g. POPE [69], RAH-Bench [16], FGHE [105] |  | \\n | Generative Task | e.g. GAVIE [73], Faith-Score [55] |  | \\n | Introducing | Negative Data | e.g. | LRV-Instruction [73]\\n | Introducing | Counterfactual Data | e.g. | HalluciDoctor [117]\\n | Mitigating Noises and Errors | e.g. ReCaption [105], EOS [120] |  | \\n | Scale-up Resolution | e.g. LLaVA-1.5 [74], InternVL [14], HallE-Switch [123] |  | \\n | Versatile | Vision Encoders | e.g. | VCoder [49], IVE [38]\\n | Dedicated Module | e.g. HallE-Switch [123] |  | \\n | Auxiliary Supervision\\n |  | Noisy Data | e.g. | HalluciDoctor [117], LLaVA-1.5 [74]\\n |  | Lack of Diversity | e.g. | LRV-Instruction [73], HalluciDoctor [117]\\n |  | Detailed descriptions (open question) | e.g. | Chen et al. [16], EOS [120]\\n |  | Frequent Objects | e.g. | POPE [69]\\n |  | Objects Occurrence | e.g. | LURE [137], VCD [64]\\n |  | Information Loss | e.g. | HallusionBench [72], AMBER [103]\\n |  | Feature Bias | e.g. | Tong et al. [98]\\n\\nfrom Data (§3.1) Hallucination from Model (§3.2)\\nHallucination Causes (§3)\\nHallucination Metrics and Benchmarks(§4)\\nHallucination from Training (§3.3) Hallucination from Inference (§3.4)\\nHallucination Metrics\\nHallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)\\n | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n\\nHallucination Mitigation (§5)\\nReinforcement Learning\\nGeneration Intervention\\nPost-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]\\nMitigating Inference-related Hallucinations (§5.4)\\nFig. 1.\\nThe main content flow and categorization of this survey.\\n(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.\\n | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n\\nFig. 2.\\nPopular architecture of multimodal large language model.',\n",
       "   'title': '2.1 Large Language Models',\n",
       "   'chunks': [{'text': 'Before moving to multimodal large language models, it is essential to introduce the concept of large language models.\\nTypically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets.\\nProminent examples include GPT-3 [8], PaLM [18], LLaMA [99], and GPT-4 [82].\\nThrough scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning[8], Chain-of-Thought prompting[107] and instruction following[86], among others.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models',\n",
       "     'page': 2,\n",
       "     'source_doc': '2.1 Large Language Models'},\n",
       "    {'text': 'The characteristics and behaviors of LLMs are intricately linked to their training processes.\\nLLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF).\\nBelow, we provide a concise overview of each stage to facilitate comprehension.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models',\n",
       "     'page': 2,\n",
       "     'source_doc': '2.1 Large Language Models'},\n",
       "    {'text': 'Pre-trianing.\\nPre-training serves as a fundamental phase in the learning process of LLMs [134].\\nDuring this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence.\\nBy undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities.\\nThis pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models',\n",
       "     'page': 2,\n",
       "     'source_doc': '2.1 Large Language Models'},\n",
       "    {'text': 'Supervised Fine-Tuning.\\nAlthough pre-training equips LLMs with substantial knowledge and skills, it’s important to acknowledge that its primary focus is on optimizing for completion.\\nConsequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user’s objective of obtaining desired responses.\\nTo address this disparity, the concept of Supervised FineTuning (SFT) [125] has been introduced.\\nSFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models',\n",
       "     'page': 2,\n",
       "     'source_doc': '2.1 Large Language Models'},\n",
       "    {'text': 'Reinforcement Learning from Human Feedback.\\nAlthough SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.\\nAmong the various methods, Reinforcement Learning from Human Feedback',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models',\n",
       "     'page': 2,\n",
       "     'source_doc': '2.1 Large Language Models'},\n",
       "    {'text': ' | Language Model | Parametric Knowledge | e.g. | VCD [64], Volcano [63]\\n | --- | --- | --- | ---\\n | Cross-modal Interface | Inferior Alignment | e.g. | HACL [52], Halle-Switch [123]\\n | Sequence Supervision | e.g. MOCHa [5], OPERA [45] |  | \\n | Visual Supervision | e.g. Chen et al. [16] |  | \\n | Human Feedback | e.g. RLHF-V [119] |  | \\n | Lose Visual Attention | e.g. OPERA [45], HaELM [104] |  | \\n | CHAIR | CHAIR [90] |  | \\n | POPE | POPE [69] |  | \\n | LLM-based | e.g. GAVIE [73], HaELM [104], HallusionBench [72] |  | \\n | Others | e.g. Faith-Score [55], AMBER [103] |  | \\n | Discriminative Task | e.g. POPE [69], RAH-Bench [16], FGHE [105] |  | \\n | Generative Task | e.g. GAVIE [73], Faith-Score [55] |  | \\n | Introducing | Negative Data | e.g. | LRV-Instruction [73]\\n | Introducing | Counterfactual Data | e.g. | HalluciDoctor [117]\\n | Mitigating Noises and Errors | e.g. ReCaption [105], EOS [120] |  | \\n | Scale-up Resolution | e.g. LLaVA-1.5 [74], InternVL [14], HallE-Switch [123] |  | \\n | Versatile | Vision Encoders | e.g. | VCoder [49], IVE [38]\\n | Dedicated Module | e.g. HallE-Switch [123] |  | \\n | Auxiliary Supervision\\n |  | Noisy Data | e.g. | HalluciDoctor [117], LLaVA-1.5 [74]\\n |  | Lack of Diversity | e.g. | LRV-Instruction [73], HalluciDoctor [117]\\n |  | Detailed descriptions (open question) | e.g. | Chen et al. [16], EOS [120]\\n |  | Frequent Objects | e.g. | POPE [69]\\n |  | Objects Occurrence | e.g. | LURE [137], VCD [64]\\n |  | Information Loss | e.g. | HallusionBench [72], AMBER [103]\\n |  | Feature Bias | e.g. | Tong et al. [98]\\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Vision Model',\n",
       "     'page': 3,\n",
       "     'source_doc': '2.1 Large Language Models'},\n",
       "    {'text': ' | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Metrics and Benchmarks(§4) > Hallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)',\n",
       "     'page': 3,\n",
       "     'source_doc': '2.1 Large Language Models'},\n",
       "    {'text': 'Fig. 1.\\nThe main content flow and categorization of this survey.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 3,\n",
       "     'source_doc': '2.1 Large Language Models'},\n",
       "    {'text': '(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 3,\n",
       "     'source_doc': '2.1 Large Language Models'},\n",
       "    {'text': ' | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 4,\n",
       "     'source_doc': '2.1 Large Language Models'},\n",
       "    {'text': 'Fig. 2.\\nPopular architecture of multimodal large language model.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 4,\n",
       "     'source_doc': '2.1 Large Language Models'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edb11e70>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2edb13c40>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2ef7608e0>]},\n",
       "  {'text': 'Data Quantity Insufficient Data e.g. AMBER [103], LLaVA-RLHF [96]',\n",
       "   'title': 'Data Quantity Insufficient Data e.g. AMBER [103], LLaVA-RLHF [96]',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Data Quality',\n",
       "   'title': 'Data Quality',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Statistic Bias',\n",
       "   'title': 'Statistic Bias',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Vision Model\\n | Language Model | Parametric Knowledge | e.g. | VCD [64], Volcano [63]\\n | --- | --- | --- | ---\\n | Cross-modal Interface | Inferior Alignment | e.g. | HACL [52], Halle-Switch [123]\\n | Sequence Supervision | e.g. MOCHa [5], OPERA [45] |  | \\n | Visual Supervision | e.g. Chen et al. [16] |  | \\n | Human Feedback | e.g. RLHF-V [119] |  | \\n | Lose Visual Attention | e.g. OPERA [45], HaELM [104] |  | \\n | CHAIR | CHAIR [90] |  | \\n | POPE | POPE [69] |  | \\n | LLM-based | e.g. GAVIE [73], HaELM [104], HallusionBench [72] |  | \\n | Others | e.g. Faith-Score [55], AMBER [103] |  | \\n | Discriminative Task | e.g. POPE [69], RAH-Bench [16], FGHE [105] |  | \\n | Generative Task | e.g. GAVIE [73], Faith-Score [55] |  | \\n | Introducing | Negative Data | e.g. | LRV-Instruction [73]\\n | Introducing | Counterfactual Data | e.g. | HalluciDoctor [117]\\n | Mitigating Noises and Errors | e.g. ReCaption [105], EOS [120] |  | \\n | Scale-up Resolution | e.g. LLaVA-1.5 [74], InternVL [14], HallE-Switch [123] |  | \\n | Versatile | Vision Encoders | e.g. | VCoder [49], IVE [38]\\n | Dedicated Module | e.g. HallE-Switch [123] |  | \\n | Auxiliary Supervision\\n |  | Noisy Data | e.g. | HalluciDoctor [117], LLaVA-1.5 [74]\\n |  | Lack of Diversity | e.g. | LRV-Instruction [73], HalluciDoctor [117]\\n |  | Detailed descriptions (open question) | e.g. | Chen et al. [16], EOS [120]\\n |  | Frequent Objects | e.g. | POPE [69]\\n |  | Objects Occurrence | e.g. | LURE [137], VCD [64]\\n |  | Information Loss | e.g. | HallusionBench [72], AMBER [103]\\n |  | Feature Bias | e.g. | Tong et al. [98]\\n',\n",
       "   'title': 'Vision Model',\n",
       "   'chunks': [{'text': ' | Language Model | Parametric Knowledge | e.g. | VCD [64], Volcano [63]\\n | --- | --- | --- | ---\\n | Cross-modal Interface | Inferior Alignment | e.g. | HACL [52], Halle-Switch [123]\\n | Sequence Supervision | e.g. MOCHa [5], OPERA [45] |  | \\n | Visual Supervision | e.g. Chen et al. [16] |  | \\n | Human Feedback | e.g. RLHF-V [119] |  | \\n | Lose Visual Attention | e.g. OPERA [45], HaELM [104] |  | \\n | CHAIR | CHAIR [90] |  | \\n | POPE | POPE [69] |  | \\n | LLM-based | e.g. GAVIE [73], HaELM [104], HallusionBench [72] |  | \\n | Others | e.g. Faith-Score [55], AMBER [103] |  | \\n | Discriminative Task | e.g. POPE [69], RAH-Bench [16], FGHE [105] |  | \\n | Generative Task | e.g. GAVIE [73], Faith-Score [55] |  | \\n | Introducing | Negative Data | e.g. | LRV-Instruction [73]\\n | Introducing | Counterfactual Data | e.g. | HalluciDoctor [117]\\n | Mitigating Noises and Errors | e.g. ReCaption [105], EOS [120] |  | \\n | Scale-up Resolution | e.g. LLaVA-1.5 [74], InternVL [14], HallE-Switch [123] |  | \\n | Versatile | Vision Encoders | e.g. | VCoder [49], IVE [38]\\n | Dedicated Module | e.g. HallE-Switch [123] |  | \\n | Auxiliary Supervision\\n |  | Noisy Data | e.g. | HalluciDoctor [117], LLaVA-1.5 [74]\\n |  | Lack of Diversity | e.g. | LRV-Instruction [73], HalluciDoctor [117]\\n |  | Detailed descriptions (open question) | e.g. | Chen et al. [16], EOS [120]\\n |  | Frequent Objects | e.g. | POPE [69]\\n |  | Objects Occurrence | e.g. | LURE [137], VCD [64]\\n |  | Information Loss | e.g. | HallusionBench [72], AMBER [103]\\n |  | Feature Bias | e.g. | Tong et al. [98]\\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Vision Model',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Vision Model'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edb11e70>]},\n",
       "  {'text': 'from Data (§3.1) Hallucination from Model (§3.2)',\n",
       "   'title': 'from Data (§3.1) Hallucination from Model (§3.2)',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Hallucination Causes (§3)',\n",
       "   'title': 'Hallucination Causes (§3)',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Hallucination Metrics and Benchmarks(§4)\\nHallucination from Training (§3.3) Hallucination from Inference (§3.4)\\nHallucination Metrics\\nHallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)\\n | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n',\n",
       "   'title': 'Hallucination Metrics and Benchmarks(§4)',\n",
       "   'chunks': [{'text': ' | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Metrics and Benchmarks(§4) > Hallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Hallucination Metrics and Benchmarks(§4)'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edb13c40>]},\n",
       "  {'text': 'Hallucination from Training (§3.3) Hallucination from Inference (§3.4)',\n",
       "   'title': 'Hallucination from Training (§3.3) Hallucination from Inference (§3.4)',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Hallucination Metrics',\n",
       "   'title': 'Hallucination Metrics',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Hallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)\\n | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n',\n",
       "   'title': 'Hallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)',\n",
       "   'chunks': [{'text': ' | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Metrics and Benchmarks(§4) > Hallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Hallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2edb13c40>]},\n",
       "  {'text': 'Hallucination Mitigation (§5)\\nReinforcement Learning\\nGeneration Intervention\\nPost-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]\\nMitigating Inference-related Hallucinations (§5.4)\\nFig. 1.\\nThe main content flow and categorization of this survey.\\n(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.\\n | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n\\nFig. 2.\\nPopular architecture of multimodal large language model.',\n",
       "   'title': 'Hallucination Mitigation (§5)',\n",
       "   'chunks': [{'text': 'Fig. 1.\\nThe main content flow and categorization of this survey.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Hallucination Mitigation (§5)'},\n",
       "    {'text': '(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Hallucination Mitigation (§5)'},\n",
       "    {'text': ' | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Hallucination Mitigation (§5)'},\n",
       "    {'text': 'Fig. 2.\\nPopular architecture of multimodal large language model.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Hallucination Mitigation (§5)'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2ef7608e0>]},\n",
       "  {'text': 'Reinforcement Learning',\n",
       "   'title': 'Reinforcement Learning',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Generation Intervention',\n",
       "   'title': 'Generation Intervention',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Post-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]',\n",
       "   'title': 'Post-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]',\n",
       "   'chunks': [],\n",
       "   'tables': []},\n",
       "  {'text': 'Mitigating Inference-related Hallucinations (§5.4)\\nFig. 1.\\nThe main content flow and categorization of this survey.\\n(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.\\n | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n\\nFig. 2.\\nPopular architecture of multimodal large language model.',\n",
       "   'title': 'Mitigating Inference-related Hallucinations (§5.4)',\n",
       "   'chunks': [{'text': 'Fig. 1.\\nThe main content flow and categorization of this survey.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Mitigating Inference-related Hallucinations (§5.4)'},\n",
       "    {'text': '(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 3,\n",
       "     'source_doc': 'Mitigating Inference-related Hallucinations (§5.4)'},\n",
       "    {'text': ' | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Mitigating Inference-related Hallucinations (§5.4)'},\n",
       "    {'text': 'Fig. 2.\\nPopular architecture of multimodal large language model.',\n",
       "     'title': 'INTRODUCTION > 2.1 Large Language Models > Hallucination Mitigation (§5) > Mitigating Inference-related Hallucinations (§5.4)',\n",
       "     'page': 4,\n",
       "     'source_doc': 'Mitigating Inference-related Hallucinations (§5.4)'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2ef7608e0>]},\n",
       "  {'text': '2.2 Multimodal Large Language Models\\nMLLMs [22, 75, 111, 138] typically refers to a series of models that enable LLMs to perceive and comprehend data from various modalities.\\nAmong them, vision+LLM is particularly prominent, owing to the extensive research on vision-language models (VLMs) [51, 88, 116] prior to LLMs.\\nAs a result, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models (LVLMs).\\nThe goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\" the world via images or videos.\\nCombined with strong reasoning and language generation abilities, MLLMs trigger a series of downstream tasks in multimodal domains, such as image/video captioning and visual question answering.\\nAdditionally, MLLMs serve as the foundation for applications in other fields, such as AI assistants, embodied agents, and robotics.\\nIntegrating the two modalities of vision and language involves primarily two types of approaches.\\nThe first line of work is built upon off-the-shelf pre-trained uni-modal models.\\nSpecifically, these MLLMs usually incorporate a learnable interface between pre-trained visual encoders and LLMs.\\nThe interface extracts and integrates information from visual modalities.\\nSuch interfaces can be further categorized into 1) learnable query-based and 2) projection layer based.\\nLearnable querybased methods, exemplified by Q-Former [66], as used in MiniGPT-4 [138] and Instruct-BLIP [22], utilize a set of learnable query tokens to capture visual signals via cross-attention.\\nProjection layerbased methods, as widely applied in LLaVA [75], Shikra [12], etc., involve training a linear projection layer or a Multi-Layer Perceptron (MLP) module to transform extracted visual features.\\nBoth types of interfaces aim to transform pre-trained visual features into the input space of pre-trained LLMs.\\nAnother line of work is represented by Fuyu-8B [4] and Gemini [97].\\nUnlike previous methods that leverage pre-trained uni-modal models, these works employ end-to-end training from scratch.\\nTaking Fuyu-8B as an example, it does not employ any pre-trained vision encoder.\\nInstead, it directly inputs image patches and employs a linear projection to transform the raw pixels of each patch into embeddings.\\nThe abstracted pipeline is depicted in Fig. 2.\\nMLLMs take input from both visual and textual modalities, learning from multimodal instructions and responses, which leads to remarkable performance across various multimodal tasks.\\nRegarding the training of MLLMs, we provide a concise overview of the training process for interface-based MLLMs.\\nGiven that end-to-end models are closed-source, the training details are unknown.\\nTypically, the training of interface-based MLLMs consists of two stages: 1) pre-training, 2) instruction tuning.\\nPlease describe this image in detail.\\nThe image shows a group of people standing in a park.\\nThere is a large tree in the background with pink blossoms.\\nThe people are all wearing different clothing, including a man in a brown jacket and a woman in a white shirt and jeans.\\nThere are also some people in the background who are not in the foreground of the image.\\nThe people are standing in a grassy area and there are some benches and a fence in the background.\\nThe sky is clear and there are some clouds in the distance.\\nWhat is the girl in the picture doing?\\nThe girl in the picture is standing with her arms outstretched, as if she is trying to catch something.\\nIt is not clear what she is trying to catch, but it could be a kite or a frisbee.\\nThe other people in the picture are standing around her, watching what she is doing.\\nIt appears that they are all having a good time together in the park.\\nCategory Hallucination\\nAttribute Hallucination\\nRelation Hallucination\\nFig. 3.\\nThree types of typical hallucination.\\nPre-training.\\nGiven that models from each modality are pre-trained on their respective data, the objective of this pre-training phase is to achieve cross-modal feature alignment.\\nDuring training, both the pre-trained visual encoder and LLM remain frozen, with only the cross-modal interface being trained.\\nSimilar to traditional VLMs training, as exemplified by CLIP [88], web-scale imagetext pairs [92] are utilized for training.\\nGiven that the final output is at the LLM side, the most widely used loss function in this stage is the text generation loss, typically cross-entropy loss, which aligns with the pre-training of LLMs.\\nCertain studies (e.g., [22, 66]) explore the incorporation of contrastive loss and image-text matching loss to further enhance alignment.\\nAfter training, the interface module maps the visual features into the input embedding space of the LLM.\\nInstruction Tuning.\\nSimilar to LLMs, after pre-training, the current model still lacks instruction following ability in the multimodal context.\\nDuring the instruction tuning stage, both machinegenerated datasets [75] and human-annotated QA datasets [48, 59, 80] are utilized to enhance the model’s ability to comprehend and follow multimodal instructions.\\nUnlike pre-training data, the format and quality of instruction tuning data significantly impact the model’s performance.\\nIt is usually in the format of visual content - instruction - response.\\nEmpirical studies also demonstrate that high-quality data significantly enhances the performance of MLLMs.\\nDuring this stage, there are various options for training, such as fine-tuning LLM parameters in full [75], or using techniques like LoRA [41] to tune specific LLM parameters.',\n",
       "   'title': '2.2 Multimodal Large Language Models',\n",
       "   'chunks': [{'text': 'MLLMs [22, 75, 111, 138] typically refers to a series of models that enable LLMs to perceive and comprehend data from various modalities.\\nAmong them, vision+LLM is particularly prominent, owing to the extensive research on vision-language models (VLMs) [51, 88, 116] prior to LLMs.\\nAs a result, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models (LVLMs).\\nThe goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\" the world via images or videos.\\nCombined with strong reasoning and language generation abilities, MLLMs trigger a series of downstream tasks in multimodal domains, such as image/video captioning and visual question answering.\\nAdditionally, MLLMs serve as the foundation for applications in other fields, such as AI assistants, embodied agents, and robotics.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 4,\n",
       "     'source_doc': '2.2 Multimodal Large Language Models'},\n",
       "    {'text': 'Integrating the two modalities of vision and language involves primarily two types of approaches.\\nThe first line of work is built upon off-the-shelf pre-trained uni-modal models.\\nSpecifically, these MLLMs usually incorporate a learnable interface between pre-trained visual encoders and LLMs.\\nThe interface extracts and integrates information from visual modalities.\\nSuch interfaces can be further categorized into 1) learnable query-based and 2) projection layer based.\\nLearnable querybased methods, exemplified by Q-Former [66], as used in MiniGPT-4 [138] and Instruct-BLIP [22], utilize a set of learnable query tokens to capture visual signals via cross-attention.\\nProjection layerbased methods, as widely applied in LLaVA [75], Shikra [12], etc., involve training a linear projection layer or a Multi-Layer Perceptron (MLP) module to transform extracted visual features.\\nBoth types of interfaces aim to transform pre-trained visual features into the input space of pre-trained LLMs.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 4,\n",
       "     'source_doc': '2.2 Multimodal Large Language Models'},\n",
       "    {'text': 'Another line of work is represented by Fuyu-8B [4] and Gemini [97].\\nUnlike previous methods that leverage pre-trained uni-modal models, these works employ end-to-end training from scratch.\\nTaking Fuyu-8B as an example, it does not employ any pre-trained vision encoder.\\nInstead, it directly inputs image patches and employs a linear projection to transform the raw pixels of each patch into embeddings.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 4,\n",
       "     'source_doc': '2.2 Multimodal Large Language Models'},\n",
       "    {'text': 'The abstracted pipeline is depicted in Fig. 2.\\nMLLMs take input from both visual and textual modalities, learning from multimodal instructions and responses, which leads to remarkable performance across various multimodal tasks.\\nRegarding the training of MLLMs, we provide a concise overview of the training process for interface-based MLLMs.\\nGiven that end-to-end models are closed-source, the training details are unknown.\\nTypically, the training of interface-based MLLMs consists of two stages: 1) pre-training, 2) instruction tuning.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 4,\n",
       "     'source_doc': '2.2 Multimodal Large Language Models'},\n",
       "    {'text': 'Please describe this image in detail.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': '2.2 Multimodal Large Language Models'},\n",
       "    {'text': 'The image shows a group of people standing in a park.\\nThere is a large tree in the background with pink blossoms.\\nThe people are all wearing different clothing, including a man in a brown jacket and a woman in a white shirt and jeans.\\nThere are also some people in the background who are not in the foreground of the image.\\nThe people are standing in a grassy area and there are some benches and a fence in the background.\\nThe sky is clear and there are some clouds in the distance.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': '2.2 Multimodal Large Language Models'},\n",
       "    {'text': 'What is the girl in the picture doing?\\nThe girl in the picture is standing with her arms outstretched, as if she is trying to catch something.\\nIt is not clear what she is trying to catch, but it could be a kite or a frisbee.\\nThe other people in the picture are standing around her, watching what she is doing.\\nIt appears that they are all having a good time together in the park.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': '2.2 Multimodal Large Language Models'},\n",
       "    {'text': 'Fig. 3.\\nThree types of typical hallucination.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': '2.2 Multimodal Large Language Models'},\n",
       "    {'text': 'Pre-training.\\nGiven that models from each modality are pre-trained on their respective data, the objective of this pre-training phase is to achieve cross-modal feature alignment.\\nDuring training, both the pre-trained visual encoder and LLM remain frozen, with only the cross-modal interface being trained.\\nSimilar to traditional VLMs training, as exemplified by CLIP [88], web-scale imagetext pairs [92] are utilized for training.\\nGiven that the final output is at the LLM side, the most widely used loss function in this stage is the text generation loss, typically cross-entropy loss, which aligns with the pre-training of LLMs.\\nCertain studies (e.g., [22, 66]) explore the incorporation of contrastive loss and image-text matching loss to further enhance alignment.\\nAfter training, the interface module maps the visual features into the input embedding space of the LLM.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': '2.2 Multimodal Large Language Models'},\n",
       "    {'text': 'Instruction Tuning.\\nSimilar to LLMs, after pre-training, the current model still lacks instruction following ability in the multimodal context.\\nDuring the instruction tuning stage, both machinegenerated datasets [75] and human-annotated QA datasets [48, 59, 80] are utilized to enhance the model’s ability to comprehend and follow multimodal instructions.\\nUnlike pre-training data, the format and quality of instruction tuning data significantly impact the model’s performance.\\nIt is usually in the format of visual content - instruction - response.\\nEmpirical studies also demonstrate that high-quality data significantly enhances the performance of MLLMs.\\nDuring this stage, there are various options for training, such as fine-tuning LLM parameters in full [75], or using techniques like LoRA [41] to tune specific LLM parameters.',\n",
       "     'title': 'INTRODUCTION > 2.2 Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': '2.2 Multimodal Large Language Models'}],\n",
       "   'tables': []},\n",
       "  {'text': '2.3 Hallucinations in Multimodal Large Language Models\\nHallucination of MLLM generally refers to the phenomenon where the generated text response does not align with the corresponding visual content.\\nState-of-the-art studies in this field primarily focus on object hallucination, given that objects are central to research in computer vision and multimodal contexts.\\nRegarding inconsistency, two typical failure modes are: 1) missing objects, and 2) describing objects that are not present in the image or with incorrect statements.\\nEmpirically, the second mode has been shown to be less preferable to humans.\\nFor example, the LSMDC challenge [91] shows that correctness is more important to human judges than specificity.\\nIn contrast, the coverage of objects is less perceptible to humans.\\nThus, object coverage is not a primary focus in studies of object hallucination.\\nEmpirically, object hallucination can be categorized into three types: object category, object attribute, and object relation.\\nAn example of the three types of hallucination is shown in Fig. 3.\\n• Category.\\nMLLMs identify nonexistent object categories or incorrect categories in the given image.\\nFor example, in Fig. 3, \"some benches and a fence\", \"some clouds\", described in the text response do not exist in the given image.\\n• Attribute.\\nThe object categories identified by MLLMs are accurate, while the descriptions of these objects’ attributes (such as color, shape, material, content, counting, action, etc.) are wrong.\\nIn Fig. 3, \"pink blossoms\" is hallucinated by the MLLM as the color is inaccurate.\\n• Relation.\\nAll objects and their attributes are described correctly, but the relationships among them (such as human-object interactions or relative positions) do not align with the actual image content.\\nIn Fig. 3, \"standing around her, watching\" is a typical example of relation hallucination, as the objects are presented in the image but the relation is inaccurate.\\nIt’s worth noting that some literature may categorize objects counting, objects event, etc., as independent hallucination categories.\\nIn this work, we classify them under the attribute category.\\nThe definition of hallucination types aligns well with the domain of compositional generalization [79, 121] of VLMs, which investigates visio-linguistic generalization and reasoning abilities.\\n3 HALLUCINATION CAUSES\\nHallucinations have multifaceted origins, spanning the entire spectrum of MLLMs’ capability acquisition process.\\nIn this section, we delve into the root causes of hallucinations in MLLMs, primarily categorized into four aspects: Data, Model, Training, and Inference.',\n",
       "   'title': '2.3 Hallucinations in Multimodal Large Language Models',\n",
       "   'chunks': [{'text': 'Hallucination of MLLM generally refers to the phenomenon where the generated text response does not align with the corresponding visual content.\\nState-of-the-art studies in this field primarily focus on object hallucination, given that objects are central to research in computer vision and multimodal contexts.\\nRegarding inconsistency, two typical failure modes are: 1) missing objects, and 2) describing objects that are not present in the image or with incorrect statements.\\nEmpirically, the second mode has been shown to be less preferable to humans.\\nFor example, the LSMDC challenge [91] shows that correctness is more important to human judges than specificity.\\nIn contrast, the coverage of objects is less perceptible to humans.\\nThus, object coverage is not a primary focus in studies of object hallucination.\\nEmpirically, object hallucination can be categorized into three types: object category, object attribute, and object relation.\\nAn example of the three types of hallucination is shown in Fig. 3.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': '2.3 Hallucinations in Multimodal Large Language Models'},\n",
       "    {'text': '• Category.\\nMLLMs identify nonexistent object categories or incorrect categories in the given image.\\nFor example, in Fig. 3, \"some benches and a fence\", \"some clouds\", described in the text response do not exist in the given image.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 5,\n",
       "     'source_doc': '2.3 Hallucinations in Multimodal Large Language Models'},\n",
       "    {'text': '• Attribute.\\nThe object categories identified by MLLMs are accurate, while the descriptions of these objects’ attributes (such as color, shape, material, content, counting, action, etc.) are wrong.\\nIn Fig. 3, \"pink blossoms\" is hallucinated by the MLLM as the color is inaccurate.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 6,\n",
       "     'source_doc': '2.3 Hallucinations in Multimodal Large Language Models'},\n",
       "    {'text': '• Relation.\\nAll objects and their attributes are described correctly, but the relationships among them (such as human-object interactions or relative positions) do not align with the actual image content.\\nIn Fig. 3, \"standing around her, watching\" is a typical example of relation hallucination, as the objects are presented in the image but the relation is inaccurate.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 6,\n",
       "     'source_doc': '2.3 Hallucinations in Multimodal Large Language Models'},\n",
       "    {'text': 'It’s worth noting that some literature may categorize objects counting, objects event, etc., as independent hallucination categories.\\nIn this work, we classify them under the attribute category.\\nThe definition of hallucination types aligns well with the domain of compositional generalization [79, 121] of VLMs, which investigates visio-linguistic generalization and reasoning abilities.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 6,\n",
       "     'source_doc': '2.3 Hallucinations in Multimodal Large Language Models'},\n",
       "    {'text': '3 HALLUCINATION CAUSES',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 6,\n",
       "     'source_doc': '2.3 Hallucinations in Multimodal Large Language Models'},\n",
       "    {'text': 'Hallucinations have multifaceted origins, spanning the entire spectrum of MLLMs’ capability acquisition process.\\nIn this section, we delve into the root causes of hallucinations in MLLMs, primarily categorized into four aspects: Data, Model, Training, and Inference.',\n",
       "     'title': 'INTRODUCTION > 2.3 Hallucinations in Multimodal Large Language Models',\n",
       "     'page': 6,\n",
       "     'source_doc': '2.3 Hallucinations in Multimodal Large Language Models'}],\n",
       "   'tables': []},\n",
       "  {'text': '3.1 Data\\nData stands as the bedrock for MLLMs, enabling them to gain cross-modal understanding and instruction-following capabilities.\\nHowever, it can inadvertently become the source of MLLM hallucinations.\\nThis mainly manifests in three aspects: quantity, quality, and statistical bias.',\n",
       "   'title': '3.1 Data',\n",
       "   'chunks': [{'text': 'Data stands as the bedrock for MLLMs, enabling them to gain cross-modal understanding and instruction-following capabilities.\\nHowever, it can inadvertently become the source of MLLM hallucinations.\\nThis mainly manifests in three aspects: quantity, quality, and statistical bias.',\n",
       "     'title': 'INTRODUCTION > 3.1 Data',\n",
       "     'page': 6,\n",
       "     'source_doc': '3.1 Data'}],\n",
       "   'tables': []},\n",
       "  {'text': '3.1.1 Quantity\\nDeep learning models are data-hungry, especially large models like MLLMs.\\nThe amount of data plays an important role in building robust and reliable MLLMs.\\nCurrently, image-text pair datasets [92] and visual QA [48, 80] data are used for training MLLMs.\\nAlthough these datasets are usually larger than typical datasets in computer vision, they are still far less abundant than the text-only data used for training LLMs in terms of quantity.\\nInsufficient data could potentially lead to problematic cross-modal alignment, resulting in hallucinations [96, 103].',\n",
       "   'title': '3.1.1 Quantity',\n",
       "   'chunks': [{'text': 'Deep learning models are data-hungry, especially large models like MLLMs.\\nThe amount of data plays an important role in building robust and reliable MLLMs.\\nCurrently, image-text pair datasets [92] and visual QA [48, 80] data are used for training MLLMs.\\nAlthough these datasets are usually larger than typical datasets in computer vision, they are still far less abundant than the text-only data used for training LLMs in terms of quantity.\\nInsufficient data could potentially lead to problematic cross-modal alignment, resulting in hallucinations [96, 103].',\n",
       "     'title': 'INTRODUCTION > 3.1.1 Quantity',\n",
       "     'page': 6,\n",
       "     'source_doc': '3.1.1 Quantity'}],\n",
       "   'tables': []},\n",
       "  {'text': '3.1.2 Quality\\nGiven the increasing demand for large-scale training data, heuristic data collection methods are employed to efficiently gather vast volumes of data.\\nWhile these methods provide extensive data, they offer no guarantee of quality, thereby increasing the risk of hallucinations.\\nData quality relevant to hallucinations can be further categorized into the following three facets.\\n• Noisy data.\\nAs mentioned in the definition section, training MLLMs involves two stages.\\nThe pre-training stage employs image-text pairs crawled from the web, which contain inaccurate, misaligned, or corrupted data samples.\\nThe noisy data would limit the cross-modal feature alignment [117, 120], which serves as the foundation of MLLMs.\\nAs for the instruction tuning data, prevalent methods, such as LLaVA [75], utilize the advanced GPT-4 [82] model to generate instructions.\\nHowever, ChatGPT is a language model that cannot interpret visual content, leading to the risk of noisy data.\\nMoreover, language models themselves suffer from the issue of hallucination [44], further increasing the risk.\\nLLaVA-1.5 [74] adds human annotated QA data into instruction following and shows improved results, revealing the effect of noisy data.\\n• Lack of diversity.\\nRecent works [73, 117] reveal that the diversity of data also plays a crucial role.\\nFor the data used in the two training stages, instruction tuning data are more likely to have this issue since it is usually in a relatively small amount.\\nOne prominent property is that most instruction following data samples are composed of conversations regarding the image content.\\nWe regard this type of data as positive instruction, as it always faithfully reflects the image content.\\nIn contrast, negative instruction data [73] and reject answering responses [11] are rare in the datasets.\\nGiven such training data, one potential drawback observed by recent studies [69, 73] is that current models tend to answer \"Yes\" for any instructions presented to the model, even when a proper answer should be \"No\", leading to hallucination.\\nThis phenomenon indicates the effect of data diversity.\\n• Detailed descriptions (open question) The impact of the level of detail in textual descriptions on this matter remains an open question.\\nAs discussed in Sec. 2.2, the texts in pre-training data, such as LAION [92], usually describe the salient objects’ overall content.\\nWhile the texts in the instructing tuning stage, such as LLaVA-150k [75], consist of more detailed descriptions.\\nThis LLaVA-150k dataset is generated by GPT-4 based on objects recognized by vision models.\\nOne recent work [16] argues that within the training data, detailed descriptions related to object position, attributes, and non-salient objects are usually absent.\\nThis property results in incomplete cross-modal alignment and deprives the model of grounding ability [62, 126].\\nHowever, another work [120] hypothesizes that the text descriptions in the instruction tuning data contain too much details, exceeding the perception limit of MLLMs.\\nWhen trained with such detailed data, in an attempt to fit the detail level and length distribution of ground truth captions, the model may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThe detail level of the training data remains an open question.\\n3.1.3 Statistic bias.\\nNeural networks, especially large language models, possess an intrinsic tendency to memorize training data, as noted in [23].\\nThe nous (e.g., objects) distribution in the training dataset has strong effects on the behavior of the model.\\nFrequently appeared objects and object co-occurrence are two prominent types of statistical bias, as discussed in [69, 90, 137].\\nFor example, ‘person’ might be one of the most frequently appearing objects in the training data.\\nDuring inference, even if the given image does not contain a person, the model still tends to predict the presence of a person.\\nOn the other hand, object co-occurrence refers to the phenomenon that the model will remember which two objects usually ‘go together’ [90].\\nFor instance, given an image of a kitchen with a refrigerator, MLLMs are prone to answer ‘Yes’ when asked about a microwave, as refrigerators and microwaves frequently appear together in kitchen scenes.\\nBias exists in most datasets.\\nIncreasing the scale of data may alleviate the effect, but cannot fully resolve it, given the long-tail distribution of the real world.',\n",
       "   'title': '3.1.2 Quality',\n",
       "   'chunks': [{'text': 'Given the increasing demand for large-scale training data, heuristic data collection methods are employed to efficiently gather vast volumes of data.\\nWhile these methods provide extensive data, they offer no guarantee of quality, thereby increasing the risk of hallucinations.\\nData quality relevant to hallucinations can be further categorized into the following three facets.',\n",
       "     'title': 'INTRODUCTION > 3.1.2 Quality',\n",
       "     'page': 6,\n",
       "     'source_doc': '3.1.2 Quality'},\n",
       "    {'text': '• Noisy data.\\nAs mentioned in the definition section, training MLLMs involves two stages.\\nThe pre-training stage employs image-text pairs crawled from the web, which contain inaccurate, misaligned, or corrupted data samples.\\nThe noisy data would limit the cross-modal feature alignment [117, 120], which serves as the foundation of MLLMs.\\nAs for the instruction tuning data, prevalent methods, such as LLaVA [75], utilize the advanced GPT-4 [82] model to generate instructions.\\nHowever, ChatGPT is a language model that cannot interpret visual content, leading to the risk of noisy data.\\nMoreover, language models themselves suffer from the issue of hallucination [44], further increasing the risk.\\nLLaVA-1.5 [74] adds human annotated QA data into instruction following and shows improved results, revealing the effect of noisy data.',\n",
       "     'title': 'INTRODUCTION > 3.1.2 Quality',\n",
       "     'page': 6,\n",
       "     'source_doc': '3.1.2 Quality'},\n",
       "    {'text': '• Lack of diversity.\\nRecent works [73, 117] reveal that the diversity of data also plays a crucial role.\\nFor the data used in the two training stages, instruction tuning data are more likely to have this issue since it is usually in a relatively small amount.\\nOne prominent property is that most instruction following data samples are composed of conversations regarding the image content.\\nWe regard this type of data as positive instruction, as it always faithfully reflects the image content.\\nIn contrast, negative instruction data [73] and reject answering responses [11] are rare in the datasets.\\nGiven such training data, one potential drawback observed by recent studies [69, 73] is that current models tend to answer \"Yes\" for any instructions presented to the model, even when a proper answer should be \"No\", leading to hallucination.\\nThis phenomenon indicates the effect of data diversity.',\n",
       "     'title': 'INTRODUCTION > 3.1.2 Quality',\n",
       "     'page': 6,\n",
       "     'source_doc': '3.1.2 Quality'},\n",
       "    {'text': '• Detailed descriptions (open question) The impact of the level of detail in textual descriptions on this matter remains an open question.\\nAs discussed in Sec. 2.2, the texts in pre-training data, such as LAION [92], usually describe the salient objects’ overall content.\\nWhile the texts in the instructing tuning stage, such as LLaVA-150k [75], consist of more detailed descriptions.\\nThis LLaVA-150k dataset is generated by GPT-4 based on objects recognized by vision models.\\nOne recent work [16] argues that within the training data, detailed descriptions related to object position, attributes, and non-salient objects are usually absent.\\nThis property results in incomplete cross-modal alignment and deprives the model of grounding ability [62, 126].\\nHowever, another work [120] hypothesizes that the text descriptions in the instruction tuning data contain too much details, exceeding the perception limit of MLLMs.\\nWhen trained with such detailed data, in an attempt to fit the detail level and length distribution of ground truth captions, the model may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThe detail level of the training data remains an open question.',\n",
       "     'title': 'INTRODUCTION > 3.1.2 Quality',\n",
       "     'page': 7,\n",
       "     'source_doc': '3.1.2 Quality'},\n",
       "    {'text': '3.1.3 Statistic bias.\\nNeural networks, especially large language models, possess an intrinsic tendency to memorize training data, as noted in [23].\\nThe nous (e.g., objects) distribution in the training dataset has strong effects on the behavior of the model.\\nFrequently appeared objects and object co-occurrence are two prominent types of statistical bias, as discussed in [69, 90, 137].\\nFor example, ‘person’ might be one of the most frequently appearing objects in the training data.\\nDuring inference, even if the given image does not contain a person, the model still tends to predict the presence of a person.\\nOn the other hand, object co-occurrence refers to the phenomenon that the model will remember which two objects usually ‘go together’ [90].\\nFor instance, given an image of a kitchen with a refrigerator, MLLMs are prone to answer ‘Yes’ when asked about a microwave, as refrigerators and microwaves frequently appear together in kitchen scenes.\\nBias exists in most datasets.\\nIncreasing the scale of data may alleviate the effect, but cannot fully resolve it, given the long-tail distribution of the real world.',\n",
       "     'title': 'INTRODUCTION > 3.1.2 Quality',\n",
       "     'page': 7,\n",
       "     'source_doc': '3.1.2 Quality'}],\n",
       "   'tables': []},\n",
       "  {'text': '3.2 Model\\nCurrently, the architecture of popular MLLMs is composed of several components, usually including pre-trained vision model, pre-trained LLM, and alignment module as we discussed above.\\nSince these models are connected together, instead of end-to-end training from scratch, the error of each module can be accumulated.\\nInferior and problematic output from each module may lead to hallucinations.\\n• Weak vision model.\\nAs mentioned in related works [31, 90, 103], a primary potential reason for hallucination is a weak vision model, which can lead to misclassification or misinterpretation of visual concepts.\\nEven themost powerful visionmodelmay still experience information loss during the encoding process.\\nWeak vision model implies weak perception, which fundamentally undermines the multimodal understanding.\\n• Language model prior.\\nThe modern architecture of MLLMs is imbalanced.\\nUsually, the language model is much larger and stronger than the vision model, leading to a tendency to prioritize language-based information [31, 63, 64, 73, 90].\\nA typical phenomenon is that the knowledge entailed in the language model, also termed as parametric knowledge, can override the visual content.\\nFor example, given an image showing a red banana, which is counter-intuitive in the real world, an MLLM may still respond with \"yellow banana\", as \"banana is yellow\" is a deep-rooted knowledge in the LLM.\\nSuch language/knowledge prior makes the model overlook the visual content and response with hallucination.\\n• Weak alignment interface.\\nThe alignment interface plays an essential role in MLLMs, as it serves as the bridge between the two modalities.\\nA weak alignment interface can easily cause hallucinations.\\nOne potential cause of a weak alignment interface is data, as discussed in earlier sections.\\nApart from that, the interface architecture itself and training loss design also matter [52, 77, 123].\\nRecent work [52] argues that the LLaVA-like linear projection interface preserves most of the information, but lacks supervision on the projected feature.\\nVisualization in [52] reveals that the features after the projection layer remain distinct from the language embeddings.\\nThe distribution gap causes trouble in cross-modal interaction, leading to hallucination.\\nOn the other hand, Q-former-like [66] architecture has diverse supervision on the extracted visual feature, aligning it to the language embedding space.\\nHowever, the use of learnable queries inevitably results in the loss of fine-grained visual information.',\n",
       "   'title': '3.2 Model',\n",
       "   'chunks': [{'text': 'Currently, the architecture of popular MLLMs is composed of several components, usually including pre-trained vision model, pre-trained LLM, and alignment module as we discussed above.\\nSince these models are connected together, instead of end-to-end training from scratch, the error of each module can be accumulated.\\nInferior and problematic output from each module may lead to hallucinations.',\n",
       "     'title': 'INTRODUCTION > 3.2 Model',\n",
       "     'page': 7,\n",
       "     'source_doc': '3.2 Model'},\n",
       "    {'text': '• Weak vision model.\\nAs mentioned in related works [31, 90, 103], a primary potential reason for hallucination is a weak vision model, which can lead to misclassification or misinterpretation of visual concepts.\\nEven themost powerful visionmodelmay still experience information loss during the encoding process.\\nWeak vision model implies weak perception, which fundamentally undermines the multimodal understanding.',\n",
       "     'title': 'INTRODUCTION > 3.2 Model',\n",
       "     'page': 7,\n",
       "     'source_doc': '3.2 Model'},\n",
       "    {'text': '• Language model prior.\\nThe modern architecture of MLLMs is imbalanced.\\nUsually, the language model is much larger and stronger than the vision model, leading to a tendency to prioritize language-based information [31, 63, 64, 73, 90].\\nA typical phenomenon is that the knowledge entailed in the language model, also termed as parametric knowledge, can override the visual content.\\nFor example, given an image showing a red banana, which is counter-intuitive in the real world, an MLLM may still respond with \"yellow banana\", as \"banana is yellow\" is a deep-rooted knowledge in the LLM.\\nSuch language/knowledge prior makes the model overlook the visual content and response with hallucination.',\n",
       "     'title': 'INTRODUCTION > 3.2 Model',\n",
       "     'page': 7,\n",
       "     'source_doc': '3.2 Model'},\n",
       "    {'text': '• Weak alignment interface.\\nThe alignment interface plays an essential role in MLLMs, as it serves as the bridge between the two modalities.\\nA weak alignment interface can easily cause hallucinations.\\nOne potential cause of a weak alignment interface is data, as discussed in earlier sections.\\nApart from that, the interface architecture itself and training loss design also matter [52, 77, 123].\\nRecent work [52] argues that the LLaVA-like linear projection interface preserves most of the information, but lacks supervision on the projected feature.\\nVisualization in [52] reveals that the features after the projection layer remain distinct from the language embeddings.\\nThe distribution gap causes trouble in cross-modal interaction, leading to hallucination.\\nOn the other hand, Q-former-like [66] architecture has diverse supervision on the extracted visual feature, aligning it to the language embedding space.\\nHowever, the use of learnable queries inevitably results in the loss of fine-grained visual information.',\n",
       "     'title': 'INTRODUCTION > 3.2 Model',\n",
       "     'page': 8,\n",
       "     'source_doc': '3.2 Model'}],\n",
       "   'tables': []},\n",
       "  {'text': '3.3 Training\\nThe training objective of MLLMs is basically the same as LLMs, i.e, auto-regressive next token prediction loss.\\nThis loss is straightforward yet effective and easy to scale up, showing promising performance in language modeling.\\nHowever, some studies in the field of MLLMs have suggested that the next-token prediction loss might not be suitable for learning visual content due to its complex spatial structure [5, 16].\\nAdditionally, the loss optimizes at the token level, while lacking supervision at the sequence level [5].\\nAnother perspective is that, unlike training LLMs, the RLHF stage is absent in training procedure of MLLMs [96, 119], becoming a potential cause of hallucination.',\n",
       "   'title': '3.3 Training',\n",
       "   'chunks': [{'text': 'The training objective of MLLMs is basically the same as LLMs, i.e, auto-regressive next token prediction loss.\\nThis loss is straightforward yet effective and easy to scale up, showing promising performance in language modeling.\\nHowever, some studies in the field of MLLMs have suggested that the next-token prediction loss might not be suitable for learning visual content due to its complex spatial structure [5, 16].\\nAdditionally, the loss optimizes at the token level, while lacking supervision at the sequence level [5].\\nAnother perspective is that, unlike training LLMs, the RLHF stage is absent in training procedure of MLLMs [96, 119], becoming a potential cause of hallucination.',\n",
       "     'title': 'INTRODUCTION > 3.3 Training',\n",
       "     'page': 8,\n",
       "     'source_doc': '3.3 Training'}],\n",
       "   'tables': []},\n",
       "  {'text': '3.4 Inference\\nAs for inference, some works also argues a potential issue in the auto-regressive generation.\\nDuring generation, as the sequence length grows, the self-attention will focus more on the previously generated text tokens, i.e., the attention on the visual content is diluted [45, 102–104].\\nThrough visualizing the attention map during generation [45, 104], it can be observed that the generated content focuses more on previous special tokens, such as punctuation, rather than visual content tokens.\\nThe issue of ’losing attention’ would also lead to the model’s output response being irrelevant to the visual content.\\n4 HALLUCINATION METRICS AND BENCHMARKS\\nIn this section, we present a comprehensive overview of existing hallucination metrics and benchmarks, which are designed to assess the extent of hallucinations generated by existing cutting-edge MLLMs.\\nCurrently, the primary focus of these benchmarks is on evaluating the object hallucination of MLLM-generated content.\\nTab.\\n1 illustrates a summary of related benchmarks.\\nCHAIR [90].\\nAs one of the early works, the metric of CHAIR was proposed to evaluate object hallucination in the traditional image captioning task.\\nThis is achieved by computing what proportion of words generated are actually in the image according to the ground truth sentences and object segmentations.\\nThe computation of the CHAIR metric is straightforward and easy to understand.\\nThe metric has two variants: per-instance (denoted as CHAIR𝑖) and per-sentence\\nTable 1.\\nSummary of most relevant benchmarks and metrics of object hallucination in MLLMs.\\nThe order is based on chronological order on arxiv.\\nIn the metric column, Acc/P/R/F1 denotes Accuracy/Precision/Recall/F1Score.\\n | Benchmark | Venue Underlying Data Source | Size | Task | Type | Metric | Hallucination | Type\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | CHAIR [90] | EMNLP’18 MSCOCO [70] | 5,000 | Gen | CHAIR | ✓ | ✗ | ✗ | ✗\\n | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | Category Attribute Relation Others\\n | POPE [69] | EMNLP’23 MSCOCO [70] | 3,000 | Dis | Acc/P/R/F1 | ✓ | ✗ | ✗ | ✗\\n | MME [113] | arXiv’23 Jun MSCOCO [70] | 1457 | Dis | Acc/Score | ✓ | ✓ | ✗ | ✓\\n | CIEM [42] | NeurIPS-W’23 MSCOCO [70] | 78120 | Dis | Acc | ✓ | ✗ | ✗ | ✗\\n | M-HalDetect [32] | arXiv’23 Aug. MSCOCO [70] | 4,000 | Dis | Reward Model Score | ✓ | ✗ | ✗ | ✗\\n | MMHal-Bench [96] arXiv’23 Sep. Open-Images [61] |  | 96 | Gen | LLM Assessment | ✓ | ✗ | ✗ | ✓\\n | GAVIE [73] | ICLR’24 Visual-Genome [59] | 1,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | NOPE [77] | arXiv’23 Oct. Open-Images [61] | 36,000 | Dis | Acc/METEOR [3] | ✓ | ✗ | ✗ | ✗\\n | HaELM [104] | arXiv’23 Oct. MSCOCO [70] | 5,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | FaithScore [55] | arXiv’23 Nov. MSCOCO [70] | 2,000 | Gen | FaithScore | ✓ | ✓ | ✓ | Obj. Counting\\n | Bingo [21] | arXiv’23 Nov. Unknown | 370 | Gen | Human Assessment | ✗ | ✗ | ✗ | Model Bias\\n | AMBER [103] | arXiv’23 Nov. Web | 15,202 | Dis & Gen | AMBER Score | ✓ | ✓ | ✓ | ✗\\n | RAH-Bench [16] | arXiv’23 Nov. MSCOCO [70] | 3,000 | Dis | False Positive Rate | ✓ | ✓ | ✓ | ✗\\n | HallusionBench [72] | CVPR’24 Unknown | 1,129 | Gen | LLM Assessment | ✗ | ✗ | ✗ | Model Diagnose\\n | CCEval [123] | arXiv’23 Dec. Visual-Genome [59] | 100 | Gen | LLM-based CHAIR | ✓ | ✗ | ✗ | ✗\\n | MERLIM [100] | arXiv’23 Dec. MSCOCO [70] | 31,373 | Dis | Accuracy | ✓ | ✗ | ✓ | Obj. Counting\\n | FGHE [105] | arXiv’23 Dec. MSCOCO [70] | 200 | Dis | Acc/P/R/F | ✓ | ✓ | ✓ | Obj. Behavior\\n | MOCHa [5] | arXiv’23 Dec. Synthetic | 2,000 | Gen | OpenCHAIR [5] | ✓ | ✓ | ✗ | ✗\\n | CorrelationQA [35] | arXiv’24 Feb. Synthetic | 7,308 | Dis | Acc/AccDrop | ✗ | ✗ | ✗ | Model Bias\\n | VQAv2-IDK [11] | arXiv’24 Feb. VQAv2 [30] | 6,624 | Dis | Acc | ✗ | ✗ | ✗ | IK [11]\\n | MHaluBench [13] | arXiv’24 Feb. MSCOCO [70] | 1,860 | Gen | Acc/P/R/F | ✓ | ✓ | ✗ | T2I\\n | VHTest [46] | arXiv’24 Feb. MSCOCO [70] | 1,200 | Dis & Gen | Acc | ✓ | ✓ | ✗ | ✓\\n | Hal-Eavl [53] | arXiv’24 Feb. MSCOCO [70] & LAION [92] | 10,000 Dis & Gen |  | Acc/P/R/F & LLM Assessment | ✓ | ✓ | ✓ | Obj. Event\\n\\n(denoted as CHAIR𝑠):\\n | CHAIR𝑖 = | |{hallucinated objects}| |{all objects mentioned}| ,\\n | CHAIR𝑠 = | |{sentences with hallucinated object}| |{all sentences}|\\n\\nn the paper of CHAIR [90], the range of objects is restricted to the 80 MSCOCO objects. \\nSentence tokenization and synonyms mapping are applied to determine whether a generated sentence contains hallucinated objects.\\n Ground-truth caption and object segmentations both serve as groundtruth objects in the computation\\n. In the MLLM era, this metric is still widely used for assessing the response of MLLM\\nPOPE [69].\\nWhen used in MLLMs, the work of [69] argues that the CHAIR metric can be affected by the instruction designs and the length of generated captions.\\nTherefore, it proposes a new evaluation metric as well as a benchmark, called Pooling-based Object Probing Evaluation (POPE).\\nThe basic idea is to convert the evaluation of hallucination into a binary classification task by prompting MLLMs with simple Yes-or-No short questions about the probing objects (e.g., Is there a car in the image?) Compared to CHAIR, POPE offers increased stability and flexibility.\\nBased on this metric design, it further proposed an evaluation benchmark, drawing 500 images from the MSCOCO dataset.\\nThe questions in the benchmark consist of both positive and negative questions.\\nThe positive questions are formed based on the ground-truth objects, while the negative questions are built from sampling nonexistent objects.\\nThe benchmark is divided into three subsets according to different negative sampling strategy: random, popular, and adversarial.\\nPopular and adversarial sampling are specifically designed to assess frequently appeared objects and object co-occurrence.\\nAs an early representative work, POPE serves as a foundation of object hallucination evaluation.\\nMME [113].\\nMME is a comprehensive evaluation benchmark for MLLMs.\\nIt covers the examination of perception and cognition abilities, encompassing 14 subtasks.\\nRegarding object hallucination, there are four popular object related subtasks in its perception evaluation, including object existence, count, position, color.\\nSimilar to POPE, these tasks are formulated as Yes-or-No tasks.\\nCIEM [42] CIEM is a benchmark to evaluate hallucination of MLLMs.\\nUnlike previous works utilize human annotated objects, CIEM is generated using an automatic pipeline.\\nThe pipeline takes the text description of a specific image as input and utilize advanced LLMs to generate QA pairs.\\nAlthough the LLM-based data generation pipeline is not completely reliable, empirical result shows that the generated data has low error rate, around 5%.\\nMMHal-Bench [96] Comprising 96 image-question pairs, ranging in 8 question categories × 12 object topics, MMHal-Bench is a dedicated benchmark for evaluating hallucination in MLLMs.\\nThe 8 question categories cover various types of hallucination, including object attributes, counting, spatial relations, etc.\\nDuring the evaluation of MMHal-Bench, the GPT-4 model is employed to analyze and rate the responses.\\nGAVIE [73]GPT4-Assisted Visual Instruction Evaluation (GAVIE) is proposed to assess the LMM output in two different aspects: Relevancy to evaluate the instruction-following performance and Accuracy to measure the visual hallucination in the LMM output.\\nIt comprises a benchmark with 1,000 samples and an evaluation approach.\\nGAVIE evaluates the output of MLLMs in an open-ended manner and does not require human-annotated ground-truth answers.\\nThe core idea is to ask the advanced GPT-4 to work as a smart teacher and score the answer by taking image content, human instruction, and model response as input.\\nNOPE [77] This paper proposes to establish a distinction between object hallucination and incorrectness.\\na) Object hallucination refers to a phenomenon in VQA where a VL model’s response includes a non-existent object, despite the ground truth answer being a negative indefinite pronoun (e.g., \"none\", \"no one\", etc).\\nThis is denoted as NegP.\\nb) Incorrectness occurs when a VL model fails to accurately respond to a question with a ground truth answer that is anything other than NegP, denoted as Others.\\nThis paper argues that the existing VQA datasets have a significantly imbalanced distribution, containing too littleNegP data.\\nTherefore, NOPE (Negative Object Presence Evaluation) is proposed in this paper to complement the absent NegP data.\\nDuring evaluation, traditional metrics, including Accuracy and METEOR, are employed.\\nHaELM [104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4 models to assess the quality of theMLLM response.\\nIn contrast, the work of Hallucination Evaluation based on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination detection.\\nIt collects a set of hallucination data generated by a wide range of MLLMs, simulates data using ChatGPT, and trains an LLM based on LLaMA [99].\\nAfter that, the HaELM model becomes proficient in hallucination evaluation, leveraging reference descriptions of images as the basis of assessment.\\nFaithScore [55] Considering the natural forms of interaction between humans and MLLMs, FaithScore aims to evaluate free-form responses to open-ended questions.\\nDifferent from LLM-based overall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate, and analyze the elements in detail.\\nSpecifically, it includes three steps: descriptive sub-sentence identification, atomic fact generation, and fact verification.\\nThe evaluation metric involves finegrained object hallucination categories, including entity, count, color, relation, and other attributes.\\nThe final computation of FaithScore is the ratio of hallucinated content.\\nBingo [21] Bingo (Bias and Interference Challenges in Visual Language Models) is a benchmark specifically designed for assessing and analyzing the limitations of current popular MLLMs, such as GPT-4V [83].\\nIt comprises 190 failure instances, along with 131 success instances as a comparison.\\nThis benchmark reveals that state-of-the-art MLLMs show the phenomenon of bias and interference.\\nBias refers to the model’s susceptibility to generating hallucinatory outputs on specific types of examples, such as OCR bias, region bias, etc.\\nInterference refers to scenarios in which the judgment model can be disrupted, making it more susceptible to hallucination.\\nDue to the small amount of data in this benchmark, the assessment and analysis are mostly conducted by humans.\\nAMBER [103] Upon the application and evaluation of MLLMs, the tasks can be roughly divided into generative tasks and discriminative tasks.\\nFor generative tasks, this paper argues that most existing works rely on additional LLMs, suffering from computational cost.\\nAs for discriminative tasks, the most popular evaluation suite is POPE [69].\\nHowever, POPE lacks fine-grained hallucination types such as attributes and relations.\\nAMBER (An LLM-free Multi-dimensional Benchmark) is proposed to support the evaluation of generative tasks and discriminative tasks, including object existence hallucination, attribute hallucination, and relation hallucination.\\nIt further combines the CHAIR [90] metric in generative tasks and F1 in discriminative tasks to form the AMBER Score as follows:\\nAMBER Score = 𝐴𝑣𝑔(1 − CHAIR, F1).\\n(1)\\nRAH-Bench [16] Relation-Associated Hallucination Benchmark (RAH-Bench) can be regarded as an upgraded version of POPE, containing 3,000 yes-or-no questions with their corresponding images.\\nDifferent from POPE, RAH-Bench further divides the negative questions into three subsets.\\nEach subset contains 500 questions with misleading statements in the different aspects, including:\\n1) categorical hallucination, 2) attribute hallucination, 3) relation hallucination.\\nHallusionBench [72] To diagnose and analyze the potential failure modes of MLLMs, HallusionBench evaluates hallucination from a different perspective.\\nIt consists of 455 visual-question control pairs, with 346 different figures and a total of 1129 questions covering diverse topics and formats.\\nThe questions are divided into two categories: Visual Dependent and Visual Supplement.\\nThe Visual Dependent questions are defined as questions that do not have an affirmative answer without the visual context.\\nThis setting aims to evaluate visual commonsense knowledge and visual reasoning skills.\\nThe Visual Supplement questions can be answered without the visual input; the visual component merely provides supplemental information or corrections.\\nThis setting is designed to evaluate visual reasoning ability and the balance between parametric memory (language prior) and image context.\\nThis division provides a new perspective for understanding and diagnosing MLLMs.\\nCCEval [123] CCEval focuses on the hallucination evaluation of detailed captions.\\nTraditional caption-based evaluation benchmarks and metrics, like CHAIR, are known to favor short captions.\\nHowever, short captions often lack detail and contain less information.\\nTo address this issue, CCEval randomly samples 100 images from Visual Genome to form a benchmark.\\nIn evaluation, GPT-4 is utilized to parse the captions generated by MLLMs and extract objects.\\nAdditionally, this work introduces the \"coverage\" metric on top of CHAIR to ensure that the captions are detailed enough.\\nThis metric computes the ratio of objects in the caption that match the ground truth to the total number of ground truth objects.\\nIt additionally records the average number of objects as well as the average length of captions as auxiliary metric.\\nCompared with CHAIR, CCEval employs more diverse objects, as reflected in the source of ground truth (Visual Genome vs. COCO) and caption parsing (GPT-4 vs. rule-based tool).\\nMERLIM [100] MERLIM (Multi-modal Evaluation benchmaRk for Large Image-language Models) is a test-bed aimed at empirically evaluating MLLMs on core computer vision tasks, including object recognition, instance counting, and identifying object-to-object relationships.\\nMERLIM contains over 279K image-question pairs, and has a strong focus on detecting cross-modal hallucinations.\\nInterestingly, when organizing the data, a set of edited images is intentionally added.\\nBased on the original image, an inpainting strategy is employed to remove one object instance in the image.\\nWith this original-edited image pair, one can compare the output of the target MLLM and identify the hallucinated objects that lack visual grounding.\\nFGHE [105] Fine-Grained Object Hallucination Evaluation (FGHE) follows a binary classification approach similar to POPE to evaluate MLLMs.\\nHowever, unlike POPE, FGHE requires a different set of binary questions to measure fine-grained hallucination.\\nThe FGHE dataset consists of 50 images and 200 binary questions divided into three categories: (a) multiple-object questions, which verify the relationships between multiple objects in the image; (b) attribute questions, which verify attributes of objects in the image; and (c) behavior questions, which verify behaviors or objects in the image.\\nThe questions are manually defined by human annotators on a subset of 50 images from the validation set of the MSCOCO dataset.\\nSimilar to POPE, the Accuracy, Precision, Recall, and F1 score are employed as the evaluation metrics.\\nOpenCHAIR [5] The traditional CHAIR metric relies on the closed list of 80 objects in the MS-COCO dataset, limiting its application.\\nTo measure object hallucination in the open-vocabulary settings, OpenCHAIR expands CHAIR by relaxing the strong reliance on the closed vocabulary.\\nThe ’open-vocabulary’ manifests in two ways.\\nFirstly, when building the benchmark, it organizes a dataset consisting of synthetic images with corresponding captions, which include diverse, openvocabulary objects using a text-to-image diffusion model.\\nSecondly, during computing the metric, CHAIR checks if words or their synonyms (as given by fixed vocabulary lists) are found in groundtruth annotations.\\nIn contrast, OpenCHAIR extracts concrete objects from a predicted caption and identifies hallucinated objects from this list by querying an LLM.\\nSimilar to CHAIR, the final metric computation is based on the hallucination rate.\\nHal-Eval [53] The work of Hal-Eval [53] identifies another type of object hallucination: event hallucination.\\nThis type of hallucination fabricates a fictional target and constructing an entire narrative around it, including its attributes, relationships, and actions.\\nThis effort further completes the definition of hallucination types.\\nIn addition, this work proposes an evaluation benchmark, which encompasses both discriminative and generative evaluation methods.\\nThis is achieved by collecting two evaluation subsets, each tailored to the discriminative and generative evaluation methods, respectively.\\nCorrelationQA [35] CorrelationQA is a dedicated benchmark to quantify the effect of hallucination induced by the spurious visual input.\\nThis type of hallucination usually occurs when providing the MLLM with images that are highly relevant but inconsistent with the answers, causing MLLMs to suffer from hallucination.\\nSuch visual inputs are defined as ’spurious visual inputs’.\\nThis benchmark reveals that most of mainstream MLLMs, including GPT-4V, suffer from hallucination when presented with such spurious visual inputs.\\nThis phenomenon indicates that an image can induce MLLMs to instinctively focus on visual content, resulting in responses that are predominantly based on visual information without proper reasoning and thinking.\\nVQAv2-IDK [11] It has been widely discussed that in the binary QA scenario, MLLMs generally have a bias on answering ’Yes-or-No,’ leading to hallucination.\\nIn a more detailed question and answer scenario, MLLMs generally tend to respond to the user’s question plausibly, even if the desired answer is ’I don’t know’.\\nThe concept is defined as ’I Know (IK)’ hallucination in the work of [11].\\nAccordingly, a new benchmark, VQAv2-IDK, is proposed to specifically evaluate this type of hallucination.\\nVQAv2-IDK is a subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators.\\nIn this benchmark, ’I Know (IK)’ hallucination has been further categorized into four types:\\n• Unanswerable: no one can know.\\n• Don’t know: human may not know, but robot might.\\n• False questions: refers non-existing.\\n• Not sure: ambiguous to answer.\\nThis benchmark opens a new track for the study of hallucination in MLLMs.\\nMHaluBench [13] This benchmark does not aim to evaluate the MLLMs themselves.\\nInstead, it is intentionally designed to evaluate the hallucination detection tools of MLLMs, i.e., judge whether a tool can successfully detect the hallucination produced by an MLLM.\\nThus, the benchmark consists of hallucinatory examples.\\nSpecifically, the benchmark unifies image-to-text tasks and the text-to-image tasks into one evaluation suite: cross-modal consistency checking.\\nThe hallucinatory examples are generated using leading MLLMs and image generation models, such as LLaVA [75], MiniGPT-4 [138], DALL-E2 [89], and DALL-E3 [6].\\nDuring evaluation, the benchmark can be used to compare different hallucination detection methods based on their performance.\\nSo far, there are not many dedicated hallucination detection methods.\\nThis work serves as a basis for this direction.\\nVHTest [46] VHTest categorizes visual properties of objects in an image into 1) individual properties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which emerge from comparisons across multiple objects, such as relative size, relative position, and counting.\\nBased on such categorization, the authors further defined 8 visual hallucination modes, providing a very detailed evaluation of hallucination in MLLMs.\\nFurthermore, the collected 1,200 evaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no question\" (YNQ).\\nSuch design enables this benchmark to evaluate both generative and discriminative tasks.\\nComparison of mainstream models We compare the mainstream MLLMs on some representative benchmarks, providing a holistic overview of their performance from different dimensions.\\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks.\\nWe observe that the MLLMs’ performance is not always consistent across different benchmarks.\\nIt indicates that different benchmarks have different evaluation dimensions and emphases.\\nTable 2.\\nComparison of mainstreamMLLMs on generative benchmarks.\\nThe numbers come from the original papers of these benchmarks.\\n | Model | LLM Size CHAIR (On AMBER) ↓ | AMBER Score ↑ | HallusionBench All-Acc ↑ | FaithScore (LLaVA-1k) ↑ | FaithScore (COCO-Cap) ↑ | Hal-Eval In-domain Gen. Acc ↑ | Hal-Eval Out-of-domain Gen. Acc ↑\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B 23.1 | 54.1 | 43.93 | 0.7167 | 0.8546 | 27.3 | 29.5\\n | Multimodal-GPT [28] | 7B - | - | - | 0.5335 | 0.5440 | - | -\\n | InstructBLIP [22] | 7B 10.3 | 86.2 | 45.26 | 0.8091 | 0.9392 | 35.5 | 41.3\\n | GPT-4V [83] | - 4.3 | 92.7 | 65.28 | - | - | - | -\\n | LLaVA (7B) [75] | 7B 13.5 | 69.3 | - | - | - | 23.3 | 26.3\\n | LLaVA (13B) [75] | 13B - | - | - | 0.8360 | 0.8729 | - | -\\n | MiniGPT-4 (7B) [138] | 7B - | - | 35.78 | 0.5713 | 0.6359 | 61.4 | 50.1\\n | MiniGPT-4 (13B) [138] | 13B 15.9 | 76.7 | - | - | - | - | -\\n | mPLUG-Owl2 [112] | 7B 10.6 | 84.0 | 47.30 | - | - | - | -\\n | LLaVA-1.5 (7B) [74] | 7B 8.6 | 82.9 | - | - | - | 44.6 | 46.4\\n | LLaVA-1.5 (13B) [74] | 13B - | - | 46.94 | 0.8566 | 0.9425 | - | -\\n | CogVLM [106] | 7B 7.9 | 86.1 | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B - | - | 39.15 | - | - | - | -\\n | Open-Flamingo [1] | 9B - | - | 38.44 | - | - | - | -\\n | LRV-Instruction [73] | - - | - | 42.78 | - | - | - | -\\n\\n5 HALLUCINATION MITIGATION\\nIn this section, we present a comprehensive review of contemporary methods aimed at mitigating hallucinations in MLLMs.\\nBased on the properties and perspectives of these methods, we systematically categorize them into four groups.\\nSpecifically, we investigate approaches addressing hallucination from Data, Model, Training, and Inference.\\nTable 3.\\nComparison of mainstream MLLMs on discriminative benchmarks.\\nThe numbers come from the original papers of these benchmarks.\\n | Model | LLM Size | MME Existence Score ↑ | MME Count Score ↑ | MME Position Score ↑ | MME Color Score ↑ | POPE Random F1-Score ↑ | POPE Random F1-Score ↑ | POPE Adversarial F1-Score ↑ | RAH-Bench F1 Score ↑ | AMBER Dis. F1-Score ↑ | AMBER Score ↑ | Hal-Eval In-domain Event. F1 ↑ | Hal-Eval Out-of-domain Event. F1 ↑\\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B | 120.00 | 50.00 | 50.00 | 55.00 | 68.06 | 66.79 | 66.82 | 69.3 | 31.2 | 54.1 | 47 | 46.6\\n | ImageBind-LLM [34] | 7B | 128.33 | 60.00 | 46.67 | 73.33 | - | - | - | - | - | - | - | -\\n | InstructBLIP [22] (7B) | 7B | - | - | - | - | - | - | - | 89.1 | 82.6 | 86.2 | 66.2 | 66.6\\n | InstructBLIP [22] (13B) | 13B | 185.00 | 143.33 | 66.67 | 153.33 | 89.29 | 83.45 | 78.45 | 84.7 | - | - | - | -\\n | VisualGLM-6B [25] | 6B | 85.00 | 50.00 | 48.33 | 55.00 | - | - | - | - | - | - | - | -\\n | Multimodal-GPT [28] | 7B | 61.67 | 55.00 | 58.33 | 68.33 | 66.68 | 66.67 | 66.67 | - | - | - | - | -\\n | PandaGPT [95] | 7B | 70.00 | 50.00 | 50.00 | 50.00 | - | - | - | - | - | - | - | -\\n | LaVIN [78] | 13B | 185.00 | 88.33 | 63.33 | 75.00 | - | - | - | - | - | - | - | -\\n | Cheetor [67] | 7B | 180.00 | 96.67 | 80.00 | 116.67 | - | - | - | - | - | - | - | -\\n | GPT-4V [83] | - | 190.00 | 160.00 | 95.00 | 150.00 | - | - | - | - | 89.6 | 92.7 | - | -\\n | LLaVA [75] (7B) | 7B | - | - | - | - | - | - | - | 73.3 | 32.0 | 69.3 | 35.1 | 14.0\\n | LLaVA [75] (13B) | 13B | 185.00 | 155.00 | 133.33 | 170.00 | 68.65 | 67.72 | 66.98 | 71.8 | - | - | - | -\\n | LRV-Instruction [73] | 7B | 165.00 | 111.67 | 86.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Lynx [122] | 7B | 195.00 | 151.67 | 90.00 | 170.00 | - | - | - | - | - | - | - | -\\n | MMICL [130] | 11B | 170.00 | 160.00 | 81.67 | 156.67 | - | - | - | - | - | - | - | -\\n | Muffin [118] | 13B | 195.00 | 163.33 | 66.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Otter [65] | 7B | 195.00 | 88.33 | 86.67 | 113.33 | - | - | - | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B | 158.33 | 150.00 | 128.33 | 170.00 | - | - | - | - | - | - | - | -\\n | SPHINX [71] | 13B | 195.00 | 160.00 | 153.33 | 160.00 | - | - | - | - | - | - | - | -\\n | VPGTrans [124] | 7B | 70.00 | 85.00 | 63.33 | 73.33 | - | - | - | - | - | - | - | -\\n | BLIVA [43] | 11B | 180.00 | 138.33 | 81.67 | 180.00 | - | - | - | - | - | - | - | -\\n | InfMLLM [135] | 13B | 195.00 | 145.00 | 170.00 | 195.00 | - | - | - | - | - | - | - | -\\n | LLaMA-Adapter V2 [26] | 7B | 185.00 | 133.33 | 56.67 | 118.33 | - | - | - | - | - | - | - | -\\n | MiniGPT-4 [138] | 13B | 68.33 | 55.00 | 43.33 | 75.00 | 78.86 | 72.21 | 71.37 | - | 69.3 | 76.7 | 48.2 | 53.0\\n | mPLUG-Owl2 [112] | 7B | 185.00 | 155.00 | 88.33 | 150.00 | - | - | - | - | 78.5 | 84.0 | - | -\\n | LLaVA-1.5 [75] | 7B | - | - | - | - | - | - | - | - | 74.4 | 82.9 | 48.9 | 34.2\\n | CogVLM [106] | 7B | 195.00 | 165.00 | 103.33 | 160.00 | - | - | - | - | 80 | 86.1 | - | -\\n\\n5.1 Data\\nAs discussed in the section on hallucination causes 3, data is one of the primary factors inducing hallucination in MLLMs.\\nFor mitigating hallucination, recent works make attempts on data, including introducing negative data [73], introducing counterfactual data [117], and reducing noise and errors in existing dataset [105, 120].\\nLRV-Instruction [73] LRV-Instruction is proposed to address the issue that existing instruction tuning data primarily focus on positive instruction samples, leading the model to consistently answer ’Yes’.\\nLRV-Instruction is designed to include both positive and negative instructions for more robust visual instruction tuning, where the negative instructions include: 1) ’Nonexistent Object Manipulation’: introducing nonexistent objects, activities, attributes, and interactions; 2) ’Existent Object Manipulation’: manipulating existent objects with inconsistent attributes; 3) ’Knowledge Manipulation’: manipulating knowledge in instructions.\\nHalluciDoctor [117] This paper addresses the object hallucination problem in MLLMs by calibrating the instruction-tuning dataset.\\nThe calibration is conducted from two perspectives.\\nFirstly, it develops a hallucination detection pipeline via consistency cross-checking of multiple MLLMs.\\nBased on the detection result, the hallucinated content can be eliminated.\\nSecondly, this work observes that long-tail distribution and object co-occurrence in the training data are two primary factors of hallucination.\\nThus, a counterfactual visual instruction generation strategy is proposed to expand the dataset.\\nUsing the proposed methods, the instruction tuning data can be balanced and experience reduced hallucination.\\nMLLMs trained on the calibrated dataset are shown to be less prone to hallucination.\\nReCaption [105] This work proposes a framework called ReCaption to rewrite the text captions of existing image-text pairs in datasets.\\nThe framework comprises two steps: 1) keyword extraction, which extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which employs an LLM to generate sentences based on the extracted keywords.\\nUltimately, the framework produces a set of high-quality image-caption pairs.\\nExperiment results show that the model trained on the rewritten caption dataset has higher accuracy on certain benchmarks, such as the POPE benchmark [69].\\nDespite the performance improvement, the question of why rewritten captions can reduce hallucination remains an open problem.\\nEOS Decision [120] Previous work [137] provides an observation that hallucination tends to occur with objects positioned later in the generated descriptions.\\nIntuitively, an ideal scenario is that the MLLM can terminate the generation process in a timely manner.\\nThis idea is thoroughly explored in the work of [120] from the perspective of end-of-sequence (EOS) decision.\\nThe key insight is that the training data may exceed the perception limit of the MLLM.\\nWhen trained with such data, the model may attempt to fit the detail level and length distribution of ground truth captions.\\nHowever, it may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThus, the authors explored approaches to enhance the model’s end-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the perception limit.\\nRegarding data, this work proposes a data filtering strategy to eliminate harmful training data that could impair the model’s ability to end sequences.\\n5.2 Model\\n5.2.1 Scale-up Resolution\\nEnhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.\\n5.2.2 Versatile Vision Encoders\\nSeveral studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.\\n5.2.3 Dedicated Module\\nFollowing our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.\\n5.3 Training\\n5.3.1 Auxiliary supervision.\\nThe primary supervision signal of training MLLMs is language modeling loss (implemented as CrossEntropyLoss) in both pre-training and finetuning stage.\\nHowever, such supervision may not be sufficient to process the rich information encoded in the visual content.\\nAccordingly, the work of [16] constructs a fine-grained vision instruction dataset based on Panoptic Scene Graph (PSG), called Relation-Associated Instruction (RAI-30k).\\nIn addition to standard dialogues, each instruction in RAI-30k is associated with a relation annotation in PSG, which includes mask annotations for related instances.\\nWith these additional annotations, it further supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [57], guiding MLLMs to focus on highly-related image content.\\nWith the additional supervision from the mask prediction loss, MLLMs are encouraged to extract features that can better represent these crucial instances, thus generating more accurate responses and mitigating vision hallucination.\\nThe intuitive idea of supervising MLLMs with grounding shows promising performance in mitigating hallucination.\\nAnother line of work analyzes the training loss from the perspective of embedding space distribution.\\nAs introduced earlier, popular MLLMs typically project the encoded vision features into the input space of a specific LLM.\\nA recent work, HACL [52], argues that an ideal projection should blend the distribution of visual and textual embeddings.\\nHowever, despite visual projection, a significant modality gap exists between textual and visual tokens, suggesting that the current learned interfaces are not effective in mapping visual representations into the textual representation space of LLMs.\\nThis issue potentially exacerbates the tendency for MLLMs to generate more hallucinations.\\nTherefore, HACL proposes enhancing the alignment between visual and textual representations through contrastive loss.\\nTexts with hallucinations are used as hard negative examples for image anchors.\\nThe loss pulls representations of non-hallucinating text and visual samples closer while pushing representations of non-hallucinating and hallucinative text apart.\\nExperiment results show that this method not only reduces hallucination but also enhances performance on other popular benchmarks.\\nRecalling the work of EOS Decision [120], to teach the model to terminate the generation process properly, this work also designs a learning objective, termed Selective EOS Supervision, in addition to the data filtering strategy.\\nThis is achieved by simply modifying the Maximum Likelihood Estimation (MLE), enabling the model to mitigate hallucination through learning from regular instruction data.\\n5.3.2 Reinforcement Learning\\nReinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.\\nAutomatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.\\nReinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.\\nA concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.\\nReinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct\\nPreference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.\\nLLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.\\nSimilarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.\\nAnother similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.\\n5.3.3 Unlearning\\nUnlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.\\n5.4 Inference\\n5.4.1 Generation Intervention.\\nContrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.\\nGuided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.\\nSimilarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.\\nHALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.\\nOthers.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.\\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.\\n5.4.2 Post-hoc Correction\\nPost-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].\\nWoodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.\\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.\\nSimilar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.\\nLogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.\\n6 CHALLENGES AND FUTURE DIRECTIONS\\nThe research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.\\n6.1 Data-centric Challenges and Innovations\\nThe reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.\\n6.2 Cross-modal Alignment and Consistency\\nThe key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.\\n6.3 Advancements in Model Architecture\\nDespite recent advancements in model architectures of LLMs and MLLMs, designing effective architectures specifically tailored to hallucination remains a challenge.\\nDeveloping advanced model architectures capable of capturing complex linguistic structures and generating coherent and contextually relevant output based on input visual content is essential for improving the performance of MLLMs.\\nFuture research can explore innovative architectural designs based on identified causes of hallucination.\\nThis includes developing stronger visual perception models, innovative cross-modal interaction modules capable of transferring cross-modal information seamlessly, and novel large language model architectures faithful to input visual content and text instructions, etc.\\n6.4 Establishing Standardized Benchmarks\\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in assessing the degree of hallucination in MLLMs.\\nIn Table 1, it can be observed that there is a variety of evaluation benchmarks, but a lack of unified standards.\\nAmong them, one of the most popular benchmarks might be POPE [69], which employs a ’Yes-or-No’ evaluation protocol.\\nHowever, this binary-QA manner does not align with how humans use MLLMs.\\nAccordingly, some benchmarks specifically evaluate the hallucination of MLLMs in the (free-form) generative context.\\nYet, they often rely on external models, such as vision expert models or other LLMs, which limits their widespread application.\\nMoving forward, future research can investigate standardized benchmarks that are theoretically sound and easy to use.\\nOtherwise, research on methods to mitigate hallucinations may be built on an incorrect foundation.\\n6.5 Reframing Hallucination as a Feature\\nRecently, discussions on social media [56] have suggested that hallucination can be regarded as an inherent feature of LLMs and MLLMs.\\nThe models are like dream machines.\\nHuman users direct their dreams with prompts.\\nThe prompts start the dream, and based on the model’s hazy recollection of its training documents, most of the time the result goes someplace useful.\\nIt’s only when the dreams enter deemed factually incorrect territory that we label them as ’hallucinations’.\\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications presents exciting opportunities for enhancing user experiences and enabling new use cases.\\nAs humans are the end-users of these models, the primary goal is to enrich human user experiences.\\nFuture research may switch the optimization objective from specific cross-modal benchmarks to human experience.\\nFor example, Some content may cause hallucinations but will not affect the user experience, while some content may.\\nAlternatively, integrating hallucination to inspire more creative ideas in real-world applications could also be intriguing.\\n6.6 Enhancing Interpretability and Trust\\nExisting methods for hallucination mitigation are primarily based on empirical observations of specific patterns, such as skipping the ‘\\\\n’ token and penalizing over-trust tokens.\\nHowever, despite the impressive improvements achieved on specific benchmarks, understanding the underlying mechanisms and decision-making processes remains challenging.\\nFuture research should focus on developing techniques for interpreting and explaining the generation process of MLLMs, thereby providing insights into the factors influencing hallucinated content.\\nThis includes investigating methods for visualizing model internals, identifying salient features and linguistic patterns, and tracing the generation process from input to output.\\nEnhancing the interpretability of MLLMs will not only improve our understanding of model behavior but also enable users to better assess hallucinated content in practical applications.\\n6.7 Navigating the Ethical Landscape\\nAs MLLMs become increasingly proficient at generating realistic text, ethical considerations surrounding the use of generated content become paramount.\\nEspecially in the context of hallucination, the generated response may contain severely concerning ethical content, amplifying the importance of the problem.\\nAddressing ethical concerns related to misinformation, bias, privacy, and societal impact is crucial for promoting responsible AI practices in the development and deployment of MLLMs.\\nIn addition to addressing typical object hallucination, future research on MLLM hallucinations should prioritize ethical considerations throughout the entire lifecycle of MLLM development, from data collection and model training to deployment and evaluation.\\n7 CONCLUSION\\nBased on powerful large language models, multimodal large language models demonstrate remarkable performance across various multimodal tasks.\\nHowever, the phenomenon of hallucination presents a significant challenge to the practical applications of MLLMs, giving rise to undeniable concerns about safety, reliability, and trustworthiness.\\nIn this comprehensive survey, we conducted a thorough examination of hallucinations within multimodal large language models, focusing on their underlying causes, evaluation metrics, benchmarks, and mitigation methods.\\nDespite considerable progress, hallucination remains a complex and persistent concern that warrants ongoing investigation.\\nThe challenge of hallucination in multimodal large language models remains compelling, requiring continuous scrutiny and innovation.\\nIn light of these challenges, we have outlined several promising future directions in this burgeoning domain.\\nThrough navigating the intricate landscape of hallucinations, we aim for this survey to serve as a foundational resource for addressing the complexities of hallucination phenomena in MLLMs.\\nWe envision this survey empowering researchers and practitioners to dedicate efforts to advancing research and developing robust solutions in this vital area of study.',\n",
       "   'title': '3.4 Inference',\n",
       "   'chunks': [{'text': 'As for inference, some works also argues a potential issue in the auto-regressive generation.\\nDuring generation, as the sequence length grows, the self-attention will focus more on the previously generated text tokens, i.e., the attention on the visual content is diluted [45, 102–104].\\nThrough visualizing the attention map during generation [45, 104], it can be observed that the generated content focuses more on previous special tokens, such as punctuation, rather than visual content tokens.\\nThe issue of ’losing attention’ would also lead to the model’s output response being irrelevant to the visual content.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 8,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '4 HALLUCINATION METRICS AND BENCHMARKS',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 8,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'In this section, we present a comprehensive overview of existing hallucination metrics and benchmarks, which are designed to assess the extent of hallucinations generated by existing cutting-edge MLLMs.\\nCurrently, the primary focus of these benchmarks is on evaluating the object hallucination of MLLM-generated content.\\nTab.\\n1 illustrates a summary of related benchmarks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 8,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'CHAIR [90].\\nAs one of the early works, the metric of CHAIR was proposed to evaluate object hallucination in the traditional image captioning task.\\nThis is achieved by computing what proportion of words generated are actually in the image according to the ground truth sentences and object segmentations.\\nThe computation of the CHAIR metric is straightforward and easy to understand.\\nThe metric has two variants: per-instance (denoted as CHAIR𝑖) and per-sentence',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 8,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Table 1.\\nSummary of most relevant benchmarks and metrics of object hallucination in MLLMs.\\nThe order is based on chronological order on arxiv.\\nIn the metric column, Acc/P/R/F1 denotes Accuracy/Precision/Recall/F1Score.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': ' | Benchmark | Venue Underlying Data Source | Size | Task | Type | Metric | Hallucination | Type\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | CHAIR [90] | EMNLP’18 MSCOCO [70] | 5,000 | Gen | CHAIR | ✓ | ✗ | ✗ | ✗\\n | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | Category Attribute Relation Others\\n | POPE [69] | EMNLP’23 MSCOCO [70] | 3,000 | Dis | Acc/P/R/F1 | ✓ | ✗ | ✗ | ✗\\n | MME [113] | arXiv’23 Jun MSCOCO [70] | 1457 | Dis | Acc/Score | ✓ | ✓ | ✗ | ✓\\n | CIEM [42] | NeurIPS-W’23 MSCOCO [70] | 78120 | Dis | Acc | ✓ | ✗ | ✗ | ✗\\n | M-HalDetect [32] | arXiv’23 Aug. MSCOCO [70] | 4,000 | Dis | Reward Model Score | ✓ | ✗ | ✗ | ✗\\n | MMHal-Bench [96] arXiv’23 Sep. Open-Images [61] |  | 96 | Gen | LLM Assessment | ✓ | ✗ | ✗ | ✓\\n | GAVIE [73] | ICLR’24 Visual-Genome [59] | 1,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | NOPE [77] | arXiv’23 Oct. Open-Images [61] | 36,000 | Dis | Acc/METEOR [3] | ✓ | ✗ | ✗ | ✗\\n | HaELM [104] | arXiv’23 Oct. MSCOCO [70] | 5,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | FaithScore [55] | arXiv’23 Nov. MSCOCO [70] | 2,000 | Gen | FaithScore | ✓ | ✓ | ✓ | Obj. Counting\\n | Bingo [21] | arXiv’23 Nov. Unknown | 370 | Gen | Human Assessment | ✗ | ✗ | ✗ | Model Bias\\n | AMBER [103] | arXiv’23 Nov. Web | 15,202 | Dis & Gen | AMBER Score | ✓ | ✓ | ✓ | ✗\\n | RAH-Bench [16] | arXiv’23 Nov. MSCOCO [70] | 3,000 | Dis | False Positive Rate | ✓ | ✓ | ✓ | ✗\\n | HallusionBench [72] | CVPR’24 Unknown | 1,129 | Gen | LLM Assessment | ✗ | ✗ | ✗ | Model Diagnose\\n | CCEval [123] | arXiv’23 Dec. Visual-Genome [59] | 100 | Gen | LLM-based CHAIR | ✓ | ✗ | ✗ | ✗\\n | MERLIM [100] | arXiv’23 Dec. MSCOCO [70] | 31,373 | Dis | Accuracy | ✓ | ✗ | ✓ | Obj. Counting\\n | FGHE [105] | arXiv’23 Dec. MSCOCO [70] | 200 | Dis | Acc/P/R/F | ✓ | ✓ | ✓ | Obj. Behavior\\n | MOCHa [5] | arXiv’23 Dec. Synthetic | 2,000 | Gen | OpenCHAIR [5] | ✓ | ✓ | ✗ | ✗\\n | CorrelationQA [35] | arXiv’24 Feb. Synthetic | 7,308 | Dis | Acc/AccDrop | ✗ | ✗ | ✗ | Model Bias\\n | VQAv2-IDK [11] | arXiv’24 Feb. VQAv2 [30] | 6,624 | Dis | Acc | ✗ | ✗ | ✗ | IK [11]\\n | MHaluBench [13] | arXiv’24 Feb. MSCOCO [70] | 1,860 | Gen | Acc/P/R/F | ✓ | ✓ | ✗ | T2I\\n | VHTest [46] | arXiv’24 Feb. MSCOCO [70] | 1,200 | Dis & Gen | Acc | ✓ | ✓ | ✗ | ✓\\n | Hal-Eavl [53] | arXiv’24 Feb. MSCOCO [70] & LAION [92] | 10,000 Dis & Gen |  | Acc/P/R/F & LLM Assessment | ✓ | ✓ | ✓ | Obj. Event\\n',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '(denoted as CHAIR𝑠):',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': ' | CHAIR𝑖 = | |{hallucinated objects}| |{all objects mentioned}| ,\\n | CHAIR𝑠 = | |{sentences with hallucinated object}| |{all sentences}|\\n',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'n the paper of CHAIR [90], the range of objects is restricted to the 80 MSCOCO objects. \\nSentence tokenization and synonyms mapping are applied to determine whether a generated sentence contains hallucinated objects.\\n Ground-truth caption and object segmentations both serve as groundtruth objects in the computation\\n. In the MLLM era, this metric is still widely used for assessing the response of MLLM',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'POPE [69].\\nWhen used in MLLMs, the work of [69] argues that the CHAIR metric can be affected by the instruction designs and the length of generated captions.\\nTherefore, it proposes a new evaluation metric as well as a benchmark, called Pooling-based Object Probing Evaluation (POPE).\\nThe basic idea is to convert the evaluation of hallucination into a binary classification task by prompting MLLMs with simple Yes-or-No short questions about the probing objects (e.g., Is there a car in the image?) Compared to CHAIR, POPE offers increased stability and flexibility.\\nBased on this metric design, it further proposed an evaluation benchmark, drawing 500 images from the MSCOCO dataset.\\nThe questions in the benchmark consist of both positive and negative questions.\\nThe positive questions are formed based on the ground-truth objects, while the negative questions are built from sampling nonexistent objects.\\nThe benchmark is divided into three subsets according to different negative sampling strategy: random, popular, and adversarial.\\nPopular and adversarial sampling are specifically designed to assess frequently appeared objects and object co-occurrence.\\nAs an early representative work, POPE serves as a foundation of object hallucination evaluation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'MME [113].\\nMME is a comprehensive evaluation benchmark for MLLMs.\\nIt covers the examination of perception and cognition abilities, encompassing 14 subtasks.\\nRegarding object hallucination, there are four popular object related subtasks in its perception evaluation, including object existence, count, position, color.\\nSimilar to POPE, these tasks are formulated as Yes-or-No tasks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 9,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'CIEM [42] CIEM is a benchmark to evaluate hallucination of MLLMs.\\nUnlike previous works utilize human annotated objects, CIEM is generated using an automatic pipeline.\\nThe pipeline takes the text description of a specific image as input and utilize advanced LLMs to generate QA pairs.\\nAlthough the LLM-based data generation pipeline is not completely reliable, empirical result shows that the generated data has low error rate, around 5%.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'MMHal-Bench [96] Comprising 96 image-question pairs, ranging in 8 question categories × 12 object topics, MMHal-Bench is a dedicated benchmark for evaluating hallucination in MLLMs.\\nThe 8 question categories cover various types of hallucination, including object attributes, counting, spatial relations, etc.\\nDuring the evaluation of MMHal-Bench, the GPT-4 model is employed to analyze and rate the responses.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'GAVIE [73]GPT4-Assisted Visual Instruction Evaluation (GAVIE) is proposed to assess the LMM output in two different aspects: Relevancy to evaluate the instruction-following performance and Accuracy to measure the visual hallucination in the LMM output.\\nIt comprises a benchmark with 1,000 samples and an evaluation approach.\\nGAVIE evaluates the output of MLLMs in an open-ended manner and does not require human-annotated ground-truth answers.\\nThe core idea is to ask the advanced GPT-4 to work as a smart teacher and score the answer by taking image content, human instruction, and model response as input.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'NOPE [77] This paper proposes to establish a distinction between object hallucination and incorrectness.\\na) Object hallucination refers to a phenomenon in VQA where a VL model’s response includes a non-existent object, despite the ground truth answer being a negative indefinite pronoun (e.g., \"none\", \"no one\", etc).\\nThis is denoted as NegP.\\nb) Incorrectness occurs when a VL model fails to accurately respond to a question with a ground truth answer that is anything other than NegP, denoted as Others.\\nThis paper argues that the existing VQA datasets have a significantly imbalanced distribution, containing too littleNegP data.\\nTherefore, NOPE (Negative Object Presence Evaluation) is proposed in this paper to complement the absent NegP data.\\nDuring evaluation, traditional metrics, including Accuracy and METEOR, are employed.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'HaELM [104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4 models to assess the quality of theMLLM response.\\nIn contrast, the work of Hallucination Evaluation based on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination detection.\\nIt collects a set of hallucination data generated by a wide range of MLLMs, simulates data using ChatGPT, and trains an LLM based on LLaMA [99].\\nAfter that, the HaELM model becomes proficient in hallucination evaluation, leveraging reference descriptions of images as the basis of assessment.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'FaithScore [55] Considering the natural forms of interaction between humans and MLLMs, FaithScore aims to evaluate free-form responses to open-ended questions.\\nDifferent from LLM-based overall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate, and analyze the elements in detail.\\nSpecifically, it includes three steps: descriptive sub-sentence identification, atomic fact generation, and fact verification.\\nThe evaluation metric involves finegrained object hallucination categories, including entity, count, color, relation, and other attributes.\\nThe final computation of FaithScore is the ratio of hallucinated content.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Bingo [21] Bingo (Bias and Interference Challenges in Visual Language Models) is a benchmark specifically designed for assessing and analyzing the limitations of current popular MLLMs, such as GPT-4V [83].\\nIt comprises 190 failure instances, along with 131 success instances as a comparison.\\nThis benchmark reveals that state-of-the-art MLLMs show the phenomenon of bias and interference.\\nBias refers to the model’s susceptibility to generating hallucinatory outputs on specific types of examples, such as OCR bias, region bias, etc.\\nInterference refers to scenarios in which the judgment model can be disrupted, making it more susceptible to hallucination.\\nDue to the small amount of data in this benchmark, the assessment and analysis are mostly conducted by humans.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 10,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'AMBER [103] Upon the application and evaluation of MLLMs, the tasks can be roughly divided into generative tasks and discriminative tasks.\\nFor generative tasks, this paper argues that most existing works rely on additional LLMs, suffering from computational cost.\\nAs for discriminative tasks, the most popular evaluation suite is POPE [69].\\nHowever, POPE lacks fine-grained hallucination types such as attributes and relations.\\nAMBER (An LLM-free Multi-dimensional Benchmark) is proposed to support the evaluation of generative tasks and discriminative tasks, including object existence hallucination, attribute hallucination, and relation hallucination.\\nIt further combines the CHAIR [90] metric in generative tasks and F1 in discriminative tasks to form the AMBER Score as follows:',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'AMBER Score = 𝐴𝑣𝑔(1 − CHAIR, F1).\\n(1)',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'RAH-Bench [16] Relation-Associated Hallucination Benchmark (RAH-Bench) can be regarded as an upgraded version of POPE, containing 3,000 yes-or-no questions with their corresponding images.\\nDifferent from POPE, RAH-Bench further divides the negative questions into three subsets.\\nEach subset contains 500 questions with misleading statements in the different aspects, including:',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '1) categorical hallucination, 2) attribute hallucination, 3) relation hallucination.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'HallusionBench [72] To diagnose and analyze the potential failure modes of MLLMs, HallusionBench evaluates hallucination from a different perspective.\\nIt consists of 455 visual-question control pairs, with 346 different figures and a total of 1129 questions covering diverse topics and formats.\\nThe questions are divided into two categories: Visual Dependent and Visual Supplement.\\nThe Visual Dependent questions are defined as questions that do not have an affirmative answer without the visual context.\\nThis setting aims to evaluate visual commonsense knowledge and visual reasoning skills.\\nThe Visual Supplement questions can be answered without the visual input; the visual component merely provides supplemental information or corrections.\\nThis setting is designed to evaluate visual reasoning ability and the balance between parametric memory (language prior) and image context.\\nThis division provides a new perspective for understanding and diagnosing MLLMs.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'CCEval [123] CCEval focuses on the hallucination evaluation of detailed captions.\\nTraditional caption-based evaluation benchmarks and metrics, like CHAIR, are known to favor short captions.\\nHowever, short captions often lack detail and contain less information.\\nTo address this issue, CCEval randomly samples 100 images from Visual Genome to form a benchmark.\\nIn evaluation, GPT-4 is utilized to parse the captions generated by MLLMs and extract objects.\\nAdditionally, this work introduces the \"coverage\" metric on top of CHAIR to ensure that the captions are detailed enough.\\nThis metric computes the ratio of objects in the caption that match the ground truth to the total number of ground truth objects.\\nIt additionally records the average number of objects as well as the average length of captions as auxiliary metric.\\nCompared with CHAIR, CCEval employs more diverse objects, as reflected in the source of ground truth (Visual Genome vs. COCO) and caption parsing (GPT-4 vs. rule-based tool).',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'MERLIM [100] MERLIM (Multi-modal Evaluation benchmaRk for Large Image-language Models) is a test-bed aimed at empirically evaluating MLLMs on core computer vision tasks, including object recognition, instance counting, and identifying object-to-object relationships.\\nMERLIM contains over 279K image-question pairs, and has a strong focus on detecting cross-modal hallucinations.\\nInterestingly, when organizing the data, a set of edited images is intentionally added.\\nBased on the original image, an inpainting strategy is employed to remove one object instance in the image.\\nWith this original-edited image pair, one can compare the output of the target MLLM and identify the hallucinated objects that lack visual grounding.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 11,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'FGHE [105] Fine-Grained Object Hallucination Evaluation (FGHE) follows a binary classification approach similar to POPE to evaluate MLLMs.\\nHowever, unlike POPE, FGHE requires a different set of binary questions to measure fine-grained hallucination.\\nThe FGHE dataset consists of 50 images and 200 binary questions divided into three categories: (a) multiple-object questions, which verify the relationships between multiple objects in the image; (b) attribute questions, which verify attributes of objects in the image; and (c) behavior questions, which verify behaviors or objects in the image.\\nThe questions are manually defined by human annotators on a subset of 50 images from the validation set of the MSCOCO dataset.\\nSimilar to POPE, the Accuracy, Precision, Recall, and F1 score are employed as the evaluation metrics.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'OpenCHAIR [5] The traditional CHAIR metric relies on the closed list of 80 objects in the MS-COCO dataset, limiting its application.\\nTo measure object hallucination in the open-vocabulary settings, OpenCHAIR expands CHAIR by relaxing the strong reliance on the closed vocabulary.\\nThe ’open-vocabulary’ manifests in two ways.\\nFirstly, when building the benchmark, it organizes a dataset consisting of synthetic images with corresponding captions, which include diverse, openvocabulary objects using a text-to-image diffusion model.\\nSecondly, during computing the metric, CHAIR checks if words or their synonyms (as given by fixed vocabulary lists) are found in groundtruth annotations.\\nIn contrast, OpenCHAIR extracts concrete objects from a predicted caption and identifies hallucinated objects from this list by querying an LLM.\\nSimilar to CHAIR, the final metric computation is based on the hallucination rate.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Hal-Eval [53] The work of Hal-Eval [53] identifies another type of object hallucination: event hallucination.\\nThis type of hallucination fabricates a fictional target and constructing an entire narrative around it, including its attributes, relationships, and actions.\\nThis effort further completes the definition of hallucination types.\\nIn addition, this work proposes an evaluation benchmark, which encompasses both discriminative and generative evaluation methods.\\nThis is achieved by collecting two evaluation subsets, each tailored to the discriminative and generative evaluation methods, respectively.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'CorrelationQA [35] CorrelationQA is a dedicated benchmark to quantify the effect of hallucination induced by the spurious visual input.\\nThis type of hallucination usually occurs when providing the MLLM with images that are highly relevant but inconsistent with the answers, causing MLLMs to suffer from hallucination.\\nSuch visual inputs are defined as ’spurious visual inputs’.\\nThis benchmark reveals that most of mainstream MLLMs, including GPT-4V, suffer from hallucination when presented with such spurious visual inputs.\\nThis phenomenon indicates that an image can induce MLLMs to instinctively focus on visual content, resulting in responses that are predominantly based on visual information without proper reasoning and thinking.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'VQAv2-IDK [11] It has been widely discussed that in the binary QA scenario, MLLMs generally have a bias on answering ’Yes-or-No,’ leading to hallucination.\\nIn a more detailed question and answer scenario, MLLMs generally tend to respond to the user’s question plausibly, even if the desired answer is ’I don’t know’.\\nThe concept is defined as ’I Know (IK)’ hallucination in the work of [11].\\nAccordingly, a new benchmark, VQAv2-IDK, is proposed to specifically evaluate this type of hallucination.\\nVQAv2-IDK is a subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators.\\nIn this benchmark, ’I Know (IK)’ hallucination has been further categorized into four types:',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '• Unanswerable: no one can know.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '• Don’t know: human may not know, but robot might.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '• False questions: refers non-existing.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '• Not sure: ambiguous to answer.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'This benchmark opens a new track for the study of hallucination in MLLMs.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 12,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'MHaluBench [13] This benchmark does not aim to evaluate the MLLMs themselves.\\nInstead, it is intentionally designed to evaluate the hallucination detection tools of MLLMs, i.e., judge whether a tool can successfully detect the hallucination produced by an MLLM.\\nThus, the benchmark consists of hallucinatory examples.\\nSpecifically, the benchmark unifies image-to-text tasks and the text-to-image tasks into one evaluation suite: cross-modal consistency checking.\\nThe hallucinatory examples are generated using leading MLLMs and image generation models, such as LLaVA [75], MiniGPT-4 [138], DALL-E2 [89], and DALL-E3 [6].\\nDuring evaluation, the benchmark can be used to compare different hallucination detection methods based on their performance.\\nSo far, there are not many dedicated hallucination detection methods.\\nThis work serves as a basis for this direction.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'VHTest [46] VHTest categorizes visual properties of objects in an image into 1) individual properties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which emerge from comparisons across multiple objects, such as relative size, relative position, and counting.\\nBased on such categorization, the authors further defined 8 visual hallucination modes, providing a very detailed evaluation of hallucination in MLLMs.\\nFurthermore, the collected 1,200 evaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no question\" (YNQ).\\nSuch design enables this benchmark to evaluate both generative and discriminative tasks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Comparison of mainstream models We compare the mainstream MLLMs on some representative benchmarks, providing a holistic overview of their performance from different dimensions.\\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks.\\nWe observe that the MLLMs’ performance is not always consistent across different benchmarks.\\nIt indicates that different benchmarks have different evaluation dimensions and emphases.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Table 2.\\nComparison of mainstreamMLLMs on generative benchmarks.\\nThe numbers come from the original papers of these benchmarks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': ' | Model | LLM Size CHAIR (On AMBER) ↓ | AMBER Score ↑ | HallusionBench All-Acc ↑ | FaithScore (LLaVA-1k) ↑ | FaithScore (COCO-Cap) ↑ | Hal-Eval In-domain Gen. Acc ↑ | Hal-Eval Out-of-domain Gen. Acc ↑\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B 23.1 | 54.1 | 43.93 | 0.7167 | 0.8546 | 27.3 | 29.5\\n | Multimodal-GPT [28] | 7B - | - | - | 0.5335 | 0.5440 | - | -\\n | InstructBLIP [22] | 7B 10.3 | 86.2 | 45.26 | 0.8091 | 0.9392 | 35.5 | 41.3\\n | GPT-4V [83] | - 4.3 | 92.7 | 65.28 | - | - | - | -\\n | LLaVA (7B) [75] | 7B 13.5 | 69.3 | - | - | - | 23.3 | 26.3\\n | LLaVA (13B) [75] | 13B - | - | - | 0.8360 | 0.8729 | - | -\\n | MiniGPT-4 (7B) [138] | 7B - | - | 35.78 | 0.5713 | 0.6359 | 61.4 | 50.1\\n | MiniGPT-4 (13B) [138] | 13B 15.9 | 76.7 | - | - | - | - | -\\n | mPLUG-Owl2 [112] | 7B 10.6 | 84.0 | 47.30 | - | - | - | -\\n | LLaVA-1.5 (7B) [74] | 7B 8.6 | 82.9 | - | - | - | 44.6 | 46.4\\n | LLaVA-1.5 (13B) [74] | 13B - | - | 46.94 | 0.8566 | 0.9425 | - | -\\n | CogVLM [106] | 7B 7.9 | 86.1 | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B - | - | 39.15 | - | - | - | -\\n | Open-Flamingo [1] | 9B - | - | 38.44 | - | - | - | -\\n | LRV-Instruction [73] | - - | - | 42.78 | - | - | - | -\\n',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '5 HALLUCINATION MITIGATION',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'In this section, we present a comprehensive review of contemporary methods aimed at mitigating hallucinations in MLLMs.\\nBased on the properties and perspectives of these methods, we systematically categorize them into four groups.\\nSpecifically, we investigate approaches addressing hallucination from Data, Model, Training, and Inference.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 13,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Table 3.\\nComparison of mainstream MLLMs on discriminative benchmarks.\\nThe numbers come from the original papers of these benchmarks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 14,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': ' | Model | LLM Size | MME Existence Score ↑ | MME Count Score ↑ | MME Position Score ↑ | MME Color Score ↑ | POPE Random F1-Score ↑ | POPE Random F1-Score ↑ | POPE Adversarial F1-Score ↑ | RAH-Bench F1 Score ↑ | AMBER Dis. F1-Score ↑ | AMBER Score ↑ | Hal-Eval In-domain Event. F1 ↑ | Hal-Eval Out-of-domain Event. F1 ↑\\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B | 120.00 | 50.00 | 50.00 | 55.00 | 68.06 | 66.79 | 66.82 | 69.3 | 31.2 | 54.1 | 47 | 46.6\\n | ImageBind-LLM [34] | 7B | 128.33 | 60.00 | 46.67 | 73.33 | - | - | - | - | - | - | - | -\\n | InstructBLIP [22] (7B) | 7B | - | - | - | - | - | - | - | 89.1 | 82.6 | 86.2 | 66.2 | 66.6\\n | InstructBLIP [22] (13B) | 13B | 185.00 | 143.33 | 66.67 | 153.33 | 89.29 | 83.45 | 78.45 | 84.7 | - | - | - | -\\n | VisualGLM-6B [25] | 6B | 85.00 | 50.00 | 48.33 | 55.00 | - | - | - | - | - | - | - | -\\n | Multimodal-GPT [28] | 7B | 61.67 | 55.00 | 58.33 | 68.33 | 66.68 | 66.67 | 66.67 | - | - | - | - | -\\n | PandaGPT [95] | 7B | 70.00 | 50.00 | 50.00 | 50.00 | - | - | - | - | - | - | - | -\\n | LaVIN [78] | 13B | 185.00 | 88.33 | 63.33 | 75.00 | - | - | - | - | - | - | - | -\\n | Cheetor [67] | 7B | 180.00 | 96.67 | 80.00 | 116.67 | - | - | - | - | - | - | - | -\\n | GPT-4V [83] | - | 190.00 | 160.00 | 95.00 | 150.00 | - | - | - | - | 89.6 | 92.7 | - | -\\n | LLaVA [75] (7B) | 7B | - | - | - | - | - | - | - | 73.3 | 32.0 | 69.3 | 35.1 | 14.0\\n | LLaVA [75] (13B) | 13B | 185.00 | 155.00 | 133.33 | 170.00 | 68.65 | 67.72 | 66.98 | 71.8 | - | - | - | -\\n | LRV-Instruction [73] | 7B | 165.00 | 111.67 | 86.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Lynx [122] | 7B | 195.00 | 151.67 | 90.00 | 170.00 | - | - | - | - | - | - | - | -\\n | MMICL [130] | 11B | 170.00 | 160.00 | 81.67 | 156.67 | - | - | - | - | - | - | - | -\\n | Muffin [118] | 13B | 195.00 | 163.33 | 66.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Otter [65] | 7B | 195.00 | 88.33 | 86.67 | 113.33 | - | - | - | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B | 158.33 | 150.00 | 128.33 | 170.00 | - | - | - | - | - | - | - | -\\n | SPHINX [71] | 13B | 195.00 | 160.00 | 153.33 | 160.00 | - | - | - | - | - | - | - | -\\n | VPGTrans [124] | 7B | 70.00 | 85.00 | 63.33 | 73.33 | - | - | - | - | - | - | - | -\\n | BLIVA [43] | 11B | 180.00 | 138.33 | 81.67 | 180.00 | - | - | - | - | - | - | - | -\\n | InfMLLM [135] | 13B | 195.00 | 145.00 | 170.00 | 195.00 | - | - | - | - | - | - | - | -\\n | LLaMA-Adapter V2 [26] | 7B | 185.00 | 133.33 | 56.67 | 118.33 | - | - | - | - | - | - | - | -\\n | MiniGPT-4 [138] | 13B | 68.33 | 55.00 | 43.33 | 75.00 | 78.86 | 72.21 | 71.37 | - | 69.3 | 76.7 | 48.2 | 53.0\\n | mPLUG-Owl2 [112] | 7B | 185.00 | 155.00 | 88.33 | 150.00 | - | - | - | - | 78.5 | 84.0 | - | -\\n | LLaVA-1.5 [75] | 7B | - | - | - | - | - | - | - | - | 74.4 | 82.9 | 48.9 | 34.2\\n | CogVLM [106] | 7B | 195.00 | 165.00 | 103.33 | 160.00 | - | - | - | - | 80 | 86.1 | - | -\\n',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference',\n",
       "     'page': 14,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'As discussed in the section on hallucination causes 3, data is one of the primary factors inducing hallucination in MLLMs.\\nFor mitigating hallucination, recent works make attempts on data, including introducing negative data [73], introducing counterfactual data [117], and reducing noise and errors in existing dataset [105, 120].',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'LRV-Instruction [73] LRV-Instruction is proposed to address the issue that existing instruction tuning data primarily focus on positive instruction samples, leading the model to consistently answer ’Yes’.\\nLRV-Instruction is designed to include both positive and negative instructions for more robust visual instruction tuning, where the negative instructions include: 1) ’Nonexistent Object Manipulation’: introducing nonexistent objects, activities, attributes, and interactions; 2) ’Existent Object Manipulation’: manipulating existent objects with inconsistent attributes; 3) ’Knowledge Manipulation’: manipulating knowledge in instructions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'HalluciDoctor [117] This paper addresses the object hallucination problem in MLLMs by calibrating the instruction-tuning dataset.\\nThe calibration is conducted from two perspectives.\\nFirstly, it develops a hallucination detection pipeline via consistency cross-checking of multiple MLLMs.\\nBased on the detection result, the hallucinated content can be eliminated.\\nSecondly, this work observes that long-tail distribution and object co-occurrence in the training data are two primary factors of hallucination.\\nThus, a counterfactual visual instruction generation strategy is proposed to expand the dataset.\\nUsing the proposed methods, the instruction tuning data can be balanced and experience reduced hallucination.\\nMLLMs trained on the calibrated dataset are shown to be less prone to hallucination.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'ReCaption [105] This work proposes a framework called ReCaption to rewrite the text captions of existing image-text pairs in datasets.\\nThe framework comprises two steps: 1) keyword extraction, which extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which employs an LLM to generate sentences based on the extracted keywords.\\nUltimately, the framework produces a set of high-quality image-caption pairs.\\nExperiment results show that the model trained on the rewritten caption dataset has higher accuracy on certain benchmarks, such as the POPE benchmark [69].\\nDespite the performance improvement, the question of why rewritten captions can reduce hallucination remains an open problem.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'EOS Decision [120] Previous work [137] provides an observation that hallucination tends to occur with objects positioned later in the generated descriptions.\\nIntuitively, an ideal scenario is that the MLLM can terminate the generation process in a timely manner.\\nThis idea is thoroughly explored in the work of [120] from the perspective of end-of-sequence (EOS) decision.\\nThe key insight is that the training data may exceed the perception limit of the MLLM.\\nWhen trained with such data, the model may attempt to fit the detail level and length distribution of ground truth captions.\\nHowever, it may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThus, the authors explored approaches to enhance the model’s end-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the perception limit.\\nRegarding data, this work proposes a data filtering strategy to eliminate harmful training data that could impair the model’s ability to end sequences.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 15,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Enhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.1 Scale-up Resolution',\n",
       "     'page': 15,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Several studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.2 Versatile Vision Encoders',\n",
       "     'page': 15,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Following our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.3 Dedicated Module',\n",
       "     'page': 16,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '5.3.1 Auxiliary supervision.\\nThe primary supervision signal of training MLLMs is language modeling loss (implemented as CrossEntropyLoss) in both pre-training and finetuning stage.\\nHowever, such supervision may not be sufficient to process the rich information encoded in the visual content.\\nAccordingly, the work of [16] constructs a fine-grained vision instruction dataset based on Panoptic Scene Graph (PSG), called Relation-Associated Instruction (RAI-30k).\\nIn addition to standard dialogues, each instruction in RAI-30k is associated with a relation annotation in PSG, which includes mask annotations for related instances.\\nWith these additional annotations, it further supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [57], guiding MLLMs to focus on highly-related image content.\\nWith the additional supervision from the mask prediction loss, MLLMs are encouraged to extract features that can better represent these crucial instances, thus generating more accurate responses and mitigating vision hallucination.\\nThe intuitive idea of supervising MLLMs with grounding shows promising performance in mitigating hallucination.\\nAnother line of work analyzes the training loss from the perspective of embedding space distribution.\\nAs introduced earlier, popular MLLMs typically project the encoded vision features into the input space of a specific LLM.\\nA recent work, HACL [52], argues that an ideal projection should blend the distribution of visual and textual embeddings.\\nHowever, despite visual projection, a significant modality gap exists between textual and visual tokens, suggesting that the current learned interfaces are not effective in mapping visual representations into the textual representation space of LLMs.\\nThis issue potentially exacerbates the tendency for MLLMs to generate more hallucinations.\\nTherefore, HACL proposes enhancing the alignment between visual and textual representations through contrastive loss.\\nTexts with hallucinations are used as hard negative examples for image anchors.\\nThe loss pulls representations of non-hallucinating text and visual samples closer while pushing representations of non-hallucinating and hallucinative text apart.\\nExperiment results show that this method not only reduces hallucination but also enhances performance on other popular benchmarks.\\nRecalling the work of EOS Decision [120], to teach the model to terminate the generation process properly, this work also designs a learning objective, termed Selective EOS Supervision, in addition to the data filtering strategy.\\nThis is achieved by simply modifying the Maximum Likelihood Estimation (MLE), enabling the model to mitigate hallucination through learning from regular instruction data.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training',\n",
       "     'page': 16,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Reinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 16,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Automatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Reinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'A concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'A more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Reinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Preference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'LLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Similarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Another similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Unlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.3 Unlearning',\n",
       "     'page': 18,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Contrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 18,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Guided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Similarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'HALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Others.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Another interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Post-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Woodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Another line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Similar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'LogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '6 CHALLENGES AND FUTURE DIRECTIONS',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'The research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '6.1 Data-centric Challenges and Innovations',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'The reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '6.2 Cross-modal Alignment and Consistency',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 21,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'The key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 21,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Despite recent advancements in model architectures of LLMs and MLLMs, designing effective architectures specifically tailored to hallucination remains a challenge.\\nDeveloping advanced model architectures capable of capturing complex linguistic structures and generating coherent and contextually relevant output based on input visual content is essential for improving the performance of MLLMs.\\nFuture research can explore innovative architectural designs based on identified causes of hallucination.\\nThis includes developing stronger visual perception models, innovative cross-modal interaction modules capable of transferring cross-modal information seamlessly, and novel large language model architectures faithful to input visual content and text instructions, etc.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.3 Advancements in Model Architecture',\n",
       "     'page': 21,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'The lack of standardized benchmarks and evaluation metrics poses significant challenges in assessing the degree of hallucination in MLLMs.\\nIn Table 1, it can be observed that there is a variety of evaluation benchmarks, but a lack of unified standards.\\nAmong them, one of the most popular benchmarks might be POPE [69], which employs a ’Yes-or-No’ evaluation protocol.\\nHowever, this binary-QA manner does not align with how humans use MLLMs.\\nAccordingly, some benchmarks specifically evaluate the hallucination of MLLMs in the (free-form) generative context.\\nYet, they often rely on external models, such as vision expert models or other LLMs, which limits their widespread application.\\nMoving forward, future research can investigate standardized benchmarks that are theoretically sound and easy to use.\\nOtherwise, research on methods to mitigate hallucinations may be built on an incorrect foundation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.4 Establishing Standardized Benchmarks',\n",
       "     'page': 21,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Recently, discussions on social media [56] have suggested that hallucination can be regarded as an inherent feature of LLMs and MLLMs.\\nThe models are like dream machines.\\nHuman users direct their dreams with prompts.\\nThe prompts start the dream, and based on the model’s hazy recollection of its training documents, most of the time the result goes someplace useful.\\nIt’s only when the dreams enter deemed factually incorrect territory that we label them as ’hallucinations’.\\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications presents exciting opportunities for enhancing user experiences and enabling new use cases.\\nAs humans are the end-users of these models, the primary goal is to enrich human user experiences.\\nFuture research may switch the optimization objective from specific cross-modal benchmarks to human experience.\\nFor example, Some content may cause hallucinations but will not affect the user experience, while some content may.\\nAlternatively, integrating hallucination to inspire more creative ideas in real-world applications could also be intriguing.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.5 Reframing Hallucination as a Feature',\n",
       "     'page': 21,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Existing methods for hallucination mitigation are primarily based on empirical observations of specific patterns, such as skipping the ‘\\\\n’ token and penalizing over-trust tokens.\\nHowever, despite the impressive improvements achieved on specific benchmarks, understanding the underlying mechanisms and decision-making processes remains challenging.\\nFuture research should focus on developing techniques for interpreting and explaining the generation process of MLLMs, thereby providing insights into the factors influencing hallucinated content.\\nThis includes investigating methods for visualizing model internals, identifying salient features and linguistic patterns, and tracing the generation process from input to output.\\nEnhancing the interpretability of MLLMs will not only improve our understanding of model behavior but also enable users to better assess hallucinated content in practical applications.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.6 Enhancing Interpretability and Trust',\n",
       "     'page': 22,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'As MLLMs become increasingly proficient at generating realistic text, ethical considerations surrounding the use of generated content become paramount.\\nEspecially in the context of hallucination, the generated response may contain severely concerning ethical content, amplifying the importance of the problem.\\nAddressing ethical concerns related to misinformation, bias, privacy, and societal impact is crucial for promoting responsible AI practices in the development and deployment of MLLMs.\\nIn addition to addressing typical object hallucination, future research on MLLM hallucinations should prioritize ethical considerations throughout the entire lifecycle of MLLM development, from data collection and model training to deployment and evaluation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.7 Navigating the Ethical Landscape',\n",
       "     'page': 22,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': '7 CONCLUSION',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.7 Navigating the Ethical Landscape',\n",
       "     'page': 22,\n",
       "     'source_doc': '3.4 Inference'},\n",
       "    {'text': 'Based on powerful large language models, multimodal large language models demonstrate remarkable performance across various multimodal tasks.\\nHowever, the phenomenon of hallucination presents a significant challenge to the practical applications of MLLMs, giving rise to undeniable concerns about safety, reliability, and trustworthiness.\\nIn this comprehensive survey, we conducted a thorough examination of hallucinations within multimodal large language models, focusing on their underlying causes, evaluation metrics, benchmarks, and mitigation methods.\\nDespite considerable progress, hallucination remains a complex and persistent concern that warrants ongoing investigation.\\nThe challenge of hallucination in multimodal large language models remains compelling, requiring continuous scrutiny and innovation.\\nIn light of these challenges, we have outlined several promising future directions in this burgeoning domain.\\nThrough navigating the intricate landscape of hallucinations, we aim for this survey to serve as a foundational resource for addressing the complexities of hallucination phenomena in MLLMs.\\nWe envision this survey empowering researchers and practitioners to dedicate efforts to advancing research and developing robust solutions in this vital area of study.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.7 Navigating the Ethical Landscape',\n",
       "     'page': 22,\n",
       "     'source_doc': '3.4 Inference'}],\n",
       "   'tables': [<llmsherpa.readers.layout_reader.Table at 0x7ff2ef7615d0>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2ef780640>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2ef780f70>,\n",
       "    <llmsherpa.readers.layout_reader.Table at 0x7ff2ef782e00>]},\n",
       "  {'text': '5.1 Data\\nAs discussed in the section on hallucination causes 3, data is one of the primary factors inducing hallucination in MLLMs.\\nFor mitigating hallucination, recent works make attempts on data, including introducing negative data [73], introducing counterfactual data [117], and reducing noise and errors in existing dataset [105, 120].\\nLRV-Instruction [73] LRV-Instruction is proposed to address the issue that existing instruction tuning data primarily focus on positive instruction samples, leading the model to consistently answer ’Yes’.\\nLRV-Instruction is designed to include both positive and negative instructions for more robust visual instruction tuning, where the negative instructions include: 1) ’Nonexistent Object Manipulation’: introducing nonexistent objects, activities, attributes, and interactions; 2) ’Existent Object Manipulation’: manipulating existent objects with inconsistent attributes; 3) ’Knowledge Manipulation’: manipulating knowledge in instructions.\\nHalluciDoctor [117] This paper addresses the object hallucination problem in MLLMs by calibrating the instruction-tuning dataset.\\nThe calibration is conducted from two perspectives.\\nFirstly, it develops a hallucination detection pipeline via consistency cross-checking of multiple MLLMs.\\nBased on the detection result, the hallucinated content can be eliminated.\\nSecondly, this work observes that long-tail distribution and object co-occurrence in the training data are two primary factors of hallucination.\\nThus, a counterfactual visual instruction generation strategy is proposed to expand the dataset.\\nUsing the proposed methods, the instruction tuning data can be balanced and experience reduced hallucination.\\nMLLMs trained on the calibrated dataset are shown to be less prone to hallucination.\\nReCaption [105] This work proposes a framework called ReCaption to rewrite the text captions of existing image-text pairs in datasets.\\nThe framework comprises two steps: 1) keyword extraction, which extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which employs an LLM to generate sentences based on the extracted keywords.\\nUltimately, the framework produces a set of high-quality image-caption pairs.\\nExperiment results show that the model trained on the rewritten caption dataset has higher accuracy on certain benchmarks, such as the POPE benchmark [69].\\nDespite the performance improvement, the question of why rewritten captions can reduce hallucination remains an open problem.\\nEOS Decision [120] Previous work [137] provides an observation that hallucination tends to occur with objects positioned later in the generated descriptions.\\nIntuitively, an ideal scenario is that the MLLM can terminate the generation process in a timely manner.\\nThis idea is thoroughly explored in the work of [120] from the perspective of end-of-sequence (EOS) decision.\\nThe key insight is that the training data may exceed the perception limit of the MLLM.\\nWhen trained with such data, the model may attempt to fit the detail level and length distribution of ground truth captions.\\nHowever, it may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThus, the authors explored approaches to enhance the model’s end-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the perception limit.\\nRegarding data, this work proposes a data filtering strategy to eliminate harmful training data that could impair the model’s ability to end sequences.',\n",
       "   'title': '5.1 Data',\n",
       "   'chunks': [{'text': 'As discussed in the section on hallucination causes 3, data is one of the primary factors inducing hallucination in MLLMs.\\nFor mitigating hallucination, recent works make attempts on data, including introducing negative data [73], introducing counterfactual data [117], and reducing noise and errors in existing dataset [105, 120].',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': '5.1 Data'},\n",
       "    {'text': 'LRV-Instruction [73] LRV-Instruction is proposed to address the issue that existing instruction tuning data primarily focus on positive instruction samples, leading the model to consistently answer ’Yes’.\\nLRV-Instruction is designed to include both positive and negative instructions for more robust visual instruction tuning, where the negative instructions include: 1) ’Nonexistent Object Manipulation’: introducing nonexistent objects, activities, attributes, and interactions; 2) ’Existent Object Manipulation’: manipulating existent objects with inconsistent attributes; 3) ’Knowledge Manipulation’: manipulating knowledge in instructions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': '5.1 Data'},\n",
       "    {'text': 'HalluciDoctor [117] This paper addresses the object hallucination problem in MLLMs by calibrating the instruction-tuning dataset.\\nThe calibration is conducted from two perspectives.\\nFirstly, it develops a hallucination detection pipeline via consistency cross-checking of multiple MLLMs.\\nBased on the detection result, the hallucinated content can be eliminated.\\nSecondly, this work observes that long-tail distribution and object co-occurrence in the training data are two primary factors of hallucination.\\nThus, a counterfactual visual instruction generation strategy is proposed to expand the dataset.\\nUsing the proposed methods, the instruction tuning data can be balanced and experience reduced hallucination.\\nMLLMs trained on the calibrated dataset are shown to be less prone to hallucination.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': '5.1 Data'},\n",
       "    {'text': 'ReCaption [105] This work proposes a framework called ReCaption to rewrite the text captions of existing image-text pairs in datasets.\\nThe framework comprises two steps: 1) keyword extraction, which extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which employs an LLM to generate sentences based on the extracted keywords.\\nUltimately, the framework produces a set of high-quality image-caption pairs.\\nExperiment results show that the model trained on the rewritten caption dataset has higher accuracy on certain benchmarks, such as the POPE benchmark [69].\\nDespite the performance improvement, the question of why rewritten captions can reduce hallucination remains an open problem.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 14,\n",
       "     'source_doc': '5.1 Data'},\n",
       "    {'text': 'EOS Decision [120] Previous work [137] provides an observation that hallucination tends to occur with objects positioned later in the generated descriptions.\\nIntuitively, an ideal scenario is that the MLLM can terminate the generation process in a timely manner.\\nThis idea is thoroughly explored in the work of [120] from the perspective of end-of-sequence (EOS) decision.\\nThe key insight is that the training data may exceed the perception limit of the MLLM.\\nWhen trained with such data, the model may attempt to fit the detail level and length distribution of ground truth captions.\\nHowever, it may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThus, the authors explored approaches to enhance the model’s end-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the perception limit.\\nRegarding data, this work proposes a data filtering strategy to eliminate harmful training data that could impair the model’s ability to end sequences.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.1 Data',\n",
       "     'page': 15,\n",
       "     'source_doc': '5.1 Data'}],\n",
       "   'tables': []},\n",
       "  {'text': '5.2 Model\\n5.2.1 Scale-up Resolution\\nEnhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.\\n5.2.2 Versatile Vision Encoders\\nSeveral studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.\\n5.2.3 Dedicated Module\\nFollowing our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.',\n",
       "   'title': '5.2 Model',\n",
       "   'chunks': [{'text': 'Enhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.1 Scale-up Resolution',\n",
       "     'page': 15,\n",
       "     'source_doc': '5.2 Model'},\n",
       "    {'text': 'Several studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.2 Versatile Vision Encoders',\n",
       "     'page': 15,\n",
       "     'source_doc': '5.2 Model'},\n",
       "    {'text': 'Following our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.3 Dedicated Module',\n",
       "     'page': 16,\n",
       "     'source_doc': '5.2 Model'}],\n",
       "   'tables': []},\n",
       "  {'text': '5.2.1 Scale-up Resolution\\nEnhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.',\n",
       "   'title': '5.2.1 Scale-up Resolution',\n",
       "   'chunks': [{'text': 'Enhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.1 Scale-up Resolution',\n",
       "     'page': 15,\n",
       "     'source_doc': '5.2.1 Scale-up Resolution'}],\n",
       "   'tables': []},\n",
       "  {'text': '5.2.2 Versatile Vision Encoders\\nSeveral studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.',\n",
       "   'title': '5.2.2 Versatile Vision Encoders',\n",
       "   'chunks': [{'text': 'Several studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.2 Versatile Vision Encoders',\n",
       "     'page': 15,\n",
       "     'source_doc': '5.2.2 Versatile Vision Encoders'}],\n",
       "   'tables': []},\n",
       "  {'text': '5.2.3 Dedicated Module\\nFollowing our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.',\n",
       "   'title': '5.2.3 Dedicated Module',\n",
       "   'chunks': [{'text': 'Following our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.2 Model > 5.2.3 Dedicated Module',\n",
       "     'page': 16,\n",
       "     'source_doc': '5.2.3 Dedicated Module'}],\n",
       "   'tables': []},\n",
       "  {'text': '5.3 Training\\n5.3.1 Auxiliary supervision.\\nThe primary supervision signal of training MLLMs is language modeling loss (implemented as CrossEntropyLoss) in both pre-training and finetuning stage.\\nHowever, such supervision may not be sufficient to process the rich information encoded in the visual content.\\nAccordingly, the work of [16] constructs a fine-grained vision instruction dataset based on Panoptic Scene Graph (PSG), called Relation-Associated Instruction (RAI-30k).\\nIn addition to standard dialogues, each instruction in RAI-30k is associated with a relation annotation in PSG, which includes mask annotations for related instances.\\nWith these additional annotations, it further supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [57], guiding MLLMs to focus on highly-related image content.\\nWith the additional supervision from the mask prediction loss, MLLMs are encouraged to extract features that can better represent these crucial instances, thus generating more accurate responses and mitigating vision hallucination.\\nThe intuitive idea of supervising MLLMs with grounding shows promising performance in mitigating hallucination.\\nAnother line of work analyzes the training loss from the perspective of embedding space distribution.\\nAs introduced earlier, popular MLLMs typically project the encoded vision features into the input space of a specific LLM.\\nA recent work, HACL [52], argues that an ideal projection should blend the distribution of visual and textual embeddings.\\nHowever, despite visual projection, a significant modality gap exists between textual and visual tokens, suggesting that the current learned interfaces are not effective in mapping visual representations into the textual representation space of LLMs.\\nThis issue potentially exacerbates the tendency for MLLMs to generate more hallucinations.\\nTherefore, HACL proposes enhancing the alignment between visual and textual representations through contrastive loss.\\nTexts with hallucinations are used as hard negative examples for image anchors.\\nThe loss pulls representations of non-hallucinating text and visual samples closer while pushing representations of non-hallucinating and hallucinative text apart.\\nExperiment results show that this method not only reduces hallucination but also enhances performance on other popular benchmarks.\\nRecalling the work of EOS Decision [120], to teach the model to terminate the generation process properly, this work also designs a learning objective, termed Selective EOS Supervision, in addition to the data filtering strategy.\\nThis is achieved by simply modifying the Maximum Likelihood Estimation (MLE), enabling the model to mitigate hallucination through learning from regular instruction data.\\n5.3.2 Reinforcement Learning\\nReinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.\\nAutomatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.\\nReinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.\\nA concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.\\nReinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct\\nPreference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.\\nLLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.\\nSimilarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.\\nAnother similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.\\n5.3.3 Unlearning\\nUnlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.',\n",
       "   'title': '5.3 Training',\n",
       "   'chunks': [{'text': '5.3.1 Auxiliary supervision.\\nThe primary supervision signal of training MLLMs is language modeling loss (implemented as CrossEntropyLoss) in both pre-training and finetuning stage.\\nHowever, such supervision may not be sufficient to process the rich information encoded in the visual content.\\nAccordingly, the work of [16] constructs a fine-grained vision instruction dataset based on Panoptic Scene Graph (PSG), called Relation-Associated Instruction (RAI-30k).\\nIn addition to standard dialogues, each instruction in RAI-30k is associated with a relation annotation in PSG, which includes mask annotations for related instances.\\nWith these additional annotations, it further supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [57], guiding MLLMs to focus on highly-related image content.\\nWith the additional supervision from the mask prediction loss, MLLMs are encouraged to extract features that can better represent these crucial instances, thus generating more accurate responses and mitigating vision hallucination.\\nThe intuitive idea of supervising MLLMs with grounding shows promising performance in mitigating hallucination.\\nAnother line of work analyzes the training loss from the perspective of embedding space distribution.\\nAs introduced earlier, popular MLLMs typically project the encoded vision features into the input space of a specific LLM.\\nA recent work, HACL [52], argues that an ideal projection should blend the distribution of visual and textual embeddings.\\nHowever, despite visual projection, a significant modality gap exists between textual and visual tokens, suggesting that the current learned interfaces are not effective in mapping visual representations into the textual representation space of LLMs.\\nThis issue potentially exacerbates the tendency for MLLMs to generate more hallucinations.\\nTherefore, HACL proposes enhancing the alignment between visual and textual representations through contrastive loss.\\nTexts with hallucinations are used as hard negative examples for image anchors.\\nThe loss pulls representations of non-hallucinating text and visual samples closer while pushing representations of non-hallucinating and hallucinative text apart.\\nExperiment results show that this method not only reduces hallucination but also enhances performance on other popular benchmarks.\\nRecalling the work of EOS Decision [120], to teach the model to terminate the generation process properly, this work also designs a learning objective, termed Selective EOS Supervision, in addition to the data filtering strategy.\\nThis is achieved by simply modifying the Maximum Likelihood Estimation (MLE), enabling the model to mitigate hallucination through learning from regular instruction data.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training',\n",
       "     'page': 16,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'Reinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 16,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'Automatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'Reinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'A concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'A more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'Reinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'Preference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'LLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'Similarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'Another similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.3 Training'},\n",
       "    {'text': 'Unlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.3 Unlearning',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.3 Training'}],\n",
       "   'tables': []},\n",
       "  {'text': '5.3.2 Reinforcement Learning\\nReinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.\\nAutomatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.\\nReinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.\\nA concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.\\nReinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct\\nPreference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.\\nLLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.\\nSimilarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.\\nAnother similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.',\n",
       "   'title': '5.3.2 Reinforcement Learning',\n",
       "   'chunks': [{'text': 'Reinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 16,\n",
       "     'source_doc': '5.3.2 Reinforcement Learning'},\n",
       "    {'text': 'Automatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '5.3.2 Reinforcement Learning'},\n",
       "    {'text': 'Reinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '5.3.2 Reinforcement Learning'},\n",
       "    {'text': 'A concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '5.3.2 Reinforcement Learning'},\n",
       "    {'text': 'A more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '5.3.2 Reinforcement Learning'},\n",
       "    {'text': 'Reinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 17,\n",
       "     'source_doc': '5.3.2 Reinforcement Learning'},\n",
       "    {'text': 'Preference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.3.2 Reinforcement Learning'},\n",
       "    {'text': 'LLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.3.2 Reinforcement Learning'},\n",
       "    {'text': 'Similarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.3.2 Reinforcement Learning'},\n",
       "    {'text': 'Another similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.2 Reinforcement Learning',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.3.2 Reinforcement Learning'}],\n",
       "   'tables': []},\n",
       "  {'text': '5.3.3 Unlearning\\nUnlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.',\n",
       "   'title': '5.3.3 Unlearning',\n",
       "   'chunks': [{'text': 'Unlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.3 Training > 5.3.3 Unlearning',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.3.3 Unlearning'}],\n",
       "   'tables': []},\n",
       "  {'text': '5.4 Inference\\n5.4.1 Generation Intervention.\\nContrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.\\nGuided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.\\nSimilarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.\\nHALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.\\nOthers.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.\\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.\\n5.4.2 Post-hoc Correction\\nPost-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].\\nWoodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.\\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.\\nSimilar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.\\nLogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.\\n6 CHALLENGES AND FUTURE DIRECTIONS\\nThe research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.\\n6.1 Data-centric Challenges and Innovations\\nThe reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.\\n6.2 Cross-modal Alignment and Consistency\\nThe key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.',\n",
       "   'title': '5.4 Inference',\n",
       "   'chunks': [{'text': 'Contrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'Guided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'Similarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'HALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'Others.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'Another interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'Post-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'Woodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'Another line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'Similar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'LogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': '6 CHALLENGES AND FUTURE DIRECTIONS',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'The research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': '6.1 Data-centric Challenges and Innovations',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'The reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': '6.2 Cross-modal Alignment and Consistency',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 21,\n",
       "     'source_doc': '5.4 Inference'},\n",
       "    {'text': 'The key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 21,\n",
       "     'source_doc': '5.4 Inference'}],\n",
       "   'tables': []},\n",
       "  {'text': '5.4.1 Generation Intervention.\\nContrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.\\nGuided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.\\nSimilarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.\\nHALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.\\nOthers.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.\\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.',\n",
       "   'title': '5.4.1 Generation Intervention.',\n",
       "   'chunks': [{'text': 'Contrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 18,\n",
       "     'source_doc': '5.4.1 Generation Intervention.'},\n",
       "    {'text': 'Guided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '5.4.1 Generation Intervention.'},\n",
       "    {'text': 'Similarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '5.4.1 Generation Intervention.'},\n",
       "    {'text': 'HALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '5.4.1 Generation Intervention.'},\n",
       "    {'text': 'Others.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '5.4.1 Generation Intervention.'},\n",
       "    {'text': 'Another interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.1 Generation Intervention.',\n",
       "     'page': 19,\n",
       "     'source_doc': '5.4.1 Generation Intervention.'}],\n",
       "   'tables': []},\n",
       "  {'text': '5.4.2 Post-hoc Correction\\nPost-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].\\nWoodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.\\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.\\nSimilar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.\\nLogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.\\n6 CHALLENGES AND FUTURE DIRECTIONS\\nThe research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.\\n6.1 Data-centric Challenges and Innovations\\nThe reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.\\n6.2 Cross-modal Alignment and Consistency\\nThe key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.',\n",
       "   'title': '5.4.2 Post-hoc Correction',\n",
       "   'chunks': [{'text': 'Post-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'},\n",
       "    {'text': 'Woodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'},\n",
       "    {'text': 'Another line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'},\n",
       "    {'text': 'Similar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'},\n",
       "    {'text': 'LogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'},\n",
       "    {'text': '6 CHALLENGES AND FUTURE DIRECTIONS',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'},\n",
       "    {'text': 'The research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'},\n",
       "    {'text': '6.1 Data-centric Challenges and Innovations',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'},\n",
       "    {'text': 'The reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 20,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'},\n",
       "    {'text': '6.2 Cross-modal Alignment and Consistency',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 21,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'},\n",
       "    {'text': 'The key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 5.4 Inference > 5.4.2 Post-hoc Correction',\n",
       "     'page': 21,\n",
       "     'source_doc': '5.4.2 Post-hoc Correction'}],\n",
       "   'tables': []},\n",
       "  {'text': '6.3 Advancements in Model Architecture\\nDespite recent advancements in model architectures of LLMs and MLLMs, designing effective architectures specifically tailored to hallucination remains a challenge.\\nDeveloping advanced model architectures capable of capturing complex linguistic structures and generating coherent and contextually relevant output based on input visual content is essential for improving the performance of MLLMs.\\nFuture research can explore innovative architectural designs based on identified causes of hallucination.\\nThis includes developing stronger visual perception models, innovative cross-modal interaction modules capable of transferring cross-modal information seamlessly, and novel large language model architectures faithful to input visual content and text instructions, etc.',\n",
       "   'title': '6.3 Advancements in Model Architecture',\n",
       "   'chunks': [{'text': 'Despite recent advancements in model architectures of LLMs and MLLMs, designing effective architectures specifically tailored to hallucination remains a challenge.\\nDeveloping advanced model architectures capable of capturing complex linguistic structures and generating coherent and contextually relevant output based on input visual content is essential for improving the performance of MLLMs.\\nFuture research can explore innovative architectural designs based on identified causes of hallucination.\\nThis includes developing stronger visual perception models, innovative cross-modal interaction modules capable of transferring cross-modal information seamlessly, and novel large language model architectures faithful to input visual content and text instructions, etc.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.3 Advancements in Model Architecture',\n",
       "     'page': 21,\n",
       "     'source_doc': '6.3 Advancements in Model Architecture'}],\n",
       "   'tables': []},\n",
       "  {'text': '6.4 Establishing Standardized Benchmarks\\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in assessing the degree of hallucination in MLLMs.\\nIn Table 1, it can be observed that there is a variety of evaluation benchmarks, but a lack of unified standards.\\nAmong them, one of the most popular benchmarks might be POPE [69], which employs a ’Yes-or-No’ evaluation protocol.\\nHowever, this binary-QA manner does not align with how humans use MLLMs.\\nAccordingly, some benchmarks specifically evaluate the hallucination of MLLMs in the (free-form) generative context.\\nYet, they often rely on external models, such as vision expert models or other LLMs, which limits their widespread application.\\nMoving forward, future research can investigate standardized benchmarks that are theoretically sound and easy to use.\\nOtherwise, research on methods to mitigate hallucinations may be built on an incorrect foundation.',\n",
       "   'title': '6.4 Establishing Standardized Benchmarks',\n",
       "   'chunks': [{'text': 'The lack of standardized benchmarks and evaluation metrics poses significant challenges in assessing the degree of hallucination in MLLMs.\\nIn Table 1, it can be observed that there is a variety of evaluation benchmarks, but a lack of unified standards.\\nAmong them, one of the most popular benchmarks might be POPE [69], which employs a ’Yes-or-No’ evaluation protocol.\\nHowever, this binary-QA manner does not align with how humans use MLLMs.\\nAccordingly, some benchmarks specifically evaluate the hallucination of MLLMs in the (free-form) generative context.\\nYet, they often rely on external models, such as vision expert models or other LLMs, which limits their widespread application.\\nMoving forward, future research can investigate standardized benchmarks that are theoretically sound and easy to use.\\nOtherwise, research on methods to mitigate hallucinations may be built on an incorrect foundation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.4 Establishing Standardized Benchmarks',\n",
       "     'page': 21,\n",
       "     'source_doc': '6.4 Establishing Standardized Benchmarks'}],\n",
       "   'tables': []},\n",
       "  {'text': '6.5 Reframing Hallucination as a Feature\\nRecently, discussions on social media [56] have suggested that hallucination can be regarded as an inherent feature of LLMs and MLLMs.\\nThe models are like dream machines.\\nHuman users direct their dreams with prompts.\\nThe prompts start the dream, and based on the model’s hazy recollection of its training documents, most of the time the result goes someplace useful.\\nIt’s only when the dreams enter deemed factually incorrect territory that we label them as ’hallucinations’.\\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications presents exciting opportunities for enhancing user experiences and enabling new use cases.\\nAs humans are the end-users of these models, the primary goal is to enrich human user experiences.\\nFuture research may switch the optimization objective from specific cross-modal benchmarks to human experience.\\nFor example, Some content may cause hallucinations but will not affect the user experience, while some content may.\\nAlternatively, integrating hallucination to inspire more creative ideas in real-world applications could also be intriguing.',\n",
       "   'title': '6.5 Reframing Hallucination as a Feature',\n",
       "   'chunks': [{'text': 'Recently, discussions on social media [56] have suggested that hallucination can be regarded as an inherent feature of LLMs and MLLMs.\\nThe models are like dream machines.\\nHuman users direct their dreams with prompts.\\nThe prompts start the dream, and based on the model’s hazy recollection of its training documents, most of the time the result goes someplace useful.\\nIt’s only when the dreams enter deemed factually incorrect territory that we label them as ’hallucinations’.\\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications presents exciting opportunities for enhancing user experiences and enabling new use cases.\\nAs humans are the end-users of these models, the primary goal is to enrich human user experiences.\\nFuture research may switch the optimization objective from specific cross-modal benchmarks to human experience.\\nFor example, Some content may cause hallucinations but will not affect the user experience, while some content may.\\nAlternatively, integrating hallucination to inspire more creative ideas in real-world applications could also be intriguing.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.5 Reframing Hallucination as a Feature',\n",
       "     'page': 21,\n",
       "     'source_doc': '6.5 Reframing Hallucination as a Feature'}],\n",
       "   'tables': []},\n",
       "  {'text': '6.6 Enhancing Interpretability and Trust\\nExisting methods for hallucination mitigation are primarily based on empirical observations of specific patterns, such as skipping the ‘\\\\n’ token and penalizing over-trust tokens.\\nHowever, despite the impressive improvements achieved on specific benchmarks, understanding the underlying mechanisms and decision-making processes remains challenging.\\nFuture research should focus on developing techniques for interpreting and explaining the generation process of MLLMs, thereby providing insights into the factors influencing hallucinated content.\\nThis includes investigating methods for visualizing model internals, identifying salient features and linguistic patterns, and tracing the generation process from input to output.\\nEnhancing the interpretability of MLLMs will not only improve our understanding of model behavior but also enable users to better assess hallucinated content in practical applications.',\n",
       "   'title': '6.6 Enhancing Interpretability and Trust',\n",
       "   'chunks': [{'text': 'Existing methods for hallucination mitigation are primarily based on empirical observations of specific patterns, such as skipping the ‘\\\\n’ token and penalizing over-trust tokens.\\nHowever, despite the impressive improvements achieved on specific benchmarks, understanding the underlying mechanisms and decision-making processes remains challenging.\\nFuture research should focus on developing techniques for interpreting and explaining the generation process of MLLMs, thereby providing insights into the factors influencing hallucinated content.\\nThis includes investigating methods for visualizing model internals, identifying salient features and linguistic patterns, and tracing the generation process from input to output.\\nEnhancing the interpretability of MLLMs will not only improve our understanding of model behavior but also enable users to better assess hallucinated content in practical applications.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.6 Enhancing Interpretability and Trust',\n",
       "     'page': 22,\n",
       "     'source_doc': '6.6 Enhancing Interpretability and Trust'}],\n",
       "   'tables': []},\n",
       "  {'text': '6.7 Navigating the Ethical Landscape\\nAs MLLMs become increasingly proficient at generating realistic text, ethical considerations surrounding the use of generated content become paramount.\\nEspecially in the context of hallucination, the generated response may contain severely concerning ethical content, amplifying the importance of the problem.\\nAddressing ethical concerns related to misinformation, bias, privacy, and societal impact is crucial for promoting responsible AI practices in the development and deployment of MLLMs.\\nIn addition to addressing typical object hallucination, future research on MLLM hallucinations should prioritize ethical considerations throughout the entire lifecycle of MLLM development, from data collection and model training to deployment and evaluation.\\n7 CONCLUSION\\nBased on powerful large language models, multimodal large language models demonstrate remarkable performance across various multimodal tasks.\\nHowever, the phenomenon of hallucination presents a significant challenge to the practical applications of MLLMs, giving rise to undeniable concerns about safety, reliability, and trustworthiness.\\nIn this comprehensive survey, we conducted a thorough examination of hallucinations within multimodal large language models, focusing on their underlying causes, evaluation metrics, benchmarks, and mitigation methods.\\nDespite considerable progress, hallucination remains a complex and persistent concern that warrants ongoing investigation.\\nThe challenge of hallucination in multimodal large language models remains compelling, requiring continuous scrutiny and innovation.\\nIn light of these challenges, we have outlined several promising future directions in this burgeoning domain.\\nThrough navigating the intricate landscape of hallucinations, we aim for this survey to serve as a foundational resource for addressing the complexities of hallucination phenomena in MLLMs.\\nWe envision this survey empowering researchers and practitioners to dedicate efforts to advancing research and developing robust solutions in this vital area of study.',\n",
       "   'title': '6.7 Navigating the Ethical Landscape',\n",
       "   'chunks': [{'text': 'As MLLMs become increasingly proficient at generating realistic text, ethical considerations surrounding the use of generated content become paramount.\\nEspecially in the context of hallucination, the generated response may contain severely concerning ethical content, amplifying the importance of the problem.\\nAddressing ethical concerns related to misinformation, bias, privacy, and societal impact is crucial for promoting responsible AI practices in the development and deployment of MLLMs.\\nIn addition to addressing typical object hallucination, future research on MLLM hallucinations should prioritize ethical considerations throughout the entire lifecycle of MLLM development, from data collection and model training to deployment and evaluation.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.7 Navigating the Ethical Landscape',\n",
       "     'page': 22,\n",
       "     'source_doc': '6.7 Navigating the Ethical Landscape'},\n",
       "    {'text': '7 CONCLUSION',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.7 Navigating the Ethical Landscape',\n",
       "     'page': 22,\n",
       "     'source_doc': '6.7 Navigating the Ethical Landscape'},\n",
       "    {'text': 'Based on powerful large language models, multimodal large language models demonstrate remarkable performance across various multimodal tasks.\\nHowever, the phenomenon of hallucination presents a significant challenge to the practical applications of MLLMs, giving rise to undeniable concerns about safety, reliability, and trustworthiness.\\nIn this comprehensive survey, we conducted a thorough examination of hallucinations within multimodal large language models, focusing on their underlying causes, evaluation metrics, benchmarks, and mitigation methods.\\nDespite considerable progress, hallucination remains a complex and persistent concern that warrants ongoing investigation.\\nThe challenge of hallucination in multimodal large language models remains compelling, requiring continuous scrutiny and innovation.\\nIn light of these challenges, we have outlined several promising future directions in this burgeoning domain.\\nThrough navigating the intricate landscape of hallucinations, we aim for this survey to serve as a foundational resource for addressing the complexities of hallucination phenomena in MLLMs.\\nWe envision this survey empowering researchers and practitioners to dedicate efforts to advancing research and developing robust solutions in this vital area of study.',\n",
       "     'title': 'INTRODUCTION > 3.4 Inference > 6.7 Navigating the Ethical Landscape',\n",
       "     'page': 22,\n",
       "     'source_doc': '6.7 Navigating the Ethical Landscape'}],\n",
       "   'tables': []},\n",
       "  {'text': 'ACKNOWLEDGMENTS\\nThis project is supported by Mike Zheng Shou’s Start-Up Grant from NUS.',\n",
       "   'title': 'ACKNOWLEDGMENTS',\n",
       "   'chunks': [{'text': 'This project is supported by Mike Zheng Shou’s Start-Up Grant from NUS.',\n",
       "     'title': 'ACKNOWLEDGMENTS',\n",
       "     'page': 23,\n",
       "     'source_doc': 'ACKNOWLEDGMENTS'}],\n",
       "   'tables': []},\n",
       "  {'text': 'REFERENCES\\n[1] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\\n2023.\\nOpenflamingo: An open-source framework for training large autoregressive vision-language models.\\narXiv preprint arXiv:2308.01390 (2023).\\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\\n2023.\\nQwen-vl: A frontier large vision-language model with versatile abilities.\\narXiv preprint arXiv:2308.12966 (2023).\\n[3] Satanjeev Banerjee and Alon Lavie.\\n2005.\\nMETEOR: An automatic metric for MT evaluation with improved correlation with human judgments.\\nIn Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization.\\n65–72.\\n[4] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar.\\n2023.\\nFuyu-8B: A Multimodal Architecture for AI Agents.\\nhttps://www.adept.ai/blog/fuyu-8b [5] Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, and Hadar Averbuch-Elor.\\n2023.\\nMOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations.\\narXiv preprint arXiv:2312.03631 (2023).\\n[6] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al.\\n2023.\\nImproving image generation with better captions.\\nComputer Science.\\nhttps://cdn.\\nopenai.\\ncom/papers/dall-e-3.\\npdf 2, 3 (2023), 8.\\n[7] Ralph Allan Bradley and Milton E Terry.\\n1952.\\nRank analysis of incomplete block designs: I. The method of paired comparisons.\\nBiometrika 39, 3/4 (1952), 324–345.\\nhttps://www.jstor.org/stable/2334029 [8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\\n2020.\\nLanguage Models are Few-Shot Learners.\\nIn Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html [9] Yinzhi Cao and Junfeng Yang.\\n2015.\\nTowards making systems forget with machine unlearning.\\nIn 2015 IEEE symposium on security and privacy.\\nIEEE, 463–480.\\n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\\n2021.\\nEmerging properties in self-supervised vision transformers.\\nIn Proceedings of the IEEE/CVF international conference on computer vision.\\n9650–9660.\\n[11] Sungguk Cha, Jusung Lee, Younghyun Lee, and Cheoljong Yang.\\n2024.\\nVisually Dehallucinative Instruction Generation:\\nKnow What You Don’t Know.\\narXiv preprint arXiv:2402.09717 (2024).\\n[12] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.\\n2023.\\nShikra: Unleashing Multimodal LLM’s Referential Dialogue Magic.\\narXiv preprint arXiv:2306.15195 (2023).\\n[13] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Jinjie Gu, and Huajun Chen.\\n2024.\\nUnified Hallucination Detection for Multimodal Large Language Models.\\narXiv preprint arXiv:2402.03190 (2024).\\n[14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al.\\n2023.\\nInternvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.\\narXiv preprint arXiv:2312.14238 (2023).\\n[15] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou.\\n2024.\\nHALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding.\\narXiv preprint arXiv:2403.00425 (2024).\\n[16] Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, JinqiaoWang, andMing Tang.\\n2023.\\nMitigating Hallucination in Visual Language Models with Visual Supervision.\\narXiv preprint arXiv:2311.16479 (2023).\\n[17] Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi PontTuset, and Su Wang.\\n2023.\\nDavidsonian scene graph: Improving reliability in fine-grained evaluation for text-image generation.\\narXiv preprint arXiv:2310.18235 (2023).\\n[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,\\nLiam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.\\n2023.\\nPaLM: Scaling Language Modeling with Pathways.\\nJ. Mach.\\nLearn.\\nRes.\\n24 (2023), 240:1–240:113. http://jmlr.org/papers/v24/22-1144.html [19] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\\n2017.\\nDeep Reinforcement Learning from Human Preferences.\\nIn Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.).\\n4299–4307.\\nhttps://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html [20] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu.\\n2023.\\nA Survey of Chain of Thought Reasoning: Advances, Frontiers and Future.\\nArXiv preprint abs/2309.15402 (2023).\\nhttps://arxiv.org/abs/2309.15402 [21] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao.\\n2023.\\nHolistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges.\\narXiv preprint arXiv:2311.03287 (2023).\\n[22] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.\\n2023.\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.\\narXiv:2305.06500 [cs.CV] [23] Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al.\\n2023.\\nLanguage modeling is compression.\\narXiv preprint arXiv:2309.10668 (2023).\\n[24] Ailin Deng, Zhirui Chen, and BryanHooi.\\n2024.\\nSeeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding.\\narXiv preprint arXiv:2402.15300 (2024).\\n[25] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\\n2022.\\nGLM: General Language Model Pretraining with Autoregressive Blank Infilling.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).\\n320–335.\\n[26] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al.\\n2023.\\nLlama-adapter v2: Parameter-efficient visual instruction model.\\narXiv preprint arXiv:2304.15010 (2023).\\n[27] Ross Girshick.\\n2015.\\nFast r-cnn.\\nIn Proceedings of the IEEE international conference on computer vision.\\n1440–1448.\\n[28] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.\\n2023.\\nMultiModal-GPT: A Vision and Language Model for Dialogue with Humans.\\narXiv:2305.04790 [cs.CV] [29] Google.\\n2023.\\nBard.\\nhttps://bard.google.com/ [30] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.\\n2017.\\nMaking the v in vqa matter: Elevating the role of image understanding in visual question answering.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition.\\n6904–6913.\\n[31] Tianrui Guan, Fuxiao Liu, Xiyang Wu Ruiqi Xian Zongxia Li, Xiaoyu Liu Xijun Wang, Lichang Chen Furong Huang Yaser Yacoob, and Dinesh Manocha Tianyi Zhou.\\n2023.\\nHALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination & Visual Illusion in Large Vision-Language Models.\\narXiv e-prints (2023), arXiv–2310.\\n[32] Anisha Gunjal, Jihan Yin, and Erhan Bas.\\n2023.\\nDetecting and preventing hallucinations in large vision language models.\\nArXiv preprint abs/2308.06394 (2023).\\nhttps://arxiv.org/abs/2308.06394 [33] Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji.\\n2023.\\nLm-switch: Lightweight language model conditioning in word embedding space.\\narXiv preprint arXiv:2305.12798 (2023).\\n[34] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al.\\n2023.\\nImagebind-llm: Multi-modality instruction tuning.\\narXiv preprint arXiv:2309.03905 (2023).\\n[35] Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, and Tong Zhang.\\n2024.\\nThe Instinctive Bias: Spurious Images lead to Hallucination in MLLMs.\\narXiv preprint arXiv:2402.03757 (2024).\\n[36] Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, and Mike Zheng Shou.\\n2024.\\nSkip \\\\𝑛: A simple method to reduce hallucination in Large Vision-Language Models.\\narXiv preprint arXiv:2402.01345 (2024).\\n[37] KaimingHe, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.\\n2017.\\nMask r-cnn.\\nIn Proceedings of the IEEE international conference on computer vision.\\n2961–2969.\\n[38] Xin He, Longhui Wei, Lingxi Xie, and Qi Tian.\\n2024.\\nIncorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models.\\narXiv preprint arXiv:2401.03105 (2024).\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021.\\nMeasuring Massive Multitask Language Understanding.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nhttps://openreview.net/forum?id=d7KBjmI3GmQ [40] Jonathan Ho and Tim Salimans.\\n2022.\\nClassifier-free diffusion guidance.\\narXiv preprint arXiv:2207.12598 (2022).\\n[41] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021.\\nLora: Low-rank adaptation of large language models.\\narXiv preprint arXiv:2106.09685 (2021).\\n[42] Hongyu Hu, Jiyuan Zhang, Minyi Zhao, and Zhenbang Sun.\\n2023.\\nCiem: Contrastive instruction evaluation method for better instruction tuning.\\narXiv preprint arXiv:2309.02301 (2023).\\n[43] Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, and Zhuowen Tu.\\n2023.\\nBliva: A simple multimodal llm for better handling of text-rich visual questions.\\narXiv preprint arXiv:2308.09936 (2023).\\n[44] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al.\\n2023.\\nA survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.\\narXiv preprint arXiv:2311.05232 (2023).\\n[45] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu.\\n2023.\\nOPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation.\\narXiv preprint arXiv:2311.17911 (2023).\\n[46] Wen Huang, Hongbin Liu, Minxin Guo, and Neil Zhenqiang Gong.\\n2024.\\nVisual Hallucinations of Multi-modal Large Language Models.\\narXiv preprint arXiv:2402.14683 (2024).\\n[47] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al.\\n2023.\\nC-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.\\nArXiv preprint abs/2305.08322 (2023).\\nhttps://arxiv.org/abs/2305.08322 [48] Drew A Hudson and Christopher D Manning.\\n2019.\\nGqa: A new dataset for real-world visual reasoning and compositional question answering.\\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\\n6700–6709.\\n[49] Jitesh Jain, Jianwei Yang, and Humphrey Shi.\\n2023.\\nVcoder: Versatile vision encoders for multimodal large language models.\\narXiv preprint arXiv:2312.14233 (2023).\\n[50] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo.\\n2022.\\nKnowledge unlearning for mitigating privacy risks in language models.\\narXiv preprint arXiv:2210.01504 (2022).\\n[51] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.\\n2021.\\nScaling up visual and vision-language representation learning with noisy text supervision.\\nIn International conference on machine learning.\\nPMLR, 4904–4916.\\n[52] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang.\\n2023.\\nHallucination Augmented Contrastive Learning for Multimodal Large Language Model.\\narXiv preprint arXiv:2312.06968 (2023).\\n[53] Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang.\\n2024.\\nHal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models.\\narXiv preprint arXiv:2402.15721 (2024).\\n[54] Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, and Ying Shen.\\n2024.\\nEnhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study.\\narXiv preprint arXiv:2401.17981 (2024).\\n[55] Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du.\\n2023.\\nFAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models.\\narXiv preprint arXiv:2311.01477 (2023).\\n[56] Andrej Karpathy.\\n2023.\\nOn the \"hallucination problem\".\\nhttps://twitter.com/karpathy/status/1733299213503787018 Access Date: 10 Mar.\\n2024.\\n[57] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.\\n2023.\\nSegment anything.\\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision.\\n4015–4026.\\n[58] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\\n2022.\\nLarge language models are zero-shot reasoners.\\nAdvances in neural information processing systems 35 (2022), 22199–22213.\\n[59] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.\\n2017.\\nVisual genome: Connecting language and vision using crowdsourced dense image annotations.\\nInternational journal of computer vision 123 (2017), 32–73.\\n[60] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\\n2012.\\nImagenet classification with deep convolutional neural networks.\\nAdvances in neural information processing systems 25 (2012).\\n[61] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al.\\n2020.\\nThe open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.\\nInternational journal of computer vision 128, 7 (2020), 1956–1981.\\nXin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.\\n2023.\\nLisa: Reasoning segmentation via large language model.\\narXiv preprint arXiv:2308.00692 (2023).\\n[63] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo.\\n2023.\\nVolcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision.\\narXiv preprint arXiv:2311.07362 (2023).\\n[64] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing.\\n2023.\\nMitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding.\\narXiv preprint arXiv:2311.16922 (2023).\\n[65] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.\\n2023.\\nOtter: A Multi-Modal Model with In-Context Instruction Tuning.\\narXiv preprint arXiv:2305.03726 (2023).\\n[66] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\\n2023.\\nBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\\narXiv preprint arXiv:2301.12597 (2023).\\n[67] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, and Yueting Zhuang.\\n2023.\\nFine-tuning multimodal llms to follow zero-shot demonstrative instructions.\\nIn The Twelfth International Conference on Learning Representations.\\n[68] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong.\\n2023.\\nSilkie: Preference distillation for large visual language models.\\narXiv preprint arXiv:2312.10665 (2023).\\n[69] Yifan Li, Yifan Du, Kun Zhou, JinpengWang, Wayne Xin Zhao, and Ji-RongWen.\\n2023.\\nEvaluating object hallucination in large vision-language models.\\narXiv preprint arXiv:2305.10355 (2023).\\n[70] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.\\n2014.\\nMicrosoft coco: Common objects in context.\\nIn Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13.\\nSpringer, 740–755.\\n[71] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al.\\n2023.\\nSphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.\\narXiv preprint arXiv:2311.07575 (2023).\\n[72] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou.\\n2023.\\nHallusionBench: You See What You Think?\\nOr You Think What You See?\\nAn Image-Context Reasoning Benchmark Challenging for GPT-4V(Ision), LLaVA-1.5, and Other Multi-Modality Models.\\nhttps://arxiv.org/abs/2310.14566 [73] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.\\n2023.\\nMitigating hallucination in large multi-modal models via robust instruction tuning.\\narXiv preprint arXiv:2306.14565 1 (2023).\\n[74] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\n2023.\\nImproved baselines with visual instruction tuning.\\narXiv preprint arXiv:2310.03744 (2023).\\n[75] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\n2023.\\nVisual instruction tuning.\\narXiv preprint arXiv:2304.08485 (2023).\\n[76] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng.\\n2024.\\nA survey on hallucination in large vision-language models.\\narXiv preprint arXiv:2402.00253 (2024).\\n[77] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung.\\n2023.\\nNegative object presence evaluation (nope) to measure object hallucination in vision-language models.\\narXiv preprint arXiv:2310.05338 (2023).\\n[78] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji.\\n2024.\\nCheap and quick: Efficient vision-language instruction tuning for large language models.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n[79] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna.\\n2023.\\nCREPE: Can VisionLanguage Foundation Models Reason Compositionally?.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n10910–10921.\\n[80] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.\\n2019.\\nOcr-vqa: Visual question answering by reading text in images.\\nIn 2019 international conference on document analysis and recognition (ICDAR).\\nIEEE, 947–952.\\n[81] OpenAI.\\n2022.\\nIntroducing chatgpt.\\nhttps://openai.com/blog/chatgpt [82] OpenAI.\\n2023.\\nGPT-4 Technical Report.\\nArXiv preprint abs/2303.08774 (2023).\\nhttps://arxiv.org/abs/2303.08774 [83] OpenAI.\\n2023.\\nGPT-4V(ision) System Card.\\nhttps://cdn.openai.com/papers/GPTV_System_Card.pdf [84] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022.\\nTraining language models to follow instructions with human feedback.\\nIn NeurIPS.\\nhttp://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html [85] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\\n2023.\\nThe RefinedWeb Dataset for Falcon LLM:\\nOutperforming Curated Corpora with Web Data, and Web Data Only.\\nArXiv preprint abs/2306.01116 (2023).\\nhttps://arxiv.org/abs/2306.01116 [86] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.\\n2023.\\nInstruction tuning with gpt-4.\\nArXiv preprint abs/2304.03277 (2023).\\nhttps://arxiv.org/abs/2304.03277 [87] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen.\\n2022.\\nReasoning with language model prompting: A survey.\\nArXiv preprint abs/2212.09597 (2022).\\nhttps://arxiv.org/abs/2212.09597 [88] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\\n2021.\\nLearning transferable visual models from natural language supervision.\\nIn International conference on machine learning.\\nPMLR, 8748–8763.\\n[89] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\\n2022.\\nHierarchical text-conditional image generation with clip latents.\\narXiv preprint arXiv:2204.06125 1, 2 (2022), 3.\\n[90] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko.\\n2018.\\nObject hallucination in image captioning.\\narXiv preprint arXiv:1809.02156 (2018).\\n[91] Anna Rohrbach, Makarand Tapaswi, Atousa Torabi, Tegan Maharaj, Marcus Rohrbach, Sanja Fidler Christopher Pal, and Bernt Schiele.\\n2017.\\nThe Joint Video and Language Understanding Workshop: MovieQA and The Large Scale Movie Description Challenge (LSMDC).\\n[92] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\\n2022.\\nLaion-5b: An open large-scale dataset for training next generation image-text models.\\nAdvances in Neural Information Processing Systems 35 (2022), 25278–25294.\\n[93] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\n2017.\\nProximal Policy Optimization Algorithms.\\nArXiv preprint abs/1707.06347 (2017).\\nhttps://arxiv.org/abs/1707.06347 [94] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano.\\n2020.\\nLearning to summarize with human feedback.\\nIn Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html [95] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai.\\n2023.\\nPandagpt: One model to instruction-follow them all.\\narXiv preprint arXiv:2305.16355 (2023).\\n[96] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al.\\n2023.\\nAligning large multimodal models with factually augmented rlhf.\\narXiv preprint arXiv:2309.14525 (2023).\\n[97] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.\\n2023.\\nGemini: a family of highly capable multimodal models.\\narXiv preprint arXiv:2312.11805 (2023).\\n[98] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie.\\n2024.\\nEyes Wide Shut?\\nExploring the Visual Shortcomings of Multimodal LLMs.\\narXiv preprint arXiv:2401.06209 (2024).\\n[99] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\n2023.\\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\\nArXiv preprint abs/2307.09288 (2023).\\nhttps://arxiv.org/abs/2307.09288 [100] Andrés Villa, Juan Carlos León Alcázar, Alvaro Soto, and Bernard Ghanem.\\n2023.\\nBehind the Magic, MERLIM:\\nMulti-modal Evaluation Benchmark for Large Image-Language Models.\\narXiv preprint arXiv:2312.02219 (2023).\\n[101] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.\\n2008.\\nExtracting and composing robust features with denoising autoencoders.\\nIn Proceedings of the 25th international conference on Machine learning.\\n1096–1103.\\n[102] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et al.\\n2023.\\nVigc: Visual instruction generation and correction.\\narXiv preprint arXiv:2308.12714 (2023).\\n[103] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang.\\n2023.\\nAn llm-free multi-dimensional benchmark for mllms hallucination evaluation.\\narXiv preprint arXiv:2311.07397 (2023).\\nJunyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al.\\n2023.\\nEvaluation and analysis of hallucination in large vision-language models.\\narXiv preprint arXiv:2308.15126 (2023).\\n[105] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim.\\n2023.\\nMitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites.\\narXiv preprint arXiv:2312.01701 (2023).\\n[106] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al.\\n2023.\\nCogvlm: Visual expert for pretrained language models.\\narXiv preprint arXiv:2311.03079 (2023).\\n[107] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\\n2022.\\nChain-of-thought prompting elicits reasoning in large language models.\\nAdvances in Neural Information Processing Systems 35 (2022), 24824–24837.\\n[108] Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, and Tieniu Tan.\\n2024.\\nLogical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models.\\narXiv preprint arXiv:2402.11622 (2024).\\n[109] Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, and Xinyu Dai.\\n2024.\\nEFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models.\\narXiv preprint arXiv:2402.09801 (2024).\\n[110] Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, and Li Erran Li.\\n2024.\\nViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling.\\narXiv preprint arXiv:2402.06118 (2024).\\n[111] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al.\\n2023. mplug-owl: Modularization empowers large language models with multimodality.\\narXiv preprint arXiv:2304.14178 (2023).\\n[112] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.\\n2023. mplugowl2: Revolutionizing multi-modal large language model with modality collaboration.\\narXiv preprint arXiv:2311.04257 (2023).\\n[113] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.\\n2023.\\nA survey on multimodal large language models.\\narXiv preprint arXiv:2306.13549 (2023).\\n[114] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen.\\n2023.\\nWoodpecker: Hallucination Correction for Multimodal Large Language Models.\\nhttps: //arxiv.org/abs/2310.16045 [115] Fei Yu, Hongbo Zhang, and Benyou Wang.\\n2023.\\nNature language reasoning, a survey.\\nArXiv preprint abs/2303.14725 (2023).\\nhttps://arxiv.org/abs/2303.14725 [116] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.\\n[n.\\nd.].\\nCoca: Contrastive captioners are image-text foundation models.\\narXiv 2022. arXiv preprint arXiv:2205.01917 ([n. d.]).\\n[117] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang.\\n2023.\\nHalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data.\\narXiv preprint arXiv:2311.13614 (2023).\\n[118] Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, Chongyi Wang, Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, et al.\\n2023.\\nReformulating vision-language foundation models and datasets towards universal multimodal assistants.\\narXiv preprint arXiv:2310.00653 (2023).\\n[119] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al.\\n2023.\\nRLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback.\\narXiv preprint arXiv:2312.00849 (2023).\\n[120] Zihao Yue, Liang Zhang, and Qin Jin.\\n2024.\\nLess is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective.\\narXiv preprint arXiv:2402.14545 (2024).\\n[121] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou.\\n2022.\\nWhen and Why VisionLanguage Models Behave like Bags-Of-Words, and What to Do About It?.\\nIn The Eleventh International Conference on Learning Representations.\\n[122] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong.\\n2023.\\nWhat matters in training a gpt4-style language model with multimodal inputs?\\narXiv preprint arXiv:2307.02469 (2023).\\n[123] Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, and Manling Li.\\n2023.\\nHallE-Switch: Controlling Object Hallucination in Large Vision Language Models.\\narXiv e-prints (2023), arXiv–2310.\\n[124] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua.\\n2024.\\nVPGTrans: Transfer visual prompt generator across LLMs.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n[125] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al.\\n2023.\\nInstruction Tuning for Large Language Models: A Survey.\\nArXiv preprint abs/2308.10792 (2023).\\nhttps://arxiv.org/abs/2308.10792 Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.\\n2023.\\nGpt4roi: Instruction tuning large language model on region-of-interest.\\narXiv preprint arXiv:2307.03601 (2023).\\n[127] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.\\n2019.\\nBertscore: Evaluating text generation with bert.\\narXiv preprint arXiv:1904.09675 (2019).\\n[128] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto.\\n2023.\\nBenchmarking large language models for news summarization.\\nArXiv preprint abs/2301.13848 (2023).\\nhttps: //arxiv.org/abs/2301.13848 [129] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, LongyueWang, Anh Tuan Luu,Wei Bi, Freda Shi, and Shuming Shi.\\n2023.\\nSiren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.\\nArXiv preprint abs/2309.01219 (2023).\\nhttps://arxiv.org/abs/2309.01219 [130] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang.\\n2023.\\nMmicl: Empowering vision-language model with multi-modal in-context learning.\\narXiv preprint arXiv:2309.07915 (2023).\\n[131] Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu.\\n2024.\\nMitigating Object Hallucination in Large VisionLanguage Models via Classifier-Free Guidance.\\narXiv preprint arXiv:2402.08680 (2024).\\n[132] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.\\n2023.\\nA survey of large language models.\\nArXiv preprint abs/2303.18223 (2023).\\nhttps://arxiv.org/abs/2303.18223 [133] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He.\\n2023.\\nBeyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization.\\narXiv preprint arXiv:2311.16839 (2023).\\n[134] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al.\\n2023.\\nLima: Less is more for alignment.\\nArXiv preprint abs/2305.11206 (2023).\\nhttps://arxiv.org/abs/2305.11206 [135] Qiang Zhou, Zhibin Wang, Wei Chu, Yinghui Xu, Hao Li, and Yuan Qi.\\n2023.\\nInfMLLM: A Unified Framework for Visual-Language Tasks.\\narXiv:2311.06791 [cs.CV] [136] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao.\\n2024.\\nAligning Modalities in Vision Large Language Models via Preference Fine-tuning.\\narXiv preprint arXiv:2402.11411 (2024).\\n[137] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.\\n2023.\\nAnalyzing and mitigating object hallucination in large vision-language models.\\narXiv preprint arXiv:2310.00754 (2023).\\n[138] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\\n2023.\\nMinigpt-4: Enhancing vision-language understanding with advanced large language models.\\narXiv preprint arXiv:2304.10592 (2023).\\n[139] Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, and Jun Liu.\\n2024.\\nIBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding.\\narXiv preprint arXiv:2402.18476 (2024).\\n[140] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang.\\n2023.\\nMultilingual machine translation with large language models: Empirical results and analysis.\\nArXiv preprint abs/2304.04675 (2023).\\nhttps://arxiv.org/abs/2304.04675',\n",
       "   'title': 'REFERENCES',\n",
       "   'chunks': [{'text': '[1] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\\n2023.\\nOpenflamingo: An open-source framework for training large autoregressive vision-language models.\\narXiv preprint arXiv:2308.01390 (2023).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 23,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\\n2023.\\nQwen-vl: A frontier large vision-language model with versatile abilities.\\narXiv preprint arXiv:2308.12966 (2023).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 23,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[3] Satanjeev Banerjee and Alon Lavie.\\n2005.\\nMETEOR: An automatic metric for MT evaluation with improved correlation with human judgments.\\nIn Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization.\\n65–72.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 23,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[4] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar.\\n2023.\\nFuyu-8B: A Multimodal Architecture for AI Agents.\\nhttps://www.adept.ai/blog/fuyu-8b [5] Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, and Hadar Averbuch-Elor.\\n2023.\\nMOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations.\\narXiv preprint arXiv:2312.03631 (2023).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 23,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[6] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al.\\n2023.\\nImproving image generation with better captions.\\nComputer Science.\\nhttps://cdn.\\nopenai.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 23,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'com/papers/dall-e-3.\\npdf 2, 3 (2023), 8.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 23,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[7] Ralph Allan Bradley and Milton E Terry.\\n1952.\\nRank analysis of incomplete block designs: I. The method of paired comparisons.\\nBiometrika 39, 3/4 (1952), 324–345.\\nhttps://www.jstor.org/stable/2334029 [8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\\n2020.\\nLanguage Models are Few-Shot Learners.\\nIn Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html [9] Yinzhi Cao and Junfeng Yang.\\n2015.\\nTowards making systems forget with machine unlearning.\\nIn 2015 IEEE symposium on security and privacy.\\nIEEE, 463–480.\\n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\\n2021.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 23,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'Emerging properties in self-supervised vision transformers.\\nIn Proceedings of the IEEE/CVF international conference on computer vision.\\n9650–9660.\\n[11] Sungguk Cha, Jusung Lee, Younghyun Lee, and Cheoljong Yang.\\n2024.\\nVisually Dehallucinative Instruction Generation:',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 23,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'Know What You Don’t Know.\\narXiv preprint arXiv:2402.09717 (2024).\\n[12] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.\\n2023.\\nShikra: Unleashing Multimodal LLM’s Referential Dialogue Magic.\\narXiv preprint arXiv:2306.15195 (2023).\\n[13] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Jinjie Gu, and Huajun Chen.\\n2024.\\nUnified Hallucination Detection for Multimodal Large Language Models.\\narXiv preprint arXiv:2402.03190 (2024).\\n[14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al.\\n2023.\\nInternvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 23,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'arXiv preprint arXiv:2312.14238 (2023).\\n[15] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou.\\n2024.\\nHALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding.\\narXiv preprint arXiv:2403.00425 (2024).\\n[16] Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, JinqiaoWang, andMing Tang.\\n2023.\\nMitigating Hallucination in Visual Language Models with Visual Supervision.\\narXiv preprint arXiv:2311.16479 (2023).\\n[17] Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi PontTuset, and Su Wang.\\n2023.\\nDavidsonian scene graph: Improving reliability in fine-grained evaluation for text-image generation.\\narXiv preprint arXiv:2310.18235 (2023).\\n[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 23,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.\\n2023.\\nPaLM: Scaling Language Modeling with Pathways.\\nJ. Mach.\\nLearn.\\nRes.\\n24 (2023), 240:1–240:113. http://jmlr.org/papers/v24/22-1144.html [19] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\\n2017.\\nDeep Reinforcement Learning from Human Preferences.\\nIn Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.).\\n4299–4307.\\nhttps://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html [20] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu.\\n2023.\\nA Survey of Chain of Thought Reasoning: Advances, Frontiers and Future.\\nArXiv preprint abs/2309.15402 (2023).\\nhttps://arxiv.org/abs/2309.15402 [21] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao.\\n2023.\\nHolistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges.\\narXiv preprint arXiv:2311.03287 (2023).\\n[22] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.\\n2023.\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.\\narXiv:2305.06500 [cs.CV] [23] Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al.\\n2023.\\nLanguage modeling is compression.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 24,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'arXiv preprint arXiv:2309.10668 (2023).\\n[24] Ailin Deng, Zhirui Chen, and BryanHooi.\\n2024.\\nSeeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding.\\narXiv preprint arXiv:2402.15300 (2024).\\n[25] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\\n2022.\\nGLM: General Language Model Pretraining with Autoregressive Blank Infilling.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).\\n320–335.\\n[26] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al.\\n2023.\\nLlama-adapter v2: Parameter-efficient visual instruction model.\\narXiv preprint arXiv:2304.15010 (2023).\\n[27] Ross Girshick.\\n2015.\\nFast r-cnn.\\nIn Proceedings of the IEEE international conference on computer vision.\\n1440–1448.\\n[28] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.\\n2023.\\nMultiModal-GPT: A Vision and Language Model for Dialogue with Humans.\\narXiv:2305.04790 [cs.CV] [29] Google.\\n2023.\\nBard.\\nhttps://bard.google.com/ [30] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.\\n2017.\\nMaking the v in vqa matter: Elevating the role of image understanding in visual question answering.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition.\\n6904–6913.\\n[31] Tianrui Guan, Fuxiao Liu, Xiyang Wu Ruiqi Xian Zongxia Li, Xiaoyu Liu Xijun Wang, Lichang Chen Furong Huang Yaser Yacoob, and Dinesh Manocha Tianyi Zhou.\\n2023.\\nHALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination & Visual Illusion in Large Vision-Language Models.\\narXiv e-prints (2023), arXiv–2310.\\n[32] Anisha Gunjal, Jihan Yin, and Erhan Bas.\\n2023.\\nDetecting and preventing hallucinations in large vision language models.\\nArXiv preprint abs/2308.06394 (2023).\\nhttps://arxiv.org/abs/2308.06394 [33] Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji.\\n2023.\\nLm-switch: Lightweight language model conditioning in word embedding space.\\narXiv preprint arXiv:2305.12798 (2023).\\n[34] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al.\\n2023.\\nImagebind-llm: Multi-modality instruction tuning.\\narXiv preprint arXiv:2309.03905 (2023).\\n[35] Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, and Tong Zhang.\\n2024.\\nThe Instinctive Bias: Spurious Images lead to Hallucination in MLLMs.\\narXiv preprint arXiv:2402.03757 (2024).\\n[36] Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, and Mike Zheng Shou.\\n2024.\\nSkip \\\\𝑛: A simple method to reduce hallucination in Large Vision-Language Models.\\narXiv preprint arXiv:2402.01345 (2024).\\n[37] KaimingHe, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.\\n2017.\\nMask r-cnn.\\nIn Proceedings of the IEEE international conference on computer vision.\\n2961–2969.\\n[38] Xin He, Longhui Wei, Lingxi Xie, and Qi Tian.\\n2024.\\nIncorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models.\\narXiv preprint arXiv:2401.03105 (2024).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 24,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021.\\nMeasuring Massive Multitask Language Understanding.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nhttps://openreview.net/forum?id=d7KBjmI3GmQ [40] Jonathan Ho and Tim Salimans.\\n2022.\\nClassifier-free diffusion guidance.\\narXiv preprint arXiv:2207.12598 (2022).\\n[41] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021.\\nLora: Low-rank adaptation of large language models.\\narXiv preprint arXiv:2106.09685 (2021).\\n[42] Hongyu Hu, Jiyuan Zhang, Minyi Zhao, and Zhenbang Sun.\\n2023.\\nCiem: Contrastive instruction evaluation method for better instruction tuning.\\narXiv preprint arXiv:2309.02301 (2023).\\n[43] Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, and Zhuowen Tu.\\n2023.\\nBliva: A simple multimodal llm for better handling of text-rich visual questions.\\narXiv preprint arXiv:2308.09936 (2023).\\n[44] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al.\\n2023.\\nA survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.\\narXiv preprint arXiv:2311.05232 (2023).\\n[45] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu.\\n2023.\\nOPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation.\\narXiv preprint arXiv:2311.17911 (2023).\\n[46] Wen Huang, Hongbin Liu, Minxin Guo, and Neil Zhenqiang Gong.\\n2024.\\nVisual Hallucinations of Multi-modal Large Language Models.\\narXiv preprint arXiv:2402.14683 (2024).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 25,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[47] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al.\\n2023.\\nC-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.\\nArXiv preprint abs/2305.08322 (2023).\\nhttps://arxiv.org/abs/2305.08322 [48] Drew A Hudson and Christopher D Manning.\\n2019.\\nGqa: A new dataset for real-world visual reasoning and compositional question answering.\\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\\n6700–6709.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 25,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[49] Jitesh Jain, Jianwei Yang, and Humphrey Shi.\\n2023.\\nVcoder: Versatile vision encoders for multimodal large language models.\\narXiv preprint arXiv:2312.14233 (2023).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 25,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[50] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo.\\n2022.\\nKnowledge unlearning for mitigating privacy risks in language models.\\narXiv preprint arXiv:2210.01504 (2022).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 25,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[51] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.\\n2021.\\nScaling up visual and vision-language representation learning with noisy text supervision.\\nIn International conference on machine learning.\\nPMLR, 4904–4916.\\n[52] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang.\\n2023.\\nHallucination Augmented Contrastive Learning for Multimodal Large Language Model.\\narXiv preprint arXiv:2312.06968 (2023).\\n[53] Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang.\\n2024.\\nHal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 25,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'arXiv preprint arXiv:2402.15721 (2024).\\n[54] Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, and Ying Shen.\\n2024.\\nEnhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study.\\narXiv preprint arXiv:2401.17981 (2024).\\n[55] Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du.\\n2023.\\nFAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models.\\narXiv preprint arXiv:2311.01477 (2023).\\n[56] Andrej Karpathy.\\n2023.\\nOn the \"hallucination problem\".\\nhttps://twitter.com/karpathy/status/1733299213503787018 Access Date: 10 Mar.\\n2024.\\n[57] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.\\n2023.\\nSegment anything.\\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision.\\n4015–4026.\\n[58] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\\n2022.\\nLarge language models are zero-shot reasoners.\\nAdvances in neural information processing systems 35 (2022), 22199–22213.\\n[59] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.\\n2017.\\nVisual genome: Connecting language and vision using crowdsourced dense image annotations.\\nInternational journal of computer vision 123 (2017), 32–73.\\n[60] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\\n2012.\\nImagenet classification with deep convolutional neural networks.\\nAdvances in neural information processing systems 25 (2012).\\n[61] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al.\\n2020.\\nThe open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.\\nInternational journal of computer vision 128, 7 (2020), 1956–1981.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 25,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.\\n2023.\\nLisa: Reasoning segmentation via large language model.\\narXiv preprint arXiv:2308.00692 (2023).\\n[63] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo.\\n2023.\\nVolcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision.\\narXiv preprint arXiv:2311.07362 (2023).\\n[64] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing.\\n2023.\\nMitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding.\\narXiv preprint arXiv:2311.16922 (2023).\\n[65] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.\\n2023.\\nOtter: A Multi-Modal Model with In-Context Instruction Tuning.\\narXiv preprint arXiv:2305.03726 (2023).\\n[66] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\\n2023.\\nBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\\narXiv preprint arXiv:2301.12597 (2023).\\n[67] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, and Yueting Zhuang.\\n2023.\\nFine-tuning multimodal llms to follow zero-shot demonstrative instructions.\\nIn The Twelfth International Conference on Learning Representations.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 26,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[68] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong.\\n2023.\\nSilkie: Preference distillation for large visual language models.\\narXiv preprint arXiv:2312.10665 (2023).\\n[69] Yifan Li, Yifan Du, Kun Zhou, JinpengWang, Wayne Xin Zhao, and Ji-RongWen.\\n2023.\\nEvaluating object hallucination in large vision-language models.\\narXiv preprint arXiv:2305.10355 (2023).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 26,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[70] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.\\n2014.\\nMicrosoft coco: Common objects in context.\\nIn Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13.\\nSpringer, 740–755.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 26,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[71] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al.\\n2023.\\nSphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 26,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'arXiv preprint arXiv:2311.07575 (2023).\\n[72] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou.\\n2023.\\nHallusionBench: You See What You Think?\\nOr You Think What You See?\\nAn Image-Context Reasoning Benchmark Challenging for GPT-4V(Ision), LLaVA-1.5, and Other Multi-Modality Models.\\nhttps://arxiv.org/abs/2310.14566 [73] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.\\n2023.\\nMitigating hallucination in large multi-modal models via robust instruction tuning.\\narXiv preprint arXiv:2306.14565 1 (2023).\\n[74] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\n2023.\\nImproved baselines with visual instruction tuning.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 26,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'arXiv preprint arXiv:2310.03744 (2023).\\n[75] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\n2023.\\nVisual instruction tuning.\\narXiv preprint arXiv:2304.08485 (2023).\\n[76] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng.\\n2024.\\nA survey on hallucination in large vision-language models.\\narXiv preprint arXiv:2402.00253 (2024).\\n[77] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung.\\n2023.\\nNegative object presence evaluation (nope) to measure object hallucination in vision-language models.\\narXiv preprint arXiv:2310.05338 (2023).\\n[78] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji.\\n2024.\\nCheap and quick: Efficient vision-language instruction tuning for large language models.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n[79] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna.\\n2023.\\nCREPE: Can VisionLanguage Foundation Models Reason Compositionally?.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n10910–10921.\\n[80] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.\\n2019.\\nOcr-vqa: Visual question answering by reading text in images.\\nIn 2019 international conference on document analysis and recognition (ICDAR).\\nIEEE, 947–952.\\n[81] OpenAI.\\n2022.\\nIntroducing chatgpt.\\nhttps://openai.com/blog/chatgpt [82] OpenAI.\\n2023.\\nGPT-4 Technical Report.\\nArXiv preprint abs/2303.08774 (2023).\\nhttps://arxiv.org/abs/2303.08774 [83] OpenAI.\\n2023.\\nGPT-4V(ision) System Card.\\nhttps://cdn.openai.com/papers/GPTV_System_Card.pdf [84] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022.\\nTraining language models to follow instructions with human feedback.\\nIn NeurIPS.\\nhttp://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html [85] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\\n2023.\\nThe RefinedWeb Dataset for Falcon LLM:',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 26,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'Outperforming Curated Corpora with Web Data, and Web Data Only.\\nArXiv preprint abs/2306.01116 (2023).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 26,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'https://arxiv.org/abs/2306.01116 [86] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.\\n2023.\\nInstruction tuning with gpt-4.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'ArXiv preprint abs/2304.03277 (2023).\\nhttps://arxiv.org/abs/2304.03277 [87] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen.\\n2022.\\nReasoning with language model prompting: A survey.\\nArXiv preprint abs/2212.09597 (2022).\\nhttps://arxiv.org/abs/2212.09597 [88] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\\n2021.\\nLearning transferable visual models from natural language supervision.\\nIn International conference on machine learning.\\nPMLR, 8748–8763.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[89] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\\n2022.\\nHierarchical text-conditional image generation with clip latents.\\narXiv preprint arXiv:2204.06125 1, 2 (2022), 3.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[90] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko.\\n2018.\\nObject hallucination in image captioning.\\narXiv preprint arXiv:1809.02156 (2018).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[91] Anna Rohrbach, Makarand Tapaswi, Atousa Torabi, Tegan Maharaj, Marcus Rohrbach, Sanja Fidler Christopher Pal, and Bernt Schiele.\\n2017.\\nThe Joint Video and Language Understanding Workshop: MovieQA and The Large Scale Movie Description Challenge (LSMDC).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[92] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\\n2022.\\nLaion-5b: An open large-scale dataset for training next generation image-text models.\\nAdvances in Neural Information Processing Systems 35 (2022), 25278–25294.\\n[93] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\n2017.\\nProximal Policy Optimization Algorithms.\\nArXiv preprint abs/1707.06347 (2017).\\nhttps://arxiv.org/abs/1707.06347 [94] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano.\\n2020.\\nLearning to summarize with human feedback.\\nIn Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html [95] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai.\\n2023.\\nPandagpt: One model to instruction-follow them all.\\narXiv preprint arXiv:2305.16355 (2023).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[96] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al.\\n2023.\\nAligning large multimodal models with factually augmented rlhf.\\narXiv preprint arXiv:2309.14525 (2023).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[97] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.\\n2023.\\nGemini: a family of highly capable multimodal models.\\narXiv preprint arXiv:2312.11805 (2023).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[98] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie.\\n2024.\\nEyes Wide Shut?\\nExploring the Visual Shortcomings of Multimodal LLMs.\\narXiv preprint arXiv:2401.06209 (2024).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': '[99] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\n2023.\\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\\nArXiv preprint abs/2307.09288 (2023).\\nhttps://arxiv.org/abs/2307.09288 [100] Andrés Villa, Juan Carlos León Alcázar, Alvaro Soto, and Bernard Ghanem.\\n2023.\\nBehind the Magic, MERLIM:',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'Multi-modal Evaluation Benchmark for Large Image-Language Models.\\narXiv preprint arXiv:2312.02219 (2023).\\n[101] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.\\n2008.\\nExtracting and composing robust features with denoising autoencoders.\\nIn Proceedings of the 25th international conference on Machine learning.\\n1096–1103.\\n[102] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et al.\\n2023.\\nVigc: Visual instruction generation and correction.\\narXiv preprint arXiv:2308.12714 (2023).\\n[103] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang.\\n2023.\\nAn llm-free multi-dimensional benchmark for mllms hallucination evaluation.\\narXiv preprint arXiv:2311.07397 (2023).',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 27,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al.\\n2023.\\nEvaluation and analysis of hallucination in large vision-language models.\\narXiv preprint arXiv:2308.15126 (2023).\\n[105] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim.\\n2023.\\nMitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites.\\narXiv preprint arXiv:2312.01701 (2023).\\n[106] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al.\\n2023.\\nCogvlm: Visual expert for pretrained language models.\\narXiv preprint arXiv:2311.03079 (2023).\\n[107] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\\n2022.\\nChain-of-thought prompting elicits reasoning in large language models.\\nAdvances in Neural Information Processing Systems 35 (2022), 24824–24837.\\n[108] Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, and Tieniu Tan.\\n2024.\\nLogical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models.\\narXiv preprint arXiv:2402.11622 (2024).\\n[109] Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, and Xinyu Dai.\\n2024.\\nEFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 28,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'arXiv preprint arXiv:2402.09801 (2024).\\n[110] Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, and Li Erran Li.\\n2024.\\nViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling.\\narXiv preprint arXiv:2402.06118 (2024).\\n[111] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al.\\n2023. mplug-owl: Modularization empowers large language models with multimodality.\\narXiv preprint arXiv:2304.14178 (2023).\\n[112] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.\\n2023. mplugowl2: Revolutionizing multi-modal large language model with modality collaboration.\\narXiv preprint arXiv:2311.04257 (2023).\\n[113] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.\\n2023.\\nA survey on multimodal large language models.\\narXiv preprint arXiv:2306.13549 (2023).\\n[114] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen.\\n2023.\\nWoodpecker: Hallucination Correction for Multimodal Large Language Models.\\nhttps: //arxiv.org/abs/2310.16045 [115] Fei Yu, Hongbo Zhang, and Benyou Wang.\\n2023.\\nNature language reasoning, a survey.\\nArXiv preprint abs/2303.14725 (2023).\\nhttps://arxiv.org/abs/2303.14725 [116] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.\\n[n.\\nd.].\\nCoca: Contrastive captioners are image-text foundation models.\\narXiv 2022. arXiv preprint arXiv:2205.01917 ([n. d.]).\\n[117] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang.\\n2023.\\nHalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data.\\narXiv preprint arXiv:2311.13614 (2023).\\n[118] Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, Chongyi Wang, Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, et al.\\n2023.\\nReformulating vision-language foundation models and datasets towards universal multimodal assistants.',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 28,\n",
       "     'source_doc': 'REFERENCES'},\n",
       "    {'text': 'arXiv preprint arXiv:2310.00653 (2023).\\n[119] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al.\\n2023.\\nRLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback.\\narXiv preprint arXiv:2312.00849 (2023).\\n[120] Zihao Yue, Liang Zhang, and Qin Jin.\\n2024.\\nLess is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective.\\narXiv preprint arXiv:2402.14545 (2024).\\n[121] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou.\\n2022.\\nWhen and Why VisionLanguage Models Behave like Bags-Of-Words, and What to Do About It?.\\nIn The Eleventh International Conference on Learning Representations.\\n[122] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong.\\n2023.\\nWhat matters in training a gpt4-style language model with multimodal inputs?\\narXiv preprint arXiv:2307.02469 (2023).\\n[123] Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, and Manling Li.\\n2023.\\nHallE-Switch: Controlling Object Hallucination in Large Vision Language Models.\\narXiv e-prints (2023), arXiv–2310.\\n[124] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua.\\n2024.\\nVPGTrans: Transfer visual prompt generator across LLMs.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n[125] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al.\\n2023.\\nInstruction Tuning for Large Language Models: A Survey.\\nArXiv preprint abs/2308.10792 (2023).\\nhttps://arxiv.org/abs/2308.10792 Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.\\n2023.\\nGpt4roi: Instruction tuning large language model on region-of-interest.\\narXiv preprint arXiv:2307.03601 (2023).\\n[127] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.\\n2019.\\nBertscore: Evaluating text generation with bert.\\narXiv preprint arXiv:1904.09675 (2019).\\n[128] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto.\\n2023.\\nBenchmarking large language models for news summarization.\\nArXiv preprint abs/2301.13848 (2023).\\nhttps: //arxiv.org/abs/2301.13848 [129] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, LongyueWang, Anh Tuan Luu,Wei Bi, Freda Shi, and Shuming Shi.\\n2023.\\nSiren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.\\nArXiv preprint abs/2309.01219 (2023).\\nhttps://arxiv.org/abs/2309.01219 [130] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang.\\n2023.\\nMmicl: Empowering vision-language model with multi-modal in-context learning.\\narXiv preprint arXiv:2309.07915 (2023).\\n[131] Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu.\\n2024.\\nMitigating Object Hallucination in Large VisionLanguage Models via Classifier-Free Guidance.\\narXiv preprint arXiv:2402.08680 (2024).\\n[132] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.\\n2023.\\nA survey of large language models.\\nArXiv preprint abs/2303.18223 (2023).\\nhttps://arxiv.org/abs/2303.18223 [133] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He.\\n2023.\\nBeyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization.\\narXiv preprint arXiv:2311.16839 (2023).\\n[134] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al.\\n2023.\\nLima: Less is more for alignment.\\nArXiv preprint abs/2305.11206 (2023).\\nhttps://arxiv.org/abs/2305.11206 [135] Qiang Zhou, Zhibin Wang, Wei Chu, Yinghui Xu, Hao Li, and Yuan Qi.\\n2023.\\nInfMLLM: A Unified Framework for Visual-Language Tasks.\\narXiv:2311.06791 [cs.CV] [136] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao.\\n2024.\\nAligning Modalities in Vision Large Language Models via Preference Fine-tuning.\\narXiv preprint arXiv:2402.11411 (2024).\\n[137] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.\\n2023.\\nAnalyzing and mitigating object hallucination in large vision-language models.\\narXiv preprint arXiv:2310.00754 (2023).\\n[138] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\\n2023.\\nMinigpt-4: Enhancing vision-language understanding with advanced large language models.\\narXiv preprint arXiv:2304.10592 (2023).\\n[139] Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, and Jun Liu.\\n2024.\\nIBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding.\\narXiv preprint arXiv:2402.18476 (2024).\\n[140] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang.\\n2023.\\nMultilingual machine translation with large language models: Empirical results and analysis.\\nArXiv preprint abs/2304.04675 (2023).\\nhttps://arxiv.org/abs/2304.04675',\n",
       "     'title': 'REFERENCES',\n",
       "     'page': 28,\n",
       "     'source_doc': 'REFERENCES'}],\n",
       "   'tables': []}]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_lst = get_files_as_docs(files)\n",
    "\n",
    "docs_sections = []\n",
    "\n",
    "for doc in docs_lst:\n",
    "    docs_sections.append(get_doc_sections(doc))\n",
    "    \n",
    "docs_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us stitch the sections together to be passed as context to gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Holmes',\n",
       " 'Benchmark the Linguistic Competence of Language Models',\n",
       " 'Andreas Waldis∗1,2, Yotam Perlitz3, Leshem Choshen4,5, Yufang Hou6, Iryna Gurevych1\\n1Ubiquitous Knowledge Processing Lab (UKP Lab) Department of Computer Science and Hessian Center for AI (hessian.AI) Technical University of Darmstadt 2Information Systems Research Lab, Lucerne University of Applied Sciences and Arts 3IBM Research AI, 4MIT CSAIL, 5MIT-IBM Watson AI Lab, 6IBM Research Europe - Ireland www.ukp.tu-darmstadt.de www.hslu.ch\\nAbstract\\nWe introduce Holmes, a benchmark to assess the linguistic competence of language models (LMs) – their ability to grasp linguistic phe- nomena.\\nUnlike prior prompting-based evalua- tions, Holmes assesses the linguistic compe- tence of LMs via their internal representations using classifier-based probing.\\nIn doing so, we disentangle specific phenomena (e.g., part-of- speech of words) from other cognitive abilities, like following textual instructions, and meet recent calls to assess LMs’ linguistic compe- tence in isolation.\\nComposing Holmes, we review over 250 probing studies and feature more than 200 datasets to assess syntax, mor- phology, semantics, reasoning, and discourse phenomena.\\nAnalyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size.\\nHow- ever, surprisingly, model architecture and in- struction tuning also significantly influence per- formance, particularly in morphology and syn- tax.\\nFinally, we propose FlashHolmes, a streamlined version of Holmes designed to lower the high computation load while main- taining high-ranking precision.\\nFigure 1: A subset of Holmes rankings (↓) for various evaluated LMs.\\nFLAN-UL2 outperforms the others overall, while different LMs prevail for the five distinct types of linguistic phenomena.\\nholmes-benchmark.github.io',\n",
       " 'Abstract\\nWe introduce Holmes, a benchmark to assess the linguistic competence of language models (LMs) – their ability to grasp linguistic phe- nomena.\\nUnlike prior prompting-based evalua- tions, Holmes assesses the linguistic compe- tence of LMs via their internal representations using classifier-based probing.\\nIn doing so, we disentangle specific phenomena (e.g., part-of- speech of words) from other cognitive abilities, like following textual instructions, and meet recent calls to assess LMs’ linguistic compe- tence in isolation.\\nComposing Holmes, we review over 250 probing studies and feature more than 200 datasets to assess syntax, mor- phology, semantics, reasoning, and discourse phenomena.\\nAnalyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size.\\nHow- ever, surprisingly, model architecture and in- struction tuning also significantly influence per- formance, particularly in morphology and syn- tax.\\nFinally, we propose FlashHolmes, a streamlined version of Holmes designed to lower the high computation load while main- taining high-ranking precision.\\nFigure 1: A subset of Holmes rankings (↓) for various evaluated LMs.\\nFLAN-UL2 outperforms the others overall, while different LMs prevail for the five distinct types of linguistic phenomena.\\nholmes-benchmark.github.io',\n",
       " '1 Introduction\\nquestions, benchmarks estimate cognitive abilities by providing textual instructions and evaluate LMs’ responses, as done for mathematical reasoning (Cobbe et al., 2021) or factual knowledge (Petroni et al., 2019, 2020).\\nHowever, they conflate latent abilities (like following provided instructions) with those under test, such as understanding specific linguistic phenomena, e.g., syntactic structures (Liang et al., 2023).\\nAs this entanglement makes it infeasible to draw definitive conclusions about distinct abilities (Hu and Levy, 2023), recent studies call to assess the linguistic competence of LMs comprehensively and in isolation (Lu et al., 2023; Mahowald et al., 2024).\\ncompetence is the unconscious understanding of language, like grasping grammatical rules (Chomsky, 1965).\\nAs language models (LMs) are trained on simple tasks like next word prediction (Brown et al., 2020), one might naturally wonder: What is the linguistic competence of LMs, and how do they differ?\\nTo answer such ∗* Corresponding author andreas.waldis@live.com In this work, we introduce the Holmes (Figure 2).\\nA benchmark to assess the linguistic competence of LMs (Figure 1) regarding numerous linguistic phenomena.\\nTo fully disentangle the understanding of these phenomena and other abilities of LMs, we use classifier-based probing (Tenney et al., 2019a; Hewitt and Manning, 2019; Belinkov, 2022).\\nA method that uses the LMs’ internal representations of text inputs to train linear models\\nFigure 2: Overview of Holmes (left) with the five phenomena types (right) and an example of probing-based evaluations for part-of-speech: encoding the input tokens and predicting the POS tag for cucumber, here NN.\\n(probes) to predict specific aspects of phenomena, such as words’ part-of-speech (POS).\\nWe then approximate the LMs’ grasp of these phenomena using the probes’ performance, rigorously verified using control tasks (Hewitt and Liang, 2019) and from an information theory perspective (Voita and Titov, 2020).\\nWith this particular and comprehensive scope, we thoroughly address the initially raised questions as follows:\\nMeta-Study (§ 3) The review of over 270 probing studies reveals a gap in comprehensively evaluating linguistic competence.\\nDespite covering over 200 probing tasks and 150 LMs, individual studies focus on particular tasks and LMs.\\nAs a result, only three LMs were probed on over 20% of the tasks, and one single task was evaluated for more than 20% of the reviewed LMs.\\nNotably, recent large LMs are significantly underrepresented.\\nBenchmark (§ 4) To address this identified deficiency, Holmes offers a structured framework to assess the English linguistic competence of LMs comprehensively.\\nIt features 208 distinct datasets covering morphology, syntax, semantics, reasoning, and discourse phenomena, including previously underrepresented ones like negation or rhetoric in text (Liang et al., 2023).\\nResults and Analysis (§ 5) From assessing 59 LMs (Figure 1), we find that no single one consistently excels the others and that their linguistic competence is more pronounced for morphology and syntax than the other phenomena types.\\nInstead, we find model size, model architecture, and instruction tuning fundamentally affect their linguistic competence.\\nFirst, LMs’ linguistic competence, particularly for morphology and syntax, scales with their model size.\\nThis generalizes previous findings (Tenney et al., 2019b; Zhang et al., 2021) beyond LMs with 350 million parameters.\\nSecond, contrary to prompting evaluations (Lu et al., 2023) and aligned with other work (Waldis et al., 2024a; Gautam et al., 2024), model architecture is critical.\\nThe linguis- tic competence of decoder-only LMs is less pronounced, and even 70 billion does not allow them to encode linguistic phenomena of words with comparable strength to encoder-only LMs of a similar size.\\nThird, while previous studies focused on aligning LMs with human interactions through instruction tuning (Ouyang et al., 2022; Touvron et al., 2023; Zhou et al., 2023), we show for the first time its effect on their linguistic competence.\\nIt improves morphology and syntax but has mixed effects for the other types of phenomena.\\nLastly, we contrast the results of Holmes with OpenLLM (Beeching et al., 2023), an extensive LM benchmark focusing on user-centered applications like mathematical reasoning.\\nWe find that Holmes provides a unique but supplementary perspective, as rankings partly align, especially for reasoning-related phenomena.\\nEfficiency (§ 6) Finally, to mitigate the heavy computational burden of evaluating a new LM on Holmes, we form the streamlined version FlashHolmes by selectively excluding samples not significantly influencing overall rankings (Perlitz et al., 2023).\\nSpecifically, FlashHolmes approximates Holmes rankings with high precision while requiring only ~3% of the computation.\\nWe summarize our contributions as follows:\\n• Benchmark.\\nHolmes comprehensively and thoroughly assesses the linguistic competence of LMs in isolation, providing substantial ground for advancements in NLP.\\n• Empirical insights.\\nExtensive experiments reveal that LMs’ linguistic competence is more pronounced for morphology and syntax, and size, architecture, and instruction tuning are crucial for LM differences.\\n• Ease of use.\\nWe provide tools to interactively explore Holmes results and straightforward code to evaluate upcoming LMs with efficiency in mind (FlashHolmes).',\n",
       " 'Preliminaries\\nLanguage Models (LMs) Language Models compute probabilities for word sequences i, enabling tasks such as classifying i, textual comparisons between i and another sequence i′, and text generation based on i. We consider LMs as any model producing representations of input i, regardless of their specific type: sparse like bag-of-words (Harris, 1954); static such as GloVe (Pennington et al., 2014); or contextualized transformer-based LMs (Devlin et al., 2019; Raffel et al., 2020).\\nLinguistic Competence Following Chomsky (1965), linguistic competence is defined as the unconscious knowledge of language, encompassing the understanding of specific linguistic phenomena, including word dependencies and their distinct parts of speech (POS).\\nLinguistic Phenomena We define the linguistic competence of LMs as their ability to understand a diversity of linguistic phenomena.\\nSpecifically, we focus on five phenomena types: morphology, the structure of words; syntax, the structure of sentences; semantics, the meaning of words; reasoning, the use of words in logical deduction and other related phenomena like negation or speculation;\\ndiscourse, the context in text like rhetorical structure.\\nFollowing Mahowald et al.\\n(2024), we categorize these phenomena types into two groups: morphology and syntax are formal phenomena, which include understanding grammatical rules and statistical patterns, while functional ones (semantics, reasoning, and discourse) focus on practical abilities like interpreting text sentiment or detecting the existence of speculation.\\nDatasets We define a dataset as text examples and labels covering a specific aspect of a linguistic phenomenon, like words and their POS tag.\\nTypically, these labels are highly unambiguous to assess the specific aspect under test in isolation.\\nProbes Using probes, we empirically assess the linguistic competence of LMs regarding the featured linguistic phenomena in Holmes.\\nTo this end, we employ probing tasks using the widely recognized classifier-based probing method (Tenney et al., 2019a; Hewitt and Manning, 2019; Belinkov, 2022), or known as diagnostic classifiers (Veldhoen et al., 2016; Giulianelli et al., 2018).\\nRunning such a probing task involves training a probe (linear model) using the specific dataset to test a distinct aspect of a linguistic phenomenon in isolation.\\nTherefore, we feed the text examples, encoded with a given LM, as training inputs.\\nSubsequently, we use the probe’s performance to approximate how an LM understands the specific linguistic phenomenon under test.\\nWith a higher score, we assume the embeddings embody patterns relevant to this phenomenon, which enhances the accuracy (Tenney et al., 2019b).\\n3 Meta-Study this section, we survey 274 studies (§ 3.1), probing LMs’ linguistic competence.\\nWe analyze these studies regarding their evolution, covered probing tasks and LMs (§ 3.2), and identify the apparent need for consolidating existing resources (§ 3.3).\\n3.1 Scope\\nWe analyze 28k papers (P) from 2015 to August 2023 of major NLP conferences (TACL, ACL, AACL, COLING, EACL, EMNLP, NAACL, and corresponding workshops) expanded with selected work from other venues such as ICLR.\\nTo identify relevant work, we follow a semiautomatic approach.\\nFirst, we automatically select papers based on their meta-data and full text.1 We select a total of 493 candidate papers matching at least one of the following three criteria (P ′ = {∀p ∈ P |p ∈ P1 ∪ p ∈\\nP2 ∪\\np\\n∈ P3}):\\nP1: papers contain probing or probe in the title.\\nP2: papers contain probing or probe in the abstract and at least five times in the main content.\\nP3: papers contain probing or probe at least ten times in the main content.\\nWe manually verified these automatically curated candidates (P ′) and found 274 relevant papers (Pr).We selected them as they either evaluate LMs regarding one or more linguistic phenomena as part of the analysis or as a main contribution.\\nThis involves filtering papers using the term probing in other senses, such as probing hash tables (Bogoychev and Lopez, 2016).\\n3.2 Analysis Next, we analyze these 274 relevant studies (Pr).\\ni) Scattered evolution calls for consolidation.\\nFirst, we analyze the evolution of the relevant studies.\\nFigure 3 relates how these studies cite each other (probing citations Cp) compared to other 1We use PyPDF2 v3.0.0, DBLP and semanticscholar API.\\nFigure 3: Citation analysis considering probing citations originating from the set of relevant work and every other citation (general citations).\\nThe color scale indicates the ratio (α) between them.\\ngathered citations (general citations Cg).\\nColorized, we show the ratio α between these two measures α = |Cp|+1 |Cg |+1.\\nFirst, only a fraction of the works gained general attention, as 16 papers exceeded 200 general citations.\\nFurther, probing works cite each other rather sparsely, with an average probing citation ratio of α = 0.1.\\nTherefore, we see other fields are paying little attention to the linguistic competence of LMs.\\nPaired with scattered citation patterns, we identify the need to consolidate existing resources to solidly ground research in this field.\\nii) Probing work prioritizes tasks and analytics over methods.\\nWe categorize the selected work according to their probing focus: methodological, new methods, like control tasks (Hewitt and Liang, 2019) or minimum description length (Voita and Titov, 2020); task-focused assessing specific linguistic phenomena as main contributions, such as discourse relations in text (Koto et al., 2021); and analytical using probing tasks to analyze LMs, such as the impact of pre-training data (Zhang et al., 2021).\\nFigure 4 shows: the majority (51.8%) of studies focus on specific probing tasks like numeric scales (Zhang et al., 2020), or morphosyntactic (Shapiro et al., 2021); 35.7% use probing as a supplementary analytical tool, for example, analyzing the effect of fine-tuning (Mosbach et al., 2020a; Zhu et al., 2022a); 12.5% address methodological problems related to probing (Wu et al., 2020; Immer et al., 2022; Zhu et al., 2022b).\\niii) The dominance of classifier-based probing.\\nNext, we analyze the specific employed probing method: classifier, using linear or shallow models to probe internal representations of LMs, as demonstrated in Tenney et al.\\n(2019a); mask, letting LMs fill gaps to verify linguistic phenomena, as shown in Talmor et al.\\n(2020) or Warstadt et al.\\n(2020); at- tention, which relies on attention patterns, as used in Pandit and Hou (2021) for bridging; and other, methods not belonging to the previous three categories, such as dimension selection (Torroba Hennigen et al., 2020).\\nMost studies utilize the classifierbased probing method (74%), 20% conduct maskbased probing, and only a minority of work (∼ 3%) considers attention patterns or other approaches.\\nFigure 4: Categorization of the selected studies by their focus and their conducted probing method.\\niv) Tasks and LMs are barely broadly evaluated.\\nFinally, we analyze which tasks and LMs the relevant probing studies consider.\\nFor example, Tenney et al.\\n(2019b) considers BERT and probes POS tagging, semantic-role labeling (SRL), and other ones.\\nAggregated over all studies, we found a broad coverage of 289 unique tasks and 161 distinct LMs.\\nBelow, we delve into the details and highlight noteworthy findings.\\nWe analyze how LMs and tasks are considered jointly in Figure 5.\\nDespite the broad coverage, single studies, including fundamental ones, maintain a particular focus and consider only a fraction of LMs and tasks.\\nFor example, while most tasks (72%) were assessed on BERT, RoBERTa’s coverage has already declined to 42%.\\nConversely, part-of-speech tagging (POS), the most probed task, was only evaluated on 23% of the LMs, for example, not covering prominent examples like BART (Lewis et al., 2020).\\nNotably, more recently released larger and powerful LMs, like PYTHIA (Biderman et al., 2023), UL2 (Tay et al., 2023), or LLAMA-2 (Touvron et al., 2023), and instruction-tuned LMs (FLAN-T5 (Chung et al., 2022), LLAMA-2-Chat (Touvron et al., 2023), or TK-Instruct (Wang et al., 2022) are missing almost entirely, with single more recent exceptions (Hu and Levy, 2023; Waldis et al., 2024a).\\nAgain, these insights underscore the need to consolidate existing resources for more dense coverage.\\nThis is further evident when considering Figure 5, where we sort LMs and tasks according to how often they were mentioned in the relevant works.\\nThen, we plot\\nFigure 5: Overview of how many tasks single LMs cover and vice versa - single examples are highlighted.\\nFigure 6: Cumulative coverage of LMs and tasks, considering all relevant studies and their focus.\\ntheir cumulative coverage concerning all mentions.\\nFor example, considering all studies (red line), the top-10 most mentioned LMs account for 80% of all LMs mentions (black dot).\\nIn contrast, the other 151 unique LMs account for only 40%.\\nComparing the paper focus, we see that methodological studies rely only on 24 LMs and 36 tasks.\\nIn contrast, task-focused and analytical work covers a similar number of LMs (91 and 99, respectively).\\nHowever, due to their distinct focus, task-focused studies cover significantly more tasks (202) than analytical ones (115).\\n3.3 Summary\\nThis meta-study emphasizes the need to consolidate existing resources for a comprehensive assessment of the linguistic competence of LMs — a manifold but rather blind spot in evaluation research.\\nApart from more thorough evaluations, such a stimulus can significantly boost future research, as happened in computer vision with ImageNet (Deng et al., 2009) or in NLP with GLUE and SuperGLUE (Wang et al., 2019a,b).\\n4 Holmes Benchmark\\nWith Holmes, we provide an extensive ground to tackle these identified deficiencies in the existing literature and comprehensively investigate the English linguistic competence of LMs.\\nSpecifically, Holmes features 208 datasets addressing distinct aspects of 66 phenomena covering morphology, syntax, semantic, reasoning, and discourse.\\n4.1 Datasets\\nTo feature a total of 208 unique datasets, we leverage existing and established resources like OntoNotes (Weischedel et al., 2013), English Web Treebank (Silveira et al., 2014), or BLIMP (Warstadt et al., 2020) and create datasets addressing phenomena like the POS of words, their dependencies or determine the linguistic acceptability of sentences.\\nFurther, we include a range of less employed data, addressing contextualization of words (Klafka and Ettinger, 2020), reasoning (Talmor et al., 2020), semantic decomposition (White et al., 2016; Rudinger et al., 2018a,b; Govindarajan et al., 2019; Vashishtha et al., 2019), grammatical knowledge (Huebner et al., 2021), bridging (Pandit and Hou, 2021), and rhetorical (Carlson et al., 2001) and discourse (Webber et al., 2019) structure in text.\\nFinally, we cover rarely probed phenomena like negation (Szarvas et al., 2008; Konstantinova et al., 2012; Vahtola et al., 2022), or word complexity (Paetzold and Specia, 2016).\\n4.2 Structure\\nApart from the comprehensive scope, Holmes provides a clear structure for specific evaluations on different levels of aggregation.\\nWe first group the datasets according to the linguistic phenomena addressed.\\nThen, we categorize these phenomena into their previously introduced type (see § 2) - morphology, syntax, semantics, reasoning and discourse.\\nWe rely on the categorization provided by the specific studies whenever given.\\nThe detailed categorization is given in § A.3.\\n4.3 Experimental Setup\\nHolmes evaluation follows the primarily used classifier-based probing paradigm, as described in § 2.\\nConsidering the internal representations allows us to maximally disentangle the understanding of distinct linguistic phenomena from each other and from other cognitive abilities (like following textual instructions).\\nFurther, this method allows us to assess any type of LMs, including sparse, static, or contextualized ones.\\nBased on the specific dataset, we either select the embeddings of the specific input tokens (like single words for POS tagging) or average embeddings across a span or the whole sentence.\\nWe define a probing task as training a probe fp (linear model without intermediate layers) using these embeddings as inputs and the dataset labels as training signals.\\nIf not defined in the original data, we divide the dataset samples into train/dev/test split following a ratio of 70/10/20.\\nWe repeat this procedure five times using different random seeds and aggregate the results afterward.\\n4.4 Evaluations\\nWe approximate how well an LM encodes specific linguistic phenomena using the absolute prediction performance of the probes.\\nIn addition, we rigorously evaluate the reliability of probing results using control tasks and from an information theory perspective (Voita and Titov, 2020; Hewitt and Liang, 2019).\\nDifferent from commonly used prompting assessments, this particular evaluation protocol refrains from known fallacies in which the results and conclusions are sensible with specific instructions (Mizrahi et al., 2024; Min et al., 2022) or few-shot examples (Lu et al., 2023).\\nTask Score Metric Based on a dataset’s specific task type, we use a corresponding performance measure, macro F1 for classification or Pearson correlation for regression.\\nIn addition, we calculate the standard deviation σ of the probe across multiple seeds.\\nA lower σ indicates a better encoding of a given linguistic phenomenon since the mea- surement is robust to noise.\\nFurther, we use the task score for ranking-based evaluation of all eval- uated LMs L = {l1,, lm} within Holmes.\\nWe calculate the mean winning rate mwr (in percentage), telling us how many times one LM l1 wins against others (Liang et al., 2023).\\nWith a higher mwr, we assume an LM encodes tested linguistic phenomena better than others.\\nCompression Next, we evaluate the probes’ reliability from an information-theoretic perspective.\\nFollowing Voita and Titov (2020), we use the compression I to measure how well a probe compresses input data.\\nA higher I means fewer bits are needed, indicating that the given linguistic phenomenon is more clearly encoded in the embeddings.\\nSelectivity A reliable probe should grasp patterns relevant to the tested phenomena in the internal representations of LMs but should not be able to learn anything else.\\nTherefore, we expect high performance when evaluating the specific dataset but low performance when we randomize training signals.\\nWe check this using control tasks introduced in Hewitt and Liang (2019).\\nSpecifically, we calculate the selectivity S as the difference between the probe trained with the original labels y and the control task where we train the probe with randomly assigned labels y′.\\nWith a higher S, we assume the detected patterns are relevant for the specific phenomena under test, as random patterns do not lead to similar performance.\\n5 Holmes Results\\nUsing Holmes, we evaluate a diverse collection of 59 LMs.2 Using the results of these extensive experiments, we first answer the research question:\\nwhat is the linguistic competence of LMs?\\nIn doing so, we discuss the reliability of results (i), the linguistic competence of LMs concerning the unique structure of Holmes (ii), and how these results relate to other downstream abilities (iii).\\nSubsequently, we examine how linguistic competence varies among LMs, as we find LMs prevailing for different types of linguistic phenomena (Figure 1) and delve into the effects of model architecture (iv), size (v), and instruction tuning (vi).\\ni) The reliability of Holmes\\nFirst, we show the reliability of probing-based evaluation, using deviation σ, compression I, and selectivity S results in Figure 7.\\nSingle outliers are datasets that are too hard for all LMs, as the sample size is too small, or the linguistic phenomena under test are too complex, as the ability to detect spans causes speculations in a text.\\nWe average these metrics for every dataset across all LMs.\\nNote, for selectivity, we consider only base-sized model (10m-200m parameters) for computational efficiency.\\nFirst, we found a low average deviation (σ = 0.02), indicating the high reliability of probes across random seeds.\\nThese results also highlight the stability of probing results, compared to prompting-based ones where results across many paraphrased prompts lead to a deviation of σ = 0.07 reported in Mizrahi et al.\\n(2024).\\nNext, substantial compression (average I = 1.9) and selectivity (average S = 0.31) further confirm the probes’ reliability.\\nInterestingly, one identifies two parallel trends for selectivity.\\nHarder datasets with many labels, like POS tagging, are arranged around a selectivity of 0.1 to 0.4 and a task metric of 0.3.\\nIn contrast, for easier binary classification tasks (such as linguistic applicability), we observe selectivity around 0.2 to 0.5 and a task metric of 0.6 to 0.9.\\n2Find a complete list in Appendix § A.2.\\nFigure 7: Reliability evaluation using deviation, compression (log), and selectivity on the y-axis for all 208 probing datasets.\\nThe x-axis represents the task metrics (either person correlation or macro F1).\\nFurther, we measure a significant (p < 0.05) positive correlation between the task metrics and the compression (τ = 0.64) and selectivity (τ = 0.65).\\nThis further confirms our reliability assumption and allows us to trust the task metric as the primary evaluation measure.\\nii) The story of Holmes\\nWe focus on what Holmes tells us in general and regarding formal and functional phenomena, as defined in § 2. We report in Figure 8 the task metric, discriminability, and selectivity, averaged for every phenomena type.\\nNote, discriminability (Rodriguez et al., 2021) quantifies the alignment of LMs ranking of one specific dataset compared to the overall rankings using the Kendall Tau correlation.\\nConsidering these three metrics, all tested LMs strongly encode formal phenomena (morphology and syntax), which often depend on the local neighborhood of words.\\nTherefore, we assume that LMs approximate these co-occurrences during pre-training with high precision.\\nFor example, the specific POS tag of a word, like man (noun), primarily depends on its surroundings, such as the frequent predecessor the.\\nIn contrast, LMs encode less information about functional phenomena (semantics, reasoning, and discourse) since they show a relatively low performance regarding the task metric.\\nFor these functional phenomena, we assume more complex co-occurrences are required to capture the broad context in language, such as the rhetorical relation of two distant text spans.\\nDespite these differences between formal and functional phenomena types, they contribute to the benchmark in a balanced way.\\nA low to medium discriminability indicates that none of these types of linguistic phenomena dominates the overall LM rankings.\\nThis balanced influence of the five phenomena types is further visible when considering their ranking correlations (Figure 9, left).\\nA high average correlation of 67.8 ± 6.6 with the overall results\\nFigure 8: Average task metric, difficulty, and discriminability for each phenomena type.\\nThe dashed lines show the average measure over all datasets.\\n(last column/row) hints that they are facets of a broader occurrence but share common characteristics.\\nStill, breaking into categories is meaningful, as the phenomena types (first five columns/rows) are medium correlated (average of 53.9 ± 14.5).\\nAnalyzing the results of phenomena types further highlights the value of this distinction.\\nWhile results of morphology and syntax are similarly correlated with the overall results (68.2 and 70.2), their direct correlation (69.1) indicates their supplementary nature.\\nFurther, discourse results show the lowest correlation with others (44.8± 16.1), indicating the particular scope.\\niii) The companions of Holmes\\nWe analyze how the results of Holmes and those from other evaluations focusing on downstream applications align (Figure 9, right).\\nWe select the OpenLLM benchmark (Beeching et al., 2023), as it covers a wide range of open LMs, in contrast to others like HELM (Liang et al., 2023).\\nFirst, Holmes and OpenLLM results of jointly evaluated LMs are medium correlated, hinting that the linguistic competence of LMs is partly aligned with their downstream abilities.\\nThe nature of this alignment is further evident when focusing on morphology, reasoning, and discourse.\\nInterestingly, and in contrast to syntax and semantics, their correlation to the OpenLLM and Holmes overall results is similar.\\nTherefore, these three phenomena presumably represent skills that are more tested in the general benchmarks.\\nThese correlation patterns are consistent across the three most meaningful OpenLLM datasets (MMLU, TruthfulQA, and GSM8K).\\nAs TruthfulQA shows lower correlations with the linguistic phenomena and other datasets within OpenLLM, we presume this dataset captures distinctly different skills (possibly knowledge).3 These insights show how different benchmarks provide a different scope and supplement themselves simulta- 3Further, it’s also known that we need to expect this dataset to be fully leaked (Balloccu et al., 2024).\\nFigure 9: Kendall-tau correlation within Holmes (left) and compared to the OpenLLM benchmark (right).\\nGreen stars indicate significant correlations (p < 0.05).\\nneously.\\nFurther, the above analysis shows, again, the value of assessing the linguistic competence of LMs across different phenomena types, for finegrained analyses.\\niv) The effect of language model architecture.\\nNext, we discuss the impact of model architecture on the linguistic competence of LMs.\\nIn Figure 11 (left), we compare encoder and decoder LMs.\\nDue to the absence of big encoder LMs, we consider five encoder and six decoder LMs with up to 220m parameters.\\nEncoder LMs show a higher mwr of 52% than decoder LMs (21%).\\nThis observation is the most saturated for morphology or syntax, encompassing a variety of token-level phenomena, like part-of-speech.\\nWe assume that the missing bi-directional encoding of decoder LMs causes this lower performance because the available context of one token heavily depends on its position.\\nThus, even common tokens, like the, have different potential representations - at the beginning or in the middle of a sentence.\\nThese instabilities are further evident when considering Figure 11 (right) which reports the accuracy for the top-20 most common POS tokens (such as the) based on the pos, xpos, upos dataset.\\nGiven their high frequency, one expects stable prediction performance.\\nSurprisingly, encoder LMs (BERT and RoBERTa) show higher median accuracy and clearly lower deviations compared to the same-size decoder counterpart (GPT2).\\nWhile scaling model size to 12B (Pythia) and 70B (Llama-2) allows for improved accuracy and lower deviations, decoder LMs do not match the encoder performance, even up to 700 times bigger.\\nv) The effect of scaling parameters.\\nWe discuss how the number of parameters influences the linguistic competence of LMs.\\nGiven the variety of LMs of different sizes, we focus on the Pythia (decoder-only) and T5 (encoder-decoder) families.\\nFrom Figure 10, we observe for both Pythia and T5 that the linguistic competence scales with model size, and it is particularly pronounced after exceeding 0.5B (Pythia) and 1.0B (T5) parameters.\\nAgain, model architecture is crucial, as T5 LMs (encoderdecoder) exhibit a clearly higher mean winning rate of 40− 70% than Pythia (decoder-only) ones with mwr of 20− 60%.\\nFurther, we found formal phenomena evolving differently with increased model size than functional ones.\\nSpecifically, morphology and syntax start at a lower level, with an apparent performance jump after 0.5B (Pythia) and 1.0B (T5) parameters, followed by slow but steady growth.\\nDifferently, semantics, reasoning, and discourse start at a higher mwr, followed by a continuous improvement as the model size grows.\\nFrom these results, we assume more parameters allow LMs to better approximate simpler co-occurrences in the near neighborhood of words to understand formal phenomena like word dependencies.\\nIn contrast, more parameters do not have the same pronounced effect on functional phenomena, like rhetorical relations, which require an LM to acquire more distant and complex word co-occurrences.\\n | Model | Morphology | Syntax | Semantics | Reasoning | Discourse | Overall\\n | --- | --- | --- | --- | --- | --- | ---\\n | Llama-2-Chat | Comparison against Llama-2 -8% | +3% | with 7 billion parameters -5% | -9% | -3% | -2%\\n | FLAN-T5 | Comparison against T5 +10% | +2% | with 11 billion parameters -2% | +6% | -2% | +1%\\n | Dolly-v2 | Comparison against Pythia +4% | -3% | with 12 billion parameters -9% | -3% | +4% | -4%\\n | Tülu-2 | Comparison against Llama-2 +5% | +2% | with 13 billion parameters -15% | 0% | -30% | -8%\\n | Orca-2 | -1% | -3% | -4% | +4% | -5% | -2%\\n | Llama-2-chat | +3% | +1% | -6% | +3% | -1% | -1%\\n | Vicuna-v1.5 | +23% | +7% | -3% | +6% | -6% | +4%\\n | FLAN-UL2 | Comparison +40% | against UL2 +16% | with 20 billion +7% | parameters +13% | +1% | +13%\\n | Mixtral-Instruct | Comparison against Mixtral +4% | +3% | with ~47 billion parameters 0% | +6% | -2% | +2%\\n | Tülu-2 | Comparison against Llama-2 +15% | 0% | with 70 billion parameters -11% | -3% | 0% | -2%\\n | Llama-2-Chat +23% +14% +2% +4% +17% +10%\\n | Average | +10% | +4% | -3% | +4% | -2% | +1%\\n\\nTable 1: Effect of instruction tuning on the mean winning rate compared to the pre-trained LMs.\\nvi) The effect of instruction tuning.\\nFinally, we focus on how instruction tuning affects LMs’ linguistic competence and compare the tuned LMs with their base models—for example, FLAN-UL2 vs. UL2.\\nFrom results in Table 1, we note less saturated effects for the overall scope while being more pronounced for the five phenomenon types again emphasizing the structured and comprehensive evaluation of linguistic competence.\\nOn average, we found instruction tuning has the highest\\nFigure 10: Effect of scaling LM parameters considering the T5 and Pythia model families providing eight and five different sizes.\\nWe address the overall scope (left) and the different types of linguistic phenomena (right).\\nFigure 11: Comparison of the phenomenon types for encoder and decoder LMs (left) and on the right, the accuracy of the top-20 most common tokens of the three part-of-speech probing datasets for BERT, RoBERTa, GPT2, Pythia, and Llama-2.\\neffect on morphology (+10%) followed by syntax (+4%), reasoning (+4%), and a negative effect for semantics −3% and discourse −2%.\\nThese results confirm previous assumptions that instruction tuning updates are often superficial (Yadav et al., 2023; Hershcovitch et al., 2024; Sharma et al., 2023) and that LMs are better at mimicking language (formal phenomena) than understanding it, measured with functional phenomena (Mahowald et al., 2024).\\nFurther, larger models benefit more from instruction tuning.\\nLlama-2-70b-Chat and FLANUL2 gain up to +23% and +40% for morphology and +10% and +13% on average.\\nIn addition, decoder-only LMs (Llama-2 and Pythia) tend to show less pronounced positive effects than encoderdecoder LMs (FLAN-T5-XXL and FLAN-UL2).\\nHowever, they better understand reasoning phenomena.\\nWhen comparing LMs based on Llama-213b, we see that specific fine-tuning methods shape the LMs differently.\\nThe top-ranked 13b LM for Holmes and OpenLLM, Vicuna, was trained on 125k instructions, less than other models.\\nThus, high quality is more important than the number of instructions for LMs’ linguistic competence.\\nTülu loses performance while being trained on a large mixture of data (approx. 330k instructions), the same for its 70b version.\\nFinally, the focus of Orca2 on reasoning is also reflected in its embedding space.\\nThese insights show again that while providing a particular perspective, Holmes shows clear differences between LMs and allows us to map them to methodological decisions.\\n6 Efficiency\\neasy, cost-effective integration of new LMs is crucial for widely adopting a benchmark.\\nAs Holmes covers many datasets and examples, it is computationally heavy in encoding text and training the probes.\\nIt takes approx.\\n6 GPU days to encode the 70 million tokens (∼230k pages of text) and 2 days to run the 208 probes for a 70b model.\\nTo account for this issue, we introduce FlashHolmes, a streamlined version of Holmes.\\nIt allows the evaluation of new LMs with a fraction of the compute while maintaining evaluation integrity.\\nBesides excluding licensed data (18 probing datasets), we analyze the effect of discarding training instances.\\nAs a result, we reduce the computation for encoding and the actual probing simultaneously.\\nWe follow Perlitz et al.\\n(2023) and calculate the rank resolution, 95% CI of model rank difference.\\nThis measure indicates the maximum expected rank deviation from evaluating an LM on FlashHolmes compared to Holmes.\\nFor example, a rank resolution of one means that an LM evaluated on FlashHolmes and Holmes has the same rank or switch place with its neighbors with a probability of 95%.\\nFigure 12 shows the resulting rank resolution when training only on a fraction of the instances, from 1/2 to 1/512.\\nSolely focusing on efficiency (1/512) still provides a decent rank resolution of ~2.7.\\nIn contrast, considering 1/2 of the training data results in the best reliability of ~1.0.\\nTo balance benchmark reliability and efficiency, we compose FlashHolmes using 1/32 of the training instances.\\nPrecisely, it reduces the computation expenses of evaluating LMs to ~3% of what Holmes would have required while pre- serving a high rank-correlation of ~1.3.\\nFigure 12: Analysis of the reliability vs. efficiency trade-off when reducing the number of training data.\\n7 Related Work\\nBenchmarking LMs Benchmarks approximate LMs abilities like general language understanding (Wang et al., 2019b,a), out-of-distribution generalization (Yang et al., 2023; Waldis et al., 2024b), adversarial scenarios (Nie et al., 2020; Wang et al., 2021), or retrieval like BEIR (Thakur et al., 2021) or MTEB (Muennighoff et al., 2023).\\nWith the advent of larger LMs, the methodological focus shifted to prompting-based evaluations which evaluate the LMs’ response to provided instructions (Brown et al., 2020; Hendrycks et al., 2021; Srivastava et al., 2022) covering application-oriented tasks (Liang et al., 2023), or mathematical reasoning (e.g., GSM8K (Cobbe et al., 2021)).\\nAssessing the Linguistic Competence of LMs The analysis of LMs’ linguistic competence ranges from analyzing static word vectors (Köhn, 2015), sentence embeddings (Conneau et al., 2018; Adi et al., 2017), the internals of translation models (Shi et al., 2016; Bau et al., 2019), or contextualized LMs (Tenney et al., 2019b,a; Hewitt and Manning, 2019).\\nOther work addressed methodological aspects, such as using control tasks (Hewitt and Liang, 2019), assessing LMs from an information theory perspective (Voita and Titov, 2020;\\nPimentel et al., 2020), or evaluating causal effects in LMs (Elazar et al., 2021).\\nFinally, another line of work focuses on whether LMs follow human understanding of linguistic competence when solving downstream tasks (Belinkov, 2022; Aw et al., 2023; Mahowald et al., 2024).\\nHowever, Mosbach et al.\\n(2020b) and Waldis et al.\\n(2024a) found finetuning for downstream tasks actually hurting the understanding of linguistic phenomena.\\nWhile prior studies assessing the linguistic competence of LMs tend to focus on a limited set of linguistic phenomena or models, Holmes provides extensive coverage of both phenomena and eval- uated LMs.\\nUnlike recent evaluations based on prompting methods (Blevins et al., 2023; Liang et al., 2023; Amouyal et al., 2024), Holmes assesses the internal representations of LMs directly.\\nThis approach allows for detailed analysis of specific model characteristics, such as architecture, and helps separate the linguistic competence from other cognitive abilities.\\nThereby, we respond to recent calls for a thorough and explicit evaluation of linguistic phenomena (Hu and Levy, 2023; Lu et al., 2023; Mahowald et al., 2024).\\n8 Conclusion\\nmarks the most up-to-date and extensive consolidation of existing resources addressing the need to assess the linguistic competence of LMs in isolation.\\nOur experiments demonstrate that LMs’ linguistic competence is pronounced regarding formal phenomena but lacks functional ones when information about broader textual contexts, such as rhetorical structure, is required.\\nFurther, size, architecture, and instruction tuning crucially account for differences among LMs.\\nAs LM and resources in the landscape of linguistics continue to grow, we will actively extend Holmes with further probing datasets, evaluate upcoming LMs, and plan to incorporate multilingualism.',\n",
       " '3.1 Scope\\nWe analyze 28k papers (P) from 2015 to August 2023 of major NLP conferences (TACL, ACL, AACL, COLING, EACL, EMNLP, NAACL, and corresponding workshops) expanded with selected work from other venues such as ICLR.\\nTo identify relevant work, we follow a semiautomatic approach.\\nFirst, we automatically select papers based on their meta-data and full text.1 We select a total of 493 candidate papers matching at least one of the following three criteria (P ′ = {∀p ∈ P |p ∈ P1 ∪ p ∈\\nP2 ∪\\np\\n∈ P3}):\\nP1: papers contain probing or probe in the title.\\nP2: papers contain probing or probe in the abstract and at least five times in the main content.\\nP3: papers contain probing or probe at least ten times in the main content.\\nWe manually verified these automatically curated candidates (P ′) and found 274 relevant papers (Pr).We selected them as they either evaluate LMs regarding one or more linguistic phenomena as part of the analysis or as a main contribution.\\nThis involves filtering papers using the term probing in other senses, such as probing hash tables (Bogoychev and Lopez, 2016).\\n3.2 Analysis Next, we analyze these 274 relevant studies (Pr).\\ni) Scattered evolution calls for consolidation.\\nFirst, we analyze the evolution of the relevant studies.\\nFigure 3 relates how these studies cite each other (probing citations Cp) compared to other 1We use PyPDF2 v3.0.0, DBLP and semanticscholar API.\\nFigure 3: Citation analysis considering probing citations originating from the set of relevant work and every other citation (general citations).\\nThe color scale indicates the ratio (α) between them.\\ngathered citations (general citations Cg).\\nColorized, we show the ratio α between these two measures α = |Cp|+1 |Cg |+1.\\nFirst, only a fraction of the works gained general attention, as 16 papers exceeded 200 general citations.\\nFurther, probing works cite each other rather sparsely, with an average probing citation ratio of α = 0.1.\\nTherefore, we see other fields are paying little attention to the linguistic competence of LMs.\\nPaired with scattered citation patterns, we identify the need to consolidate existing resources to solidly ground research in this field.\\nii) Probing work prioritizes tasks and analytics over methods.\\nWe categorize the selected work according to their probing focus: methodological, new methods, like control tasks (Hewitt and Liang, 2019) or minimum description length (Voita and Titov, 2020); task-focused assessing specific linguistic phenomena as main contributions, such as discourse relations in text (Koto et al., 2021); and analytical using probing tasks to analyze LMs, such as the impact of pre-training data (Zhang et al., 2021).\\nFigure 4 shows: the majority (51.8%) of studies focus on specific probing tasks like numeric scales (Zhang et al., 2020), or morphosyntactic (Shapiro et al., 2021); 35.7% use probing as a supplementary analytical tool, for example, analyzing the effect of fine-tuning (Mosbach et al., 2020a; Zhu et al., 2022a); 12.5% address methodological problems related to probing (Wu et al., 2020; Immer et al., 2022; Zhu et al., 2022b).\\niii) The dominance of classifier-based probing.\\nNext, we analyze the specific employed probing method: classifier, using linear or shallow models to probe internal representations of LMs, as demonstrated in Tenney et al.\\n(2019a); mask, letting LMs fill gaps to verify linguistic phenomena, as shown in Talmor et al.\\n(2020) or Warstadt et al.\\n(2020); at- tention, which relies on attention patterns, as used in Pandit and Hou (2021) for bridging; and other, methods not belonging to the previous three categories, such as dimension selection (Torroba Hennigen et al., 2020).\\nMost studies utilize the classifierbased probing method (74%), 20% conduct maskbased probing, and only a minority of work (∼ 3%) considers attention patterns or other approaches.\\nFigure 4: Categorization of the selected studies by their focus and their conducted probing method.\\niv) Tasks and LMs are barely broadly evaluated.\\nFinally, we analyze which tasks and LMs the relevant probing studies consider.\\nFor example, Tenney et al.\\n(2019b) considers BERT and probes POS tagging, semantic-role labeling (SRL), and other ones.\\nAggregated over all studies, we found a broad coverage of 289 unique tasks and 161 distinct LMs.\\nBelow, we delve into the details and highlight noteworthy findings.\\nWe analyze how LMs and tasks are considered jointly in Figure 5.\\nDespite the broad coverage, single studies, including fundamental ones, maintain a particular focus and consider only a fraction of LMs and tasks.\\nFor example, while most tasks (72%) were assessed on BERT, RoBERTa’s coverage has already declined to 42%.\\nConversely, part-of-speech tagging (POS), the most probed task, was only evaluated on 23% of the LMs, for example, not covering prominent examples like BART (Lewis et al., 2020).\\nNotably, more recently released larger and powerful LMs, like PYTHIA (Biderman et al., 2023), UL2 (Tay et al., 2023), or LLAMA-2 (Touvron et al., 2023), and instruction-tuned LMs (FLAN-T5 (Chung et al., 2022), LLAMA-2-Chat (Touvron et al., 2023), or TK-Instruct (Wang et al., 2022) are missing almost entirely, with single more recent exceptions (Hu and Levy, 2023; Waldis et al., 2024a).\\nAgain, these insights underscore the need to consolidate existing resources for more dense coverage.\\nThis is further evident when considering Figure 5, where we sort LMs and tasks according to how often they were mentioned in the relevant works.\\nThen, we plot\\nFigure 5: Overview of how many tasks single LMs cover and vice versa - single examples are highlighted.\\nFigure 6: Cumulative coverage of LMs and tasks, considering all relevant studies and their focus.\\ntheir cumulative coverage concerning all mentions.\\nFor example, considering all studies (red line), the top-10 most mentioned LMs account for 80% of all LMs mentions (black dot).\\nIn contrast, the other 151 unique LMs account for only 40%.\\nComparing the paper focus, we see that methodological studies rely only on 24 LMs and 36 tasks.\\nIn contrast, task-focused and analytical work covers a similar number of LMs (91 and 99, respectively).\\nHowever, due to their distinct focus, task-focused studies cover significantly more tasks (202) than analytical ones (115).',\n",
       " '3.3 Summary\\nThis meta-study emphasizes the need to consolidate existing resources for a comprehensive assessment of the linguistic competence of LMs — a manifold but rather blind spot in evaluation research.\\nApart from more thorough evaluations, such a stimulus can significantly boost future research, as happened in computer vision with ImageNet (Deng et al., 2009) or in NLP with GLUE and SuperGLUE (Wang et al., 2019a,b).\\n4 Holmes Benchmark\\nWith Holmes, we provide an extensive ground to tackle these identified deficiencies in the existing literature and comprehensively investigate the English linguistic competence of LMs.\\nSpecifically, Holmes features 208 datasets addressing distinct aspects of 66 phenomena covering morphology, syntax, semantic, reasoning, and discourse.\\n4.1 Datasets\\nTo feature a total of 208 unique datasets, we leverage existing and established resources like OntoNotes (Weischedel et al., 2013), English Web Treebank (Silveira et al., 2014), or BLIMP (Warstadt et al., 2020) and create datasets addressing phenomena like the POS of words, their dependencies or determine the linguistic acceptability of sentences.\\nFurther, we include a range of less employed data, addressing contextualization of words (Klafka and Ettinger, 2020), reasoning (Talmor et al., 2020), semantic decomposition (White et al., 2016; Rudinger et al., 2018a,b; Govindarajan et al., 2019; Vashishtha et al., 2019), grammatical knowledge (Huebner et al., 2021), bridging (Pandit and Hou, 2021), and rhetorical (Carlson et al., 2001) and discourse (Webber et al., 2019) structure in text.\\nFinally, we cover rarely probed phenomena like negation (Szarvas et al., 2008; Konstantinova et al., 2012; Vahtola et al., 2022), or word complexity (Paetzold and Specia, 2016).\\n4.2 Structure\\nApart from the comprehensive scope, Holmes provides a clear structure for specific evaluations on different levels of aggregation.\\nWe first group the datasets according to the linguistic phenomena addressed.\\nThen, we categorize these phenomena into their previously introduced type (see § 2) - morphology, syntax, semantics, reasoning and discourse.\\nWe rely on the categorization provided by the specific studies whenever given.\\nThe detailed categorization is given in § A.3.\\n4.3 Experimental Setup\\nHolmes evaluation follows the primarily used classifier-based probing paradigm, as described in § 2.\\nConsidering the internal representations allows us to maximally disentangle the understanding of distinct linguistic phenomena from each other and from other cognitive abilities (like following textual instructions).\\nFurther, this method allows us to assess any type of LMs, including sparse, static, or contextualized ones.\\nBased on the specific dataset, we either select the embeddings of the specific input tokens (like single words for POS tagging) or average embeddings across a span or the whole sentence.\\nWe define a probing task as training a probe fp (linear model without intermediate layers) using these embeddings as inputs and the dataset labels as training signals.\\nIf not defined in the original data, we divide the dataset samples into train/dev/test split following a ratio of 70/10/20.\\nWe repeat this procedure five times using different random seeds and aggregate the results afterward.\\n4.4 Evaluations\\nWe approximate how well an LM encodes specific linguistic phenomena using the absolute prediction performance of the probes.\\nIn addition, we rigorously evaluate the reliability of probing results using control tasks and from an information theory perspective (Voita and Titov, 2020; Hewitt and Liang, 2019).\\nDifferent from commonly used prompting assessments, this particular evaluation protocol refrains from known fallacies in which the results and conclusions are sensible with specific instructions (Mizrahi et al., 2024; Min et al., 2022) or few-shot examples (Lu et al., 2023).\\nTask Score Metric Based on a dataset’s specific task type, we use a corresponding performance measure, macro F1 for classification or Pearson correlation for regression.\\nIn addition, we calculate the standard deviation σ of the probe across multiple seeds.\\nA lower σ indicates a better encoding of a given linguistic phenomenon since the mea- surement is robust to noise.\\nFurther, we use the task score for ranking-based evaluation of all eval- uated LMs L = {l1,, lm} within Holmes.\\nWe calculate the mean winning rate mwr (in percentage), telling us how many times one LM l1 wins against others (Liang et al., 2023).\\nWith a higher mwr, we assume an LM encodes tested linguistic phenomena better than others.\\nCompression Next, we evaluate the probes’ reliability from an information-theoretic perspective.\\nFollowing Voita and Titov (2020), we use the compression I to measure how well a probe compresses input data.\\nA higher I means fewer bits are needed, indicating that the given linguistic phenomenon is more clearly encoded in the embeddings.\\nSelectivity A reliable probe should grasp patterns relevant to the tested phenomena in the internal representations of LMs but should not be able to learn anything else.\\nTherefore, we expect high performance when evaluating the specific dataset but low performance when we randomize training signals.\\nWe check this using control tasks introduced in Hewitt and Liang (2019).\\nSpecifically, we calculate the selectivity S as the difference between the probe trained with the original labels y and the control task where we train the probe with randomly assigned labels y′.\\nWith a higher S, we assume the detected patterns are relevant for the specific phenomena under test, as random patterns do not lead to similar performance.\\n5 Holmes Results\\nUsing Holmes, we evaluate a diverse collection of 59 LMs.2 Using the results of these extensive experiments, we first answer the research question:\\nwhat is the linguistic competence of LMs?\\nIn doing so, we discuss the reliability of results (i), the linguistic competence of LMs concerning the unique structure of Holmes (ii), and how these results relate to other downstream abilities (iii).\\nSubsequently, we examine how linguistic competence varies among LMs, as we find LMs prevailing for different types of linguistic phenomena (Figure 1) and delve into the effects of model architecture (iv), size (v), and instruction tuning (vi).',\n",
       " '4.3 Experimental Setup\\nHolmes evaluation follows the primarily used classifier-based probing paradigm, as described in § 2.\\nConsidering the internal representations allows us to maximally disentangle the understanding of distinct linguistic phenomena from each other and from other cognitive abilities (like following textual instructions).\\nFurther, this method allows us to assess any type of LMs, including sparse, static, or contextualized ones.\\nBased on the specific dataset, we either select the embeddings of the specific input tokens (like single words for POS tagging) or average embeddings across a span or the whole sentence.\\nWe define a probing task as training a probe fp (linear model without intermediate layers) using these embeddings as inputs and the dataset labels as training signals.\\nIf not defined in the original data, we divide the dataset samples into train/dev/test split following a ratio of 70/10/20.\\nWe repeat this procedure five times using different random seeds and aggregate the results afterward.',\n",
       " '4.4 Evaluations\\nWe approximate how well an LM encodes specific linguistic phenomena using the absolute prediction performance of the probes.\\nIn addition, we rigorously evaluate the reliability of probing results using control tasks and from an information theory perspective (Voita and Titov, 2020; Hewitt and Liang, 2019).\\nDifferent from commonly used prompting assessments, this particular evaluation protocol refrains from known fallacies in which the results and conclusions are sensible with specific instructions (Mizrahi et al., 2024; Min et al., 2022) or few-shot examples (Lu et al., 2023).\\nTask Score Metric Based on a dataset’s specific task type, we use a corresponding performance measure, macro F1 for classification or Pearson correlation for regression.\\nIn addition, we calculate the standard deviation σ of the probe across multiple seeds.\\nA lower σ indicates a better encoding of a given linguistic phenomenon since the mea- surement is robust to noise.\\nFurther, we use the task score for ranking-based evaluation of all eval- uated LMs L = {l1,, lm} within Holmes.\\nWe calculate the mean winning rate mwr (in percentage), telling us how many times one LM l1 wins against others (Liang et al., 2023).\\nWith a higher mwr, we assume an LM encodes tested linguistic phenomena better than others.\\nCompression Next, we evaluate the probes’ reliability from an information-theoretic perspective.\\nFollowing Voita and Titov (2020), we use the compression I to measure how well a probe compresses input data.\\nA higher I means fewer bits are needed, indicating that the given linguistic phenomenon is more clearly encoded in the embeddings.\\nSelectivity A reliable probe should grasp patterns relevant to the tested phenomena in the internal representations of LMs but should not be able to learn anything else.\\nTherefore, we expect high performance when evaluating the specific dataset but low performance when we randomize training signals.\\nWe check this using control tasks introduced in Hewitt and Liang (2019).\\nSpecifically, we calculate the selectivity S as the difference between the probe trained with the original labels y and the control task where we train the probe with randomly assigned labels y′.\\nWith a higher S, we assume the detected patterns are relevant for the specific phenomena under test, as random patterns do not lead to similar performance.\\n5 Holmes Results\\nUsing Holmes, we evaluate a diverse collection of 59 LMs.2 Using the results of these extensive experiments, we first answer the research question:\\nwhat is the linguistic competence of LMs?\\nIn doing so, we discuss the reliability of results (i), the linguistic competence of LMs concerning the unique structure of Holmes (ii), and how these results relate to other downstream abilities (iii).\\nSubsequently, we examine how linguistic competence varies among LMs, as we find LMs prevailing for different types of linguistic phenomena (Figure 1) and delve into the effects of model architecture (iv), size (v), and instruction tuning (vi).',\n",
       " 'i) The reliability of Holmes\\nFirst, we show the reliability of probing-based evaluation, using deviation σ, compression I, and selectivity S results in Figure 7.\\nSingle outliers are datasets that are too hard for all LMs, as the sample size is too small, or the linguistic phenomena under test are too complex, as the ability to detect spans causes speculations in a text.\\nWe average these metrics for every dataset across all LMs.\\nNote, for selectivity, we consider only base-sized model (10m-200m parameters) for computational efficiency.\\nFirst, we found a low average deviation (σ = 0.02), indicating the high reliability of probes across random seeds.\\nThese results also highlight the stability of probing results, compared to prompting-based ones where results across many paraphrased prompts lead to a deviation of σ = 0.07 reported in Mizrahi et al.\\n(2024).\\nNext, substantial compression (average I = 1.9) and selectivity (average S = 0.31) further confirm the probes’ reliability.\\nInterestingly, one identifies two parallel trends for selectivity.\\nHarder datasets with many labels, like POS tagging, are arranged around a selectivity of 0.1 to 0.4 and a task metric of 0.3.\\nIn contrast, for easier binary classification tasks (such as linguistic applicability), we observe selectivity around 0.2 to 0.5 and a task metric of 0.6 to 0.9.\\n2Find a complete list in Appendix § A.2.\\nFigure 7: Reliability evaluation using deviation, compression (log), and selectivity on the y-axis for all 208 probing datasets.\\nThe x-axis represents the task metrics (either person correlation or macro F1).\\nFurther, we measure a significant (p < 0.05) positive correlation between the task metrics and the compression (τ = 0.64) and selectivity (τ = 0.65).\\nThis further confirms our reliability assumption and allows us to trust the task metric as the primary evaluation measure.',\n",
       " 'ii) The story of Holmes\\nWe focus on what Holmes tells us in general and regarding formal and functional phenomena, as defined in § 2. We report in Figure 8 the task metric, discriminability, and selectivity, averaged for every phenomena type.\\nNote, discriminability (Rodriguez et al., 2021) quantifies the alignment of LMs ranking of one specific dataset compared to the overall rankings using the Kendall Tau correlation.\\nConsidering these three metrics, all tested LMs strongly encode formal phenomena (morphology and syntax), which often depend on the local neighborhood of words.\\nTherefore, we assume that LMs approximate these co-occurrences during pre-training with high precision.\\nFor example, the specific POS tag of a word, like man (noun), primarily depends on its surroundings, such as the frequent predecessor the.\\nIn contrast, LMs encode less information about functional phenomena (semantics, reasoning, and discourse) since they show a relatively low performance regarding the task metric.\\nFor these functional phenomena, we assume more complex co-occurrences are required to capture the broad context in language, such as the rhetorical relation of two distant text spans.\\nDespite these differences between formal and functional phenomena types, they contribute to the benchmark in a balanced way.\\nA low to medium discriminability indicates that none of these types of linguistic phenomena dominates the overall LM rankings.\\nThis balanced influence of the five phenomena types is further visible when considering their ranking correlations (Figure 9, left).\\nA high average correlation of 67.8 ± 6.6 with the overall results\\nFigure 8: Average task metric, difficulty, and discriminability for each phenomena type.\\nThe dashed lines show the average measure over all datasets.\\n(last column/row) hints that they are facets of a broader occurrence but share common characteristics.\\nStill, breaking into categories is meaningful, as the phenomena types (first five columns/rows) are medium correlated (average of 53.9 ± 14.5).\\nAnalyzing the results of phenomena types further highlights the value of this distinction.\\nWhile results of morphology and syntax are similarly correlated with the overall results (68.2 and 70.2), their direct correlation (69.1) indicates their supplementary nature.\\nFurther, discourse results show the lowest correlation with others (44.8± 16.1), indicating the particular scope.',\n",
       " 'iii) The companions of Holmes\\nWe analyze how the results of Holmes and those from other evaluations focusing on downstream applications align (Figure 9, right).\\nWe select the OpenLLM benchmark (Beeching et al., 2023), as it covers a wide range of open LMs, in contrast to others like HELM (Liang et al., 2023).\\nFirst, Holmes and OpenLLM results of jointly evaluated LMs are medium correlated, hinting that the linguistic competence of LMs is partly aligned with their downstream abilities.\\nThe nature of this alignment is further evident when focusing on morphology, reasoning, and discourse.\\nInterestingly, and in contrast to syntax and semantics, their correlation to the OpenLLM and Holmes overall results is similar.\\nTherefore, these three phenomena presumably represent skills that are more tested in the general benchmarks.\\nThese correlation patterns are consistent across the three most meaningful OpenLLM datasets (MMLU, TruthfulQA, and GSM8K).\\nAs TruthfulQA shows lower correlations with the linguistic phenomena and other datasets within OpenLLM, we presume this dataset captures distinctly different skills (possibly knowledge).3 These insights show how different benchmarks provide a different scope and supplement themselves simulta- 3Further, it’s also known that we need to expect this dataset to be fully leaked (Balloccu et al., 2024).\\nFigure 9: Kendall-tau correlation within Holmes (left) and compared to the OpenLLM benchmark (right).\\nGreen stars indicate significant correlations (p < 0.05).\\nneously.\\nFurther, the above analysis shows, again, the value of assessing the linguistic competence of LMs across different phenomena types, for finegrained analyses.\\niv) The effect of language model architecture.\\nNext, we discuss the impact of model architecture on the linguistic competence of LMs.\\nIn Figure 11 (left), we compare encoder and decoder LMs.\\nDue to the absence of big encoder LMs, we consider five encoder and six decoder LMs with up to 220m parameters.\\nEncoder LMs show a higher mwr of 52% than decoder LMs (21%).\\nThis observation is the most saturated for morphology or syntax, encompassing a variety of token-level phenomena, like part-of-speech.\\nWe assume that the missing bi-directional encoding of decoder LMs causes this lower performance because the available context of one token heavily depends on its position.\\nThus, even common tokens, like the, have different potential representations - at the beginning or in the middle of a sentence.\\nThese instabilities are further evident when considering Figure 11 (right) which reports the accuracy for the top-20 most common POS tokens (such as the) based on the pos, xpos, upos dataset.\\nGiven their high frequency, one expects stable prediction performance.\\nSurprisingly, encoder LMs (BERT and RoBERTa) show higher median accuracy and clearly lower deviations compared to the same-size decoder counterpart (GPT2).\\nWhile scaling model size to 12B (Pythia) and 70B (Llama-2) allows for improved accuracy and lower deviations, decoder LMs do not match the encoder performance, even up to 700 times bigger.\\nv) The effect of scaling parameters.\\nWe discuss how the number of parameters influences the linguistic competence of LMs.\\nGiven the variety of LMs of different sizes, we focus on the Pythia (decoder-only) and T5 (encoder-decoder) families.\\nFrom Figure 10, we observe for both Pythia and T5 that the linguistic competence scales with model size, and it is particularly pronounced after exceeding 0.5B (Pythia) and 1.0B (T5) parameters.\\nAgain, model architecture is crucial, as T5 LMs (encoderdecoder) exhibit a clearly higher mean winning rate of 40− 70% than Pythia (decoder-only) ones with mwr of 20− 60%.\\nFurther, we found formal phenomena evolving differently with increased model size than functional ones.\\nSpecifically, morphology and syntax start at a lower level, with an apparent performance jump after 0.5B (Pythia) and 1.0B (T5) parameters, followed by slow but steady growth.\\nDifferently, semantics, reasoning, and discourse start at a higher mwr, followed by a continuous improvement as the model size grows.\\nFrom these results, we assume more parameters allow LMs to better approximate simpler co-occurrences in the near neighborhood of words to understand formal phenomena like word dependencies.\\nIn contrast, more parameters do not have the same pronounced effect on functional phenomena, like rhetorical relations, which require an LM to acquire more distant and complex word co-occurrences.\\n | Model | Morphology | Syntax | Semantics | Reasoning | Discourse | Overall\\n | --- | --- | --- | --- | --- | --- | ---\\n | Llama-2-Chat | Comparison against Llama-2 -8% | +3% | with 7 billion parameters -5% | -9% | -3% | -2%\\n | FLAN-T5 | Comparison against T5 +10% | +2% | with 11 billion parameters -2% | +6% | -2% | +1%\\n | Dolly-v2 | Comparison against Pythia +4% | -3% | with 12 billion parameters -9% | -3% | +4% | -4%\\n | Tülu-2 | Comparison against Llama-2 +5% | +2% | with 13 billion parameters -15% | 0% | -30% | -8%\\n | Orca-2 | -1% | -3% | -4% | +4% | -5% | -2%\\n | Llama-2-chat | +3% | +1% | -6% | +3% | -1% | -1%\\n | Vicuna-v1.5 | +23% | +7% | -3% | +6% | -6% | +4%\\n | FLAN-UL2 | Comparison +40% | against UL2 +16% | with 20 billion +7% | parameters +13% | +1% | +13%\\n | Mixtral-Instruct | Comparison against Mixtral +4% | +3% | with ~47 billion parameters 0% | +6% | -2% | +2%\\n | Tülu-2 | Comparison against Llama-2 +15% | 0% | with 70 billion parameters -11% | -3% | 0% | -2%\\n | Llama-2-Chat +23% +14% +2% +4% +17% +10%\\n | Average | +10% | +4% | -3% | +4% | -2% | +1%\\n\\nTable 1: Effect of instruction tuning on the mean winning rate compared to the pre-trained LMs.\\nvi) The effect of instruction tuning.\\nFinally, we focus on how instruction tuning affects LMs’ linguistic competence and compare the tuned LMs with their base models—for example, FLAN-UL2 vs. UL2.\\nFrom results in Table 1, we note less saturated effects for the overall scope while being more pronounced for the five phenomenon types again emphasizing the structured and comprehensive evaluation of linguistic competence.\\nOn average, we found instruction tuning has the highest\\nFigure 10: Effect of scaling LM parameters considering the T5 and Pythia model families providing eight and five different sizes.\\nWe address the overall scope (left) and the different types of linguistic phenomena (right).\\nFigure 11: Comparison of the phenomenon types for encoder and decoder LMs (left) and on the right, the accuracy of the top-20 most common tokens of the three part-of-speech probing datasets for BERT, RoBERTa, GPT2, Pythia, and Llama-2.\\neffect on morphology (+10%) followed by syntax (+4%), reasoning (+4%), and a negative effect for semantics −3% and discourse −2%.\\nThese results confirm previous assumptions that instruction tuning updates are often superficial (Yadav et al., 2023; Hershcovitch et al., 2024; Sharma et al., 2023) and that LMs are better at mimicking language (formal phenomena) than understanding it, measured with functional phenomena (Mahowald et al., 2024).\\nFurther, larger models benefit more from instruction tuning.\\nLlama-2-70b-Chat and FLANUL2 gain up to +23% and +40% for morphology and +10% and +13% on average.\\nIn addition, decoder-only LMs (Llama-2 and Pythia) tend to show less pronounced positive effects than encoderdecoder LMs (FLAN-T5-XXL and FLAN-UL2).\\nHowever, they better understand reasoning phenomena.\\nWhen comparing LMs based on Llama-213b, we see that specific fine-tuning methods shape the LMs differently.\\nThe top-ranked 13b LM for Holmes and OpenLLM, Vicuna, was trained on 125k instructions, less than other models.\\nThus, high quality is more important than the number of instructions for LMs’ linguistic competence.\\nTülu loses performance while being trained on a large mixture of data (approx. 330k instructions), the same for its 70b version.\\nFinally, the focus of Orca2 on reasoning is also reflected in its embedding space.\\nThese insights show again that while providing a particular perspective, Holmes shows clear differences between LMs and allows us to map them to methodological decisions.\\n6 Efficiency\\neasy, cost-effective integration of new LMs is crucial for widely adopting a benchmark.\\nAs Holmes covers many datasets and examples, it is computationally heavy in encoding text and training the probes.\\nIt takes approx.\\n6 GPU days to encode the 70 million tokens (∼230k pages of text) and 2 days to run the 208 probes for a 70b model.\\nTo account for this issue, we introduce FlashHolmes, a streamlined version of Holmes.\\nIt allows the evaluation of new LMs with a fraction of the compute while maintaining evaluation integrity.\\nBesides excluding licensed data (18 probing datasets), we analyze the effect of discarding training instances.\\nAs a result, we reduce the computation for encoding and the actual probing simultaneously.\\nWe follow Perlitz et al.\\n(2023) and calculate the rank resolution, 95% CI of model rank difference.\\nThis measure indicates the maximum expected rank deviation from evaluating an LM on FlashHolmes compared to Holmes.\\nFor example, a rank resolution of one means that an LM evaluated on FlashHolmes and Holmes has the same rank or switch place with its neighbors with a probability of 95%.\\nFigure 12 shows the resulting rank resolution when training only on a fraction of the instances, from 1/2 to 1/512.\\nSolely focusing on efficiency (1/512) still provides a decent rank resolution of ~2.7.\\nIn contrast, considering 1/2 of the training data results in the best reliability of ~1.0.\\nTo balance benchmark reliability and efficiency, we compose FlashHolmes using 1/32 of the training instances.\\nPrecisely, it reduces the computation expenses of evaluating LMs to ~3% of what Holmes would have required while pre- serving a high rank-correlation of ~1.3.\\nFigure 12: Analysis of the reliability vs. efficiency trade-off when reducing the number of training data.\\n7 Related Work\\nBenchmarking LMs Benchmarks approximate LMs abilities like general language understanding (Wang et al., 2019b,a), out-of-distribution generalization (Yang et al., 2023; Waldis et al., 2024b), adversarial scenarios (Nie et al., 2020; Wang et al., 2021), or retrieval like BEIR (Thakur et al., 2021) or MTEB (Muennighoff et al., 2023).\\nWith the advent of larger LMs, the methodological focus shifted to prompting-based evaluations which evaluate the LMs’ response to provided instructions (Brown et al., 2020; Hendrycks et al., 2021; Srivastava et al., 2022) covering application-oriented tasks (Liang et al., 2023), or mathematical reasoning (e.g., GSM8K (Cobbe et al., 2021)).\\nAssessing the Linguistic Competence of LMs The analysis of LMs’ linguistic competence ranges from analyzing static word vectors (Köhn, 2015), sentence embeddings (Conneau et al., 2018; Adi et al., 2017), the internals of translation models (Shi et al., 2016; Bau et al., 2019), or contextualized LMs (Tenney et al., 2019b,a; Hewitt and Manning, 2019).\\nOther work addressed methodological aspects, such as using control tasks (Hewitt and Liang, 2019), assessing LMs from an information theory perspective (Voita and Titov, 2020;\\nPimentel et al., 2020), or evaluating causal effects in LMs (Elazar et al., 2021).\\nFinally, another line of work focuses on whether LMs follow human understanding of linguistic competence when solving downstream tasks (Belinkov, 2022; Aw et al., 2023; Mahowald et al., 2024).\\nHowever, Mosbach et al.\\n(2020b) and Waldis et al.\\n(2024a) found finetuning for downstream tasks actually hurting the understanding of linguistic phenomena.\\nWhile prior studies assessing the linguistic competence of LMs tend to focus on a limited set of linguistic phenomena or models, Holmes provides extensive coverage of both phenomena and eval- uated LMs.\\nUnlike recent evaluations based on prompting methods (Blevins et al., 2023; Liang et al., 2023; Amouyal et al., 2024), Holmes assesses the internal representations of LMs directly.\\nThis approach allows for detailed analysis of specific model characteristics, such as architecture, and helps separate the linguistic competence from other cognitive abilities.\\nThereby, we respond to recent calls for a thorough and explicit evaluation of linguistic phenomena (Hu and Levy, 2023; Lu et al., 2023; Mahowald et al., 2024).\\n8 Conclusion\\nmarks the most up-to-date and extensive consolidation of existing resources addressing the need to assess the linguistic competence of LMs in isolation.\\nOur experiments demonstrate that LMs’ linguistic competence is pronounced regarding formal phenomena but lacks functional ones when information about broader textual contexts, such as rhetorical structure, is required.\\nFurther, size, architecture, and instruction tuning crucially account for differences among LMs.\\nAs LM and resources in the landscape of linguistics continue to grow, we will actively extend Holmes with further probing datasets, evaluate upcoming LMs, and plan to incorporate multilingualism.',\n",
       " 'Ethical Considerations and Limitations\\nLanguage Holmes as well as FlashHolmes solely assess linguistic phenomena for the English language.\\nAs we plan to expand the benchmark and scope of multilingual data, we focus momentarily on English because of the widespread availability of resources, including curated corpora and the diversity of available LMs.\\nPhenomena and LM Coverage We agree with Liang et al.\\n(2023) and see one fundamental aspect in composing a benchmark in acknowledging its incompleteness.\\nBoth linguistic phenomena and LMs are a subset of the variety of available resources.\\nWe consolidated them carefully to provide a comprehensive scope of the linguistic competence and various LMs.\\nHowever, as benchmarks evolve as tools to assess LMs, we will further expand Holmes both with the existing and upcoming LMs and data resources.\\nData Availability Linguistic annotations, in particular more complex ones targeting phenomena like discourse, are money and time-wise expensive.\\nOut of 208 datasets included in Holmes, 18 probing datasets are based on licensed resources and are not freely available.\\nHowever, with FlashHolmes, we provide an effective and efficient alternative based on open-access resources.\\nFurthermore, upon confirming the granted access, we are happy to share our probing datasets, including those based on the licensed resources.\\nBias As Holmes relies on existing resources, it inherits the bias embodied in this data.\\nExamples of such bias are gender equality or gender fairness, like the use of neo pronouns such as em in Lauscher et al.\\n(2023).',\n",
       " 'References\\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg.\\n2017.\\nFine-grained analysis of sentence embeddings using auxiliary prediction tasks.\\nIn 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\\nOpenReview.net.\\nEhsan Aghazadeh, Mohsen Fayyaz, and Yadollah Yaghoobzadeh.\\n2022.\\nMetaphors in pre-trained language models: Probing and generalization across datasets and languages.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2037– 2050, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nSamuel Amouyal, Aya Meltzer-Asscher, and Jonathan Berant.\\n2024.\\nLarge language models for psycholinguistic plausibility pretesting.\\nIn Findings of the Association for Computational Linguistics: EACL 2024, pages 166–181, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.\\nKhai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, and Antoine Bosselut.\\n2023.\\nInstruction-tuning aligns llms to the human brain.\\nCoRR, abs/2312.00575.\\nSimone Balloccu, Patrícia Schmidtová, Mateusz Lango, and Ondrej Dusek.\\n2024.\\nLeak, cheat, repeat: Data contamination and evaluation malpractices in closedsource LLMs.\\nIn Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 67–93, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.\\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James R. Glass.\\n2019.\\nIdentifying and controlling important neurons in neural machine translation.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nEdward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.\\n2023.\\nOpen llm leaderboard.\\nhttps://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard.\\nYonatan Belinkov.\\n2022.\\nProbing classifiers: Promises, shortcomings, and advances.\\nComputational Linguistics, 48(1):207–219.\\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass.\\n2017.\\nWhat do neural machine translation models learn about morphology?\\nIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861–872, Vancouver, Canada.\\nAssociation for Computational Linguistics.\\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal.\\n2023.\\nPythia: A suite for analyzing large language models across training and scaling.\\nIn International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2397–2430.\\nPMLR.\\nJulia Birke and Anoop Sarkar.\\n2006.\\nA clustering approach for nearly unsupervised recognition of nonliteral language.\\nIn 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 329–336, Trento, Italy.\\nAssociation for Computational Linguistics.\\nTerra Blevins, Hila Gonen, and Luke Zettlemoyer.\\n2023.\\nPrompting language models for linguistic structure.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6649–6663, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nNikolay Bogoychev and Adam Lopez.\\n2016.\\nN-gram language models for massively parallel devices.\\nIn\\nProceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 1944–1953, Berlin, Germany.\\nAssociation for Computational Linguistics.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\\n2020.\\nLanguage models are few-shot learners.\\nIn Advances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nLynn Carlson, Daniel Marcu, and Mary Ellen Okurovsky.\\n2001.\\nBuilding a discourse-tagged corpus in the framework of rhetorical structure theory.\\nIn Proceedings of the SIGDIAL 2001 Workshop, The 2nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, Saturday, September 1, 2001 to Sunday, September 2, 2001, Aalborg, Denmark.\\nThe Association for Computer Linguistics.\\nNoam Chomsky.\\n1965.\\nAspects of the Theory of Syntax.\\nThe MIT Press, Cambridge.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\\n2022.\\nScaling instruction-finetuned language models.\\nCoRR, abs/2210.11416.\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.\\n2020.\\nELECTRA: pretraining text encoders as discriminators rather than generators.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\\n2021.\\nTraining verifiers to solve math word problems.\\nCoRR, abs/2110.14168.\\nAlexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni.\\n2018.\\nWhat you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.\\nIn\\nProceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 2126–2136, Melbourne, Australia.\\nAssociation for Computational Linguistics.\\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.\\n2023.\\nFree dolly: Introducing the world’s first truly open instructiontuned llm.\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei.\\n2009.\\nImagenet: A large-scale hierarchical image database.\\n2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\n2019.\\nBERT: Pre-training of deep bidirectional transformers for language under- standing.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.\\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg.\\n2021.\\nAmnesic probing: Behavioral explanation with amnesic counterfactuals.\\nTransactions of the Association for Computational Linguistics, 9:160– 175.\\nRudolf Franz Flesch.\\n1948.\\nA new readability yardstick.\\nThe Journal of applied psychology, 32(3):221–233.\\nWilliam Gantt, Lelia Glass, and Aaron Steven White.\\n2022.\\nDecomposing and recomposing event structure.\\nTransactions of the Association for Computational Linguistics, 10:17–34.\\nVagrant Gautam, Eileen Bingert, D. Zhu, Anne Lauscher, and Dietrich Klakow.\\n2024.\\nRobust pronoun use fidelity with english llms: Are they reasoning, repeating, or just biased?\\nCoRR, abs/2404.03134.\\nMario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema.\\n2018.\\nUnder the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 240–248, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nVenkata Govindarajan, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nDecomposing generalization: Models of generic, habitual, and episodic statements.\\nTransactions of the Association for Computational Linguistics, 7:501–517.\\nZellig S Harris.\\n1954.\\nDistributional structure.\\nWord, 10(2-3):146–162.\\nPengcheng He, Jianfeng Gao, and Weizhu Chen.\\n2023.\\nDebertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.\\n2021.\\nDeberta: decoding-enhanced bert with disentangled attention.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.\\n2010.\\nSemEval-2010 task 8: Multiway classification of semantic relations between pairs of nominals.\\nIn Proceedings of the 5th International\\nWorkshop on Semantic Evaluation, pages 33–38, Uppsala, Sweden.\\nAssociation for Computational Linguistics.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021.\\nMeasuring massive multitask language understanding.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nMoshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, Swaminathan Sundararaman, and Danny Harnik.\\n2024.\\nLossless and nearlossless compression for foundation models.\\nCoRR, abs/2404.15198.\\nJohn Hewitt and Percy Liang.\\n2019.\\nDesigning and interpreting probes with control tasks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–2743, Hong Kong, China.\\nAssociation for Computational Linguistics.\\nJohn Hewitt and Christopher D. Manning.\\n2019.\\nA structural probe for finding syntax in word representations.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.\\nYufang Hou.\\n2018.\\nEnhanced word representations for bridging anaphora resolution.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 1–7, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.\\nYufang Hou.\\n2020.\\nBridging anaphora resolution as question answering.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1428–1438, Online.\\nAssociation for Computational Linguistics.\\nJennifer Hu and Roger Levy.\\n2023.\\nPrompting is not a substitute for probability measurements in large language models.\\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5040–5060, Singapore.\\nAssociation for Computational Linguistics.\\nPhilip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth.\\n2021.\\nBabyBERTa: Learning more grammar with small-scale child-directed language.\\nIn Proceedings of the 25th Conference on Computational Natural Language Learning, pages 624–646, Online.\\nAssociation for Computational Linguistics.\\nAlexander Immer, Lucas Torroba Hennigen, Vincent Fortuin, and Ryan Cotterell.\\n2022.\\nProbing as quantifying inductive bias.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1839– 1851, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2023.\\nMistral 7b.\\nCoRR, abs/2310.06825.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2024.\\nMixtral of experts.\\nCoRR, abs/2401.04088.\\nJosef Klafka and Allyson Ettinger.\\n2020.\\nSpying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4801–4811, Online.\\nAssociation for Computational Linguistics.\\nArne Köhn.\\n2015.\\nWhat’s in an embedding?\\nanalyzing word embeddings through multilingual evaluation.\\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073, Lisbon, Portugal.\\nAssociation for Computational Linguistics.\\nNatalia Konstantinova, Sheila C.M.\\nde Sousa, Noa P. Cruz, Manuel J. Maña, Maite Taboada, and Ruslan Mitkov.\\n2012.\\nA review corpus annotated for negation, speculation and their scope.\\nIn Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 3190–3195, Istanbul, Turkey.\\nEuropean Language Resources Association (ELRA).\\nFajri Koto, Jey Han Lau, and Timothy Baldwin.\\n2021.\\nDiscourse probing of pretrained language models.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3849–3864, Online.\\nAssociation for Computational Linguistics.\\nKatarzyna Krasnowska-Kieraś and Alina Wróblewska.\\n2019.\\nEmpirical linguistic study of sentence embeddings.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5729–5739, Florence, Italy.\\nAssociation for Computational Linguistics.\\nMurathan Kurfalı and Robert Östling.\\n2021.\\nProbing multilingual language models for discourse.\\nIn\\nProceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 8–19, Online.\\nAssociation for Computational Linguistics.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\\n2020.\\nALBERT: A lite BERT for self-supervised learning of language representations.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.\\nAnne Lauscher, Debora Nozza, Ehm Miltersen, Archie Crowley, and Dirk Hovy.\\n2023.\\nWhat about “em”?\\nhow commercial machine translation fails to handle (neo-)pronouns.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 377–392, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020.\\nBART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online.\\nAssociation for Computational Linguistics.\\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana AcostaNavas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.\\n2023.\\nHolistic evaluation of language models.\\nTransactions on Machine Learning Research.\\nFeatured Certification, Expert Certification.\\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\\n2016.\\nAssessing the ability of LSTMs to learn syntaxsensitive dependencies.\\nTransactions of the Association for Computational Linguistics, 4:521–535.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019.\\nRoberta: A robustly optimized BERT pretraining approach.\\nCoRR, abs/1907.11692.\\nIlya Loshchilov and Frank Hutter.\\n2019.\\nDecoupled weight decay regularization.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenRe- view.net.\\nSheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych.\\n2023.\\nAre emergent abilities in large language models just in-context learning?\\nCoRR, abs/2309.01809.\\nKyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko.\\n2024.\\nDissociating language and thought in large language models.\\nTrends in Cognitive Sciences.\\nGeorge A. Miller.\\n1995.\\nWordnet: A lexical database for english.\\nCommunications of the ACM, 38(11):39– 41.\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.\\n2022.\\nRethinking the role of demonstrations:\\nWhat makes in-context learning work?\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048–11064, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nArindam Mitra, Luciano Del Corro, Shweti Mahajan, Andrés Codas, Clarisse Simões, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah.\\n2023.\\nOrca 2: Teaching small language models how to reason.\\nCoRR, abs/2311.11045.\\nMoran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky.\\n2024.\\nState of what art? A call for multi-prompt LLM evaluation.\\nCoRR, abs/2401.00595.\\nMichael Mohler, Mary Brunson, Bryan Rink, and Marc Tomlinson.\\n2016.\\nIntroducing the LCC metaphor datasets.\\nIn Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 4221–4227, Portorož, Slovenia.\\nEuropean Language Resources Association (ELRA).\\nRoser Morante and Eduardo Blanco.\\n2012.\\n*SEM 2012 shared task: Resolving the scope and focus of negation.\\nIn *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 265–274, Montréal, Canada.\\nAssociation for Computational Linguistics.\\nMarius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020a.\\nOn the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers.\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 68–82, Online.\\nAssociation for Computational Linguistics.\\nMarius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020b.\\nOn the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2502–2516, Online.\\nAssociation for Computational Linguistics.\\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers.\\n2023.\\nMTEB: Massive text embedding benchmark.\\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014–2037, Dubrovnik, Croatia.\\nAssociation for Computational Linguistics.\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\\n2018.\\nDon’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for extreme summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nAllen Nie, Erin Bennett, and Noah Goodman.\\n2019.\\nDisSent: Learning sentence representations from explicit discourse relations.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4497–4510, Florence, Italy.\\nAssociation for Computational Linguistics.\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\\n2020.\\nAdversarial NLI: A new benchmark for natural language understanding.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885–4901, Online.\\nAssociation for Computational Linguistics.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022.\\nTraining language models to follow instructions with human feedback.\\nIn Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.\\nGustavo Paetzold and Lucia Specia.\\n2016.\\nSemEval 2016 task 11: Complex word identification.\\nIn Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 560–569, San Diego, California.\\nAssociation for Computational Linguistics.\\nOnkar Pandit and Yufang Hou.\\n2021.\\nProbing for bridging inference in transformer language models.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4153–4163, Online.\\nAssociation for Computational Linguistics.\\nJeffrey Pennington, Richard Socher, and Christopher Manning.\\n2014.\\nGloVe: Global vectors for word representation.\\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar.\\nAssociation for Computational Linguistics.\\nYotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen.\\n2023.\\nEfficient benchmarking (of language models).\\nCoRR, abs/2308.11696.\\nFabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel.\\n2020.\\nHow context affects language models’ factual predictions.\\nIn Conference on Automated Knowledge Base Construction, AKBC 2020, Virtual, June 22-24, 2020.\\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller.\\n2019.\\nLanguage models as knowledge bases?\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2463–2473.\\nAssociation for Computational Linguistics.\\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell.\\n2020.\\nInformation-theoretic probing for linguistic structure.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4609–4622, Online.\\nAssociation for Computational Linguistics.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\n2019.\\nLanguage models are unsupervised multitask learners.\\nOpenAI blog, 1(8):9.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\\n2020.\\nExploring the limits of transfer learning with a unified text-to-text transformer.\\nJournal of Machine Learning Research, 21(140):1–67.\\nPedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, and Jordan BoydGraber.\\n2021.\\nEvaluation examples are not equally informative: How should that change NLP leaderboards?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4486–4503, Online.\\nAssociation for Computational Linguistics.\\nRachel Rudinger, Adam Teichert, Ryan Culkin, Sheng Zhang, and Benjamin Van Durme.\\n2018a.\\nNeuralDavidsonian semantic proto-role labeling.\\nIn Pro-\\nceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 944–955, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nRachel Rudinger, Aaron Steven White, and Benjamin Van Durme.\\n2018b.\\nNeural models of factuality.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 731–744, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.\\nNaomi Shapiro, Amandalynne Paullada, and Shane Steinert-Threlkeld.\\n2021.\\nA multilabel approach to morphosyntactic probing.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4486–4524, Punta Cana, Dominican Republic.\\nAssociation for Computational Linguistics.\\nPratyusha Sharma, Jordan T. Ash, and Dipendra Misra.\\n2023.\\nThe truth is in there: Improving reasoning in language models with layer-selective rank reduction.\\nCoRR, abs/2312.13558.\\nXing Shi, Inkit Padhi, and Kevin Knight.\\n2016.\\nDoes string-based neural MT learn source syntax?\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526– 1534, Austin, Texas.\\nAssociation for Computational Linguistics.\\nNatalia Silveira, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning.\\n2014.\\nA gold standard dependency corpus for English.\\nIn Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2897– 2904, Reykjavik, Iceland.\\nEuropean Language Resources Association (ELRA).\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts.\\n2013.\\nRecursive deep models for semantic compositionality over a sentiment treebank.\\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA.\\nAssociation for Computational Linguistics.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al.\\n2022.\\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.\\nCoRR, abs/2206.04615.\\nGerard J Steen, Aletta G Dorst, J Berenike Herrmann, Anna A Kaal, Tina Krennmayr, Tryntje Pasma, et al.\\n2010.\\nA method for linguistic metaphor identification.\\nConverging evidence in language and communication research.\\nJohn Benjamins Publishing Company Amsterdam.\\nShivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, and Akash Srivastava.\\n2024.\\nLAB: large-scale alignment for chatbots.\\nCoRR, abs/2403.01081.\\nGyörgy Szarvas, Veronika Vincze, Richárd Farkas, and János Csirik.\\n2008.\\nThe BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts.\\nIn Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 38–45, Columbus, Ohio.\\nAssociation for Computational Linguistics.\\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant.\\n2020. oLMpics-On What Language Model Pre-training Captures.\\nTransactions of the Association for Computational Linguistics, 8:743–758.\\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler.\\n2023.\\nUL2: unifying language learning paradigms.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.\\nIan Tenney, Dipanjan Das, and Ellie Pavlick.\\n2019a.\\nBERT rediscovers the classical NLP pipeline.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy.\\nAssociation for Computational Linguistics.\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick.\\n2019b.\\nWhat do you learn from context?\\nprobing for sentence structure in contextualized word representations.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych.\\n2021.\\nBEIR:\\nA heterogeneous benchmark for zero-shot evaluation of information retrieval models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\nLucas Torroba Hennigen, Adina Williams, and Ryan Cotterell.\\n2020.\\nIntrinsic probing through dimension selection.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 197–216, Online.\\nAssociation for Computational Linguistics.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\n2023.\\nLlama 2: Open foundation and finetuned chat models.\\nCoRR, abs/2307.09288.\\nTeemu Vahtola, Mathias Creutz, and Jörg Tiedemann.\\n2022.\\nIt is not easy to detect paraphrases: Analysing semantic similarity with antonyms and negation using the new SemAntoNeg benchmark.\\nIn Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 249–262, Abu Dhabi, United Arab Emirates (Hybrid).\\nAssociation for Computational Linguistics.\\nSiddharth Vashishtha, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nFine-grained temporal relation extraction.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2906–2919, Florence, Italy.\\nAssociation for Computational Linguistics.\\nSara Veldhoen, Dieuwke Hupkes, and Willem H. Zuidema.\\n2016.\\nDiagnostic classifiers revealing how neural networks process hierarchical structure.\\nIn Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings.\\nCEUR-WS.org.\\nElena Voita and Ivan Titov.\\n2020.\\nInformation-theoretic probing with minimum description length.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183–196, Online.\\nAssociation for Computational Linguistics.\\nAndreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024a.\\nDive into the chasm: Probing the gap between in- and cross-topic generalization.\\nIn Findings of the Association for Computational Linguistics: EACL 2024, pages 2197–2214, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.\\nAndreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024b.\\nHow to handle different types of out-ofdistribution scenarios in computational argumentation?\\na comprehensive and fine-grained field study.\\nCoRR, abs/2309.08316.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\n2019a.\\nSuperglue: A stickier benchmark for general-purpose language understanding systems.\\nIn Advances in Neural Information Processing Systems, volume 32.\\nCurran Associates, Inc. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\n2019b.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nBoxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li.\\n2021.\\nAdversarial GLUE: A multitask benchmark for robustness evaluation of language models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\\n2023.\\nHow far can camels go?\\nexploring the state of instruction tuning on open resources.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen.\\n2022.\\nSuper-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085–5109, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman.\\n2020.\\nBLiMP: The benchmark of linguistic minimal pairs for English.\\nTransactions of the Association for Computational Linguistics, 8:377– 392.\\nBonnie Webber, Rashmi Prasad, Alan Lee, and Aravind Joshi.\\n2019.\\nThe penn discourse treebank 3.0 annotation manual.\\nPhiladelphia, University of Pennsylvania.\\nRalph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al.\\n2013.\\nOntonotes release 5.0.\\nLinguistic Data Consortium, Philadelphia, PA, 23:170.\\nAaron Steven White, Drew Reisinger, Keisuke Sakaguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme.\\n2016.\\nUniversal decompositional semantics on Universal Dependencies.\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1713–1723, Austin, Texas.\\nAssociation for Computational Linguistics.\\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu.\\n2020.\\nPerturbed masking: Parameter-free probing for analyzing and interpreting BERT.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4166–4176, Online.\\nAssociation for Computational Linguistics.\\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.\\n2023.\\nWizardlm: Empowering large language models to follow complex instructions.\\nCoRR, abs/2304.12244.\\nPrateek Yadav, Leshem Choshen, Colin Raffel, and Mohit Bansal.\\n2023.\\nCompeft: Compression for communicating parameter efficient updates via sparsification and quantization.\\nCoRR, abs/2311.13171.\\nLinyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang.\\n2023.\\nGLUE-X: Evaluating natural language understanding models from an out-ofdistribution generalization perspective.\\nIn Findings of the Association for Computational Linguistics:\\nACL 2023, pages 12731–12750, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nAmir Zeldes.\\n2017.\\nThe GUM corpus: Creating multilayer resources in the classroom.\\nLanguage Resources and Evaluation, 51(3):581–612.\\nXikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth.\\n2020. Do language embeddings capture scales?\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 292–299, Online.\\nAssociation for Computational Linguistics.\\nYian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman.\\n2021.\\nWhen do you need billions of words of pretraining data?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1112–1125, Online.\\nAssociation for Computational Linguistics.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\n2023.\\nJudging llm-as-a-judge with mt-bench and chatbot arena.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\\n2023.\\nLIMA: less is more for alignment.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nZining Zhu, Soroosh Shahtalebi, and Frank Rudzicz.\\n2022a.\\nPredicting fine-tuning performance with probing.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11534–11547, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nZining Zhu, Jixuan Wang, Bai Li, and Frank Rudzicz.\\n2022b.\\nOn the data requirements of probing.\\nIn Findings of the Association for Computational Linguistics: ACL 2022, pages 4132–4147, Dublin, Ireland.\\nAssociation for Computational Linguistics.',\n",
       " 'Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 1944–1953, Berlin, Germany.\\nAssociation for Computational Linguistics.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\\n2020.\\nLanguage models are few-shot learners.\\nIn Advances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nLynn Carlson, Daniel Marcu, and Mary Ellen Okurovsky.\\n2001.\\nBuilding a discourse-tagged corpus in the framework of rhetorical structure theory.\\nIn Proceedings of the SIGDIAL 2001 Workshop, The 2nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, Saturday, September 1, 2001 to Sunday, September 2, 2001, Aalborg, Denmark.\\nThe Association for Computer Linguistics.\\nNoam Chomsky.\\n1965.\\nAspects of the Theory of Syntax.\\nThe MIT Press, Cambridge.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\\n2022.\\nScaling instruction-finetuned language models.\\nCoRR, abs/2210.11416.\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.\\n2020.\\nELECTRA: pretraining text encoders as discriminators rather than generators.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\\n2021.\\nTraining verifiers to solve math word problems.\\nCoRR, abs/2110.14168.\\nAlexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni.\\n2018.\\nWhat you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.\\nIn',\n",
       " 'Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 2126–2136, Melbourne, Australia.\\nAssociation for Computational Linguistics.\\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.\\n2023.\\nFree dolly: Introducing the world’s first truly open instructiontuned llm.\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei.\\n2009.\\nImagenet: A large-scale hierarchical image database.\\n2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\n2019.\\nBERT: Pre-training of deep bidirectional transformers for language under- standing.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.\\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg.\\n2021.\\nAmnesic probing: Behavioral explanation with amnesic counterfactuals.\\nTransactions of the Association for Computational Linguistics, 9:160– 175.\\nRudolf Franz Flesch.\\n1948.\\nA new readability yardstick.\\nThe Journal of applied psychology, 32(3):221–233.\\nWilliam Gantt, Lelia Glass, and Aaron Steven White.\\n2022.\\nDecomposing and recomposing event structure.\\nTransactions of the Association for Computational Linguistics, 10:17–34.\\nVagrant Gautam, Eileen Bingert, D. Zhu, Anne Lauscher, and Dietrich Klakow.\\n2024.\\nRobust pronoun use fidelity with english llms: Are they reasoning, repeating, or just biased?\\nCoRR, abs/2404.03134.\\nMario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema.\\n2018.\\nUnder the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 240–248, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nVenkata Govindarajan, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nDecomposing generalization: Models of generic, habitual, and episodic statements.\\nTransactions of the Association for Computational Linguistics, 7:501–517.\\nZellig S Harris.\\n1954.\\nDistributional structure.\\nWord, 10(2-3):146–162.\\nPengcheng He, Jianfeng Gao, and Weizhu Chen.\\n2023.\\nDebertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.\\n2021.\\nDeberta: decoding-enhanced bert with disentangled attention.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.\\n2010.\\nSemEval-2010 task 8: Multiway classification of semantic relations between pairs of nominals.\\nIn Proceedings of the 5th International\\nWorkshop on Semantic Evaluation, pages 33–38, Uppsala, Sweden.\\nAssociation for Computational Linguistics.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021.\\nMeasuring massive multitask language understanding.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nMoshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, Swaminathan Sundararaman, and Danny Harnik.\\n2024.\\nLossless and nearlossless compression for foundation models.\\nCoRR, abs/2404.15198.\\nJohn Hewitt and Percy Liang.\\n2019.\\nDesigning and interpreting probes with control tasks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–2743, Hong Kong, China.\\nAssociation for Computational Linguistics.\\nJohn Hewitt and Christopher D. Manning.\\n2019.\\nA structural probe for finding syntax in word representations.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.\\nYufang Hou.\\n2018.\\nEnhanced word representations for bridging anaphora resolution.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 1–7, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.\\nYufang Hou.\\n2020.\\nBridging anaphora resolution as question answering.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1428–1438, Online.\\nAssociation for Computational Linguistics.\\nJennifer Hu and Roger Levy.\\n2023.\\nPrompting is not a substitute for probability measurements in large language models.\\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5040–5060, Singapore.\\nAssociation for Computational Linguistics.\\nPhilip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth.\\n2021.\\nBabyBERTa: Learning more grammar with small-scale child-directed language.\\nIn Proceedings of the 25th Conference on Computational Natural Language Learning, pages 624–646, Online.\\nAssociation for Computational Linguistics.\\nAlexander Immer, Lucas Torroba Hennigen, Vincent Fortuin, and Ryan Cotterell.\\n2022.\\nProbing as quantifying inductive bias.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1839– 1851, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2023.\\nMistral 7b.\\nCoRR, abs/2310.06825.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\n2024.\\nMixtral of experts.\\nCoRR, abs/2401.04088.\\nJosef Klafka and Allyson Ettinger.\\n2020.\\nSpying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4801–4811, Online.\\nAssociation for Computational Linguistics.\\nArne Köhn.\\n2015.\\nWhat’s in an embedding?\\nanalyzing word embeddings through multilingual evaluation.\\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073, Lisbon, Portugal.\\nAssociation for Computational Linguistics.\\nNatalia Konstantinova, Sheila C.M.\\nde Sousa, Noa P. Cruz, Manuel J. Maña, Maite Taboada, and Ruslan Mitkov.\\n2012.\\nA review corpus annotated for negation, speculation and their scope.\\nIn Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 3190–3195, Istanbul, Turkey.\\nEuropean Language Resources Association (ELRA).\\nFajri Koto, Jey Han Lau, and Timothy Baldwin.\\n2021.\\nDiscourse probing of pretrained language models.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3849–3864, Online.\\nAssociation for Computational Linguistics.\\nKatarzyna Krasnowska-Kieraś and Alina Wróblewska.\\n2019.\\nEmpirical linguistic study of sentence embeddings.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5729–5739, Florence, Italy.\\nAssociation for Computational Linguistics.\\nMurathan Kurfalı and Robert Östling.\\n2021.\\nProbing multilingual language models for discourse.\\nIn\\nProceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 8–19, Online.\\nAssociation for Computational Linguistics.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\\n2020.\\nALBERT: A lite BERT for self-supervised learning of language representations.\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\\nOpenReview.net.\\nAnne Lauscher, Debora Nozza, Ehm Miltersen, Archie Crowley, and Dirk Hovy.\\n2023.\\nWhat about “em”?\\nhow commercial machine translation fails to handle (neo-)pronouns.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 377–392, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020.\\nBART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online.\\nAssociation for Computational Linguistics.\\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana AcostaNavas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.\\n2023.\\nHolistic evaluation of language models.\\nTransactions on Machine Learning Research.\\nFeatured Certification, Expert Certification.\\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\\n2016.\\nAssessing the ability of LSTMs to learn syntaxsensitive dependencies.\\nTransactions of the Association for Computational Linguistics, 4:521–535.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019.\\nRoberta: A robustly optimized BERT pretraining approach.\\nCoRR, abs/1907.11692.\\nIlya Loshchilov and Frank Hutter.\\n2019.\\nDecoupled weight decay regularization.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenRe- view.net.\\nSheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych.\\n2023.\\nAre emergent abilities in large language models just in-context learning?\\nCoRR, abs/2309.01809.\\nKyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko.\\n2024.\\nDissociating language and thought in large language models.\\nTrends in Cognitive Sciences.\\nGeorge A. Miller.\\n1995.\\nWordnet: A lexical database for english.\\nCommunications of the ACM, 38(11):39– 41.\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.\\n2022.\\nRethinking the role of demonstrations:\\nWhat makes in-context learning work?\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048–11064, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nArindam Mitra, Luciano Del Corro, Shweti Mahajan, Andrés Codas, Clarisse Simões, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah.\\n2023.\\nOrca 2: Teaching small language models how to reason.\\nCoRR, abs/2311.11045.\\nMoran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky.\\n2024.\\nState of what art? A call for multi-prompt LLM evaluation.\\nCoRR, abs/2401.00595.\\nMichael Mohler, Mary Brunson, Bryan Rink, and Marc Tomlinson.\\n2016.\\nIntroducing the LCC metaphor datasets.\\nIn Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 4221–4227, Portorož, Slovenia.\\nEuropean Language Resources Association (ELRA).\\nRoser Morante and Eduardo Blanco.\\n2012.\\n*SEM 2012 shared task: Resolving the scope and focus of negation.\\nIn *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 265–274, Montréal, Canada.\\nAssociation for Computational Linguistics.\\nMarius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020a.\\nOn the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers.\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 68–82, Online.\\nAssociation for Computational Linguistics.\\nMarius Mosbach, Anna Khokhlova, Michael A. Hedderich, and Dietrich Klakow.\\n2020b.\\nOn the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2502–2516, Online.\\nAssociation for Computational Linguistics.\\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers.\\n2023.\\nMTEB: Massive text embedding benchmark.\\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014–2037, Dubrovnik, Croatia.\\nAssociation for Computational Linguistics.\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\\n2018.\\nDon’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for extreme summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nAllen Nie, Erin Bennett, and Noah Goodman.\\n2019.\\nDisSent: Learning sentence representations from explicit discourse relations.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4497–4510, Florence, Italy.\\nAssociation for Computational Linguistics.\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\\n2020.\\nAdversarial NLI: A new benchmark for natural language understanding.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885–4901, Online.\\nAssociation for Computational Linguistics.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022.\\nTraining language models to follow instructions with human feedback.\\nIn Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.\\nGustavo Paetzold and Lucia Specia.\\n2016.\\nSemEval 2016 task 11: Complex word identification.\\nIn Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 560–569, San Diego, California.\\nAssociation for Computational Linguistics.\\nOnkar Pandit and Yufang Hou.\\n2021.\\nProbing for bridging inference in transformer language models.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4153–4163, Online.\\nAssociation for Computational Linguistics.\\nJeffrey Pennington, Richard Socher, and Christopher Manning.\\n2014.\\nGloVe: Global vectors for word representation.\\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar.\\nAssociation for Computational Linguistics.\\nYotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen.\\n2023.\\nEfficient benchmarking (of language models).\\nCoRR, abs/2308.11696.\\nFabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel.\\n2020.\\nHow context affects language models’ factual predictions.\\nIn Conference on Automated Knowledge Base Construction, AKBC 2020, Virtual, June 22-24, 2020.\\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller.\\n2019.\\nLanguage models as knowledge bases?\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2463–2473.\\nAssociation for Computational Linguistics.\\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell.\\n2020.\\nInformation-theoretic probing for linguistic structure.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4609–4622, Online.\\nAssociation for Computational Linguistics.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\n2019.\\nLanguage models are unsupervised multitask learners.\\nOpenAI blog, 1(8):9.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\\n2020.\\nExploring the limits of transfer learning with a unified text-to-text transformer.\\nJournal of Machine Learning Research, 21(140):1–67.\\nPedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, and Jordan BoydGraber.\\n2021.\\nEvaluation examples are not equally informative: How should that change NLP leaderboards?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4486–4503, Online.\\nAssociation for Computational Linguistics.\\nRachel Rudinger, Adam Teichert, Ryan Culkin, Sheng Zhang, and Benjamin Van Durme.\\n2018a.\\nNeuralDavidsonian semantic proto-role labeling.\\nIn Pro-\\nceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 944–955, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nRachel Rudinger, Aaron Steven White, and Benjamin Van Durme.\\n2018b.\\nNeural models of factuality.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 731–744, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.\\nNaomi Shapiro, Amandalynne Paullada, and Shane Steinert-Threlkeld.\\n2021.\\nA multilabel approach to morphosyntactic probing.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4486–4524, Punta Cana, Dominican Republic.\\nAssociation for Computational Linguistics.\\nPratyusha Sharma, Jordan T. Ash, and Dipendra Misra.\\n2023.\\nThe truth is in there: Improving reasoning in language models with layer-selective rank reduction.\\nCoRR, abs/2312.13558.\\nXing Shi, Inkit Padhi, and Kevin Knight.\\n2016.\\nDoes string-based neural MT learn source syntax?\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526– 1534, Austin, Texas.\\nAssociation for Computational Linguistics.\\nNatalia Silveira, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning.\\n2014.\\nA gold standard dependency corpus for English.\\nIn Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2897– 2904, Reykjavik, Iceland.\\nEuropean Language Resources Association (ELRA).\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts.\\n2013.\\nRecursive deep models for semantic compositionality over a sentiment treebank.\\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA.\\nAssociation for Computational Linguistics.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al.\\n2022.\\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.\\nCoRR, abs/2206.04615.\\nGerard J Steen, Aletta G Dorst, J Berenike Herrmann, Anna A Kaal, Tina Krennmayr, Tryntje Pasma, et al.\\n2010.\\nA method for linguistic metaphor identification.\\nConverging evidence in language and communication research.\\nJohn Benjamins Publishing Company Amsterdam.\\nShivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, and Akash Srivastava.\\n2024.\\nLAB: large-scale alignment for chatbots.\\nCoRR, abs/2403.01081.\\nGyörgy Szarvas, Veronika Vincze, Richárd Farkas, and János Csirik.\\n2008.\\nThe BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts.\\nIn Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 38–45, Columbus, Ohio.\\nAssociation for Computational Linguistics.\\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant.\\n2020. oLMpics-On What Language Model Pre-training Captures.\\nTransactions of the Association for Computational Linguistics, 8:743–758.\\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler.\\n2023.\\nUL2: unifying language learning paradigms.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net.\\nIan Tenney, Dipanjan Das, and Ellie Pavlick.\\n2019a.\\nBERT rediscovers the classical NLP pipeline.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy.\\nAssociation for Computational Linguistics.\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick.\\n2019b.\\nWhat do you learn from context?\\nprobing for sentence structure in contextualized word representations.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych.\\n2021.\\nBEIR:\\nA heterogeneous benchmark for zero-shot evaluation of information retrieval models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\nLucas Torroba Hennigen, Adina Williams, and Ryan Cotterell.\\n2020.\\nIntrinsic probing through dimension selection.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 197–216, Online.\\nAssociation for Computational Linguistics.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\n2023.\\nLlama 2: Open foundation and finetuned chat models.\\nCoRR, abs/2307.09288.\\nTeemu Vahtola, Mathias Creutz, and Jörg Tiedemann.\\n2022.\\nIt is not easy to detect paraphrases: Analysing semantic similarity with antonyms and negation using the new SemAntoNeg benchmark.\\nIn Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 249–262, Abu Dhabi, United Arab Emirates (Hybrid).\\nAssociation for Computational Linguistics.\\nSiddharth Vashishtha, Benjamin Van Durme, and Aaron Steven White.\\n2019.\\nFine-grained temporal relation extraction.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2906–2919, Florence, Italy.\\nAssociation for Computational Linguistics.\\nSara Veldhoen, Dieuwke Hupkes, and Willem H. Zuidema.\\n2016.\\nDiagnostic classifiers revealing how neural networks process hierarchical structure.\\nIn Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings.\\nCEUR-WS.org.\\nElena Voita and Ivan Titov.\\n2020.\\nInformation-theoretic probing with minimum description length.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183–196, Online.\\nAssociation for Computational Linguistics.\\nAndreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024a.\\nDive into the chasm: Probing the gap between in- and cross-topic generalization.\\nIn Findings of the Association for Computational Linguistics: EACL 2024, pages 2197–2214, St. Julian’s, Malta.\\nAssociation for Computational Linguistics.\\nAndreas Waldis, Yufang Hou, and Iryna Gurevych.\\n2024b.\\nHow to handle different types of out-ofdistribution scenarios in computational argumentation?\\na comprehensive and fine-grained field study.\\nCoRR, abs/2309.08316.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\n2019a.\\nSuperglue: A stickier benchmark for general-purpose language understanding systems.\\nIn Advances in Neural Information Processing Systems, volume 32.\\nCurran Associates, Inc. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\n2019b.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nBoxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li.\\n2021.\\nAdversarial GLUE: A multitask benchmark for robustness evaluation of language models.\\nIn Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\\n2023.\\nHow far can camels go?\\nexploring the state of instruction tuning on open resources.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen.\\n2022.\\nSuper-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085–5109, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman.\\n2020.\\nBLiMP: The benchmark of linguistic minimal pairs for English.\\nTransactions of the Association for Computational Linguistics, 8:377– 392.\\nBonnie Webber, Rashmi Prasad, Alan Lee, and Aravind Joshi.\\n2019.\\nThe penn discourse treebank 3.0 annotation manual.\\nPhiladelphia, University of Pennsylvania.\\nRalph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al.\\n2013.\\nOntonotes release 5.0.\\nLinguistic Data Consortium, Philadelphia, PA, 23:170.\\nAaron Steven White, Drew Reisinger, Keisuke Sakaguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme.\\n2016.\\nUniversal decompositional semantics on Universal Dependencies.\\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1713–1723, Austin, Texas.\\nAssociation for Computational Linguistics.\\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu.\\n2020.\\nPerturbed masking: Parameter-free probing for analyzing and interpreting BERT.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4166–4176, Online.\\nAssociation for Computational Linguistics.\\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.\\n2023.\\nWizardlm: Empowering large language models to follow complex instructions.\\nCoRR, abs/2304.12244.\\nPrateek Yadav, Leshem Choshen, Colin Raffel, and Mohit Bansal.\\n2023.\\nCompeft: Compression for communicating parameter efficient updates via sparsification and quantization.\\nCoRR, abs/2311.13171.\\nLinyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang.\\n2023.\\nGLUE-X: Evaluating natural language understanding models from an out-ofdistribution generalization perspective.\\nIn Findings of the Association for Computational Linguistics:\\nACL 2023, pages 12731–12750, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nAmir Zeldes.\\n2017.\\nThe GUM corpus: Creating multilayer resources in the classroom.\\nLanguage Resources and Evaluation, 51(3):581–612.\\nXikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth.\\n2020. Do language embeddings capture scales?\\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 292–299, Online.\\nAssociation for Computational Linguistics.\\nYian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman.\\n2021.\\nWhen do you need billions of words of pretraining data?\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1112–1125, Online.\\nAssociation for Computational Linguistics.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\n2023.\\nJudging llm-as-a-judge with mt-bench and chatbot arena.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\\n2023.\\nLIMA: less is more for alignment.\\nIn Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\\nZining Zhu, Soroosh Shahtalebi, and Frank Rudzicz.\\n2022a.\\nPredicting fine-tuning performance with probing.\\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11534–11547, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nZining Zhu, Jixuan Wang, Bai Li, and Frank Rudzicz.\\n2022b.\\nOn the data requirements of probing.\\nIn Findings of the Association for Computational Linguistics: ACL 2022, pages 4132–4147, Dublin, Ireland.\\nAssociation for Computational Linguistics.',\n",
       " 'A Additional Details of Holmes\\nA.1 Additional Details on the Evolution of Probing Literature\\nWe analyze publication trends by year and venue as shown in Table 2.\\nLess work was published between 2015-2018 (earlier) focusing on LSTMbased (Linzen et al., 2016; Conneau et al., 2018) and static LMs (Köhn, 2015; Linzen et al., 2016;\\nBelinkov et al., 2017; Conneau et al., 2018).\\nWith the release of BERT (Devlin et al., 2019) in 2019, we note increasing attention to analyzing linguistic abilities within LMs, with a peak of 90 papers in 2022.4 Considering the venue, more than half of the relevant work (149 papers) was published at major conferences (ACL and EMNLP), and 68 papers were published at AACL, EACL, NAACL, and COLING.5 In addition, we observe a constant contribution of TACL, various workshops, such as Analyzing and Interpreting Neural Networks for NLP or Representation Learning for NLP.\\nA.2 Experimental Details\\nProbing Hyperparameters Following previous work (Hewitt and Liang, 2019; Voita and Titov, 2020), we use fixed hyperparameters for training the probes: 20 epochs, where we find the best one using dev instances; AdamW (Loshchilov and Hutter, 2019) as optimizer; a batch size of 64; a learning rate of 0.0005; a dropout rate of 0.2; a warmup rate of 10% of the steps; random seeds: [0, 1, 2, 3, 4] Hardware We run all of our experiments using 12 Nvidia RTX A6000 GPUs.\\nEvery GPU provides 48GB of memory and 10752 CUDA Cores.\\nConsidered LMs Table 8 outlines the details of the LMs we evaluate on Holmes in this work.\\nA.3 Linguistic Task Categorization\\nWe show in Table 3, Table 4, Table 7, Table 5, and Table 6 which resources Holmes use to cover morphology, syntax, semantics, reasoning, and discourse phenomena.\\nThis includes 33 works providing the data, the specific linguistic phenomena, or both.\\nFor example, for readability we use the data of Weischedel et al.\\n(2013) and calculated the flesch score (Flesch, 1948).\\n4Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-analysis.\\n5Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.\\n |  | earlier 2019 | 2020 | 2021 | 2022 | 2023 | Total\\n | --- | --- | --- | --- | --- | --- | ---\\n | ACL | 2 | 10 | 12 | 9 | 34 | 25 | 92\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | AACL | - | - | - | - | 1 | - | 1\\n | COLING | - | - | 10 | - | 9 | - | 19\\n | EACL | - | - | - | 7 | - | 15 | 22\\n | EMNLP | 2 | 4 | 13 | 17 | 21 | - | 57\\n | NAACL | - | 3 | - | 9 | 14 | - | 26\\n | TACL | 1 | 1 | 2 | 3 | 3 | 1 | 11\\n | Workshops | 4 | 4 | 10 | 10 | 7 | 1 | 36\\n | Other | 1 | 2 | 1 | 1 | 1 | 4 | 10\\n | Probing | 10 | 24 | 48 | 56 | 90 | 46 | 274\\n\\nAll Papers 8,056 3,111 3,822 4,294 5,133 3,647 28,063\\nTable 2: Evolution of probing studies.\\nNote that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.\\nMorphology First, we feature 19 tasks verifying morphology phenomena: Anaphor agreement, determiner noun agreement, subject-verb agreement and irregular forms (Warstadt et al., 2020; Huebner et al., 2021).\\nSyntax The second group of 75 tasks verifies the following syntax phenomena: Part-of-speech tagging and constituent labeling (Weischedel et al., 2013); dependency labeling (Silveira et al., 2014); bigram-shift, tree-depth, top-constituent-task, and sentence-length (Conneau et al., 2018); subject- & object-number, and deoncausative-inchoative alternation based on Klafka and Ettinger (2020); binding, control/raising, negative polarity item licensing, island-effects, argument-structure, ellipsis, and filler-gap (Warstadt et al., 2020; Huebner et al., 2021).\\nSemantics Third, consider 67 tasks covering semantics phenomena: Named-entity labeling and semantic-role labeling (Weischedel et al., 2013); subject- and object-number, tense, semantic odd man out, word content, and coordination inversion (Conneau et al., 2018); semantic relation classification (Hendrickx et al., 2010); semantic proto-roles (Rudinger et al., 2018a); factuality (Rudinger et al., 2018b); genericity (Govindarajan et al., 2019); event structure (Gantt et al., 2022); time (Vashishtha et al., 2019); word sense (White et al., 2016); sentiment analysis (Socher et al., 2013); object- and subject-animacy, objectand subject-gender, verb-tense, and verb-dynamic Klafka and Ettinger (2020); metaphor (Mohler et al., 2016; Birke and Sarkar, 2006; Steen et al., 2010); complex word identification (Paetzold and Specia, 2016); and passive (Krasnowska-Kieraś and Wróblewska, 2019).\\nIn addition, we derive synonym-/antonym-detection task using WordNet (Miller, 1995) and the texts from OntoNotes v5 Weischedel et al.\\n(2013).\\nReasoning Forth, 19 tasks cover reasoning phenomena: Paraphrasticity with negation and antonyms (Vahtola et al., 2022); negation detection (Szarvas et al., 2008; Konstantinova et al., 2012; Morante and Blanco, 2012); negation-span classification (Szarvas et al., 2008; Konstantinova et al., 2012); negation-correspondence (Szarvas et al., 2008; Konstantinova et al., 2012); speculation detection, speculation-span classification, and\\nspeculation-correspondence (Szarvas et al., 2008); and always-never, age comparison, objects comparison, antonym negation, property conjunction, taxonomy connection, encyclopedic composition, and multi-hop composition (Talmor et al., 2020).\\nDiscourse Finally, Holmes embodies 28 task addressing discourse phenomena: Co-reference resolution Weischedel et al.\\n(2013); bridging (Hou, 2018, 2020; Pandit and Hou, 2021); discourse connective (Nie et al., 2019); sentence order and next-sentence prediction (Narayan et al., 2018); discourse correspondence, discourse order, discourse relation, discourse distance, discourse explicit classes, discourse implicit classes (Webber et al., 2019; Kurfalı and Östling, 2021); and rst-count/-depth/-distance/-relation/-relationgroup/-successively/-type (Carlson et al., 2001; Koto et al., 2021; Kurfalı and Östling, 2021; Zeldes, 2017).\\nA.4 Details of Probing Dataset Composition\\nWhenever possible, we rely on established probing datasets and transform instances into a unified format: 1) an input x which is either one or a pair of span(s) or sentence(s), including the string and an optional starting and ending index in the context c when task type is either a span or span-pair classification; 2) an optional textual context c to encode x, for example the sentence in which a span occurs; and 3) a corresponding label y. If given, we use the original train/dev/test splits.\\nHowever, if this division does not exist, we use a 70/10/20 ratio to form these splits.\\nFurthermore, we adapt the design of some tasks to map to our task format.\\nExemplary, for the oLMmpics (Talmor et al., 2020) dataset, we transform the mask-filling tasks into a binary classification where the correct label corresponds to a sentence with a correctly filled mask incorrect to a sentence where the mask was filled wrongly.\\nOnToNotes Following Tenney et al.\\n(2019b,a), we use the OntoNotes (Weischedel et al., 2013) dataset to derive part-of-speech tagging, constituent labeling, named-entity labeling, semantic role, and co-reference resolution probing datasets.\\nFurther, we consider with constituent maximum depth and constituent node length further properties of the constituent tree this dataset OntoNotes.\\nDependency Corpus As in Tenney et al.\\n(2019b,a), we use Universal Dependencies annotations of the English Web Treebank to form a dependency labeling datasets.\\nContext Probes Presented in Klafka and Ettinger (2020), we compose nine datasets to verify information about context words.\\nBLiMP Dataset Using the data presented in the BLiMP benchmark (Warstadt et al., 2020), we derive 67 probing datasets verifying specific phenomena, like island effect, covering morphology, syntax, and semantics.\\nUnlike the original version, we compose a binary classification task for every phenomenon.\\nPrecisely, whether to accept or reject a given sentence, where rejecting means that the given linguistic phenomena is violated.\\nZorro Dataset As for the BLiMP tasks, we convert the 21 distinct Zorro tasks into a binary classification task on whether a sentence accepts or rejects the given linguistic phenomena is violated.\\nSemEval-2010 Task 8 For semantic relation classification we rely on the dataset of Hendrickx et al.\\n(2010).\\nDecompositional Semantics Initiative The Decompositional Semantics Initiative6 provides a large number of datasets to verify semantic phenomena.\\nApart of the common use semantic protoroles (Rudinger et al., 2018a), we use their collection of works to compose probing datasets for factuality (Rudinger et al., 2018b), genericity (Govindarajan et al., 2019), event structure (Vashishtha et al., 2019), time (Vashishtha et al., 2019), and word sense (White et al., 2016).\\nSentiment Analysis We use the commonly used work of Socher et al.\\n(2013) and form a probing dataset targeting sentiment.\\nMetaphor As in Aghazadeh et al.\\n(2022), we use the data from Mohler et al.\\n(2016); Birke and Sarkar (2006); Steen et al.\\n(2010) to form three metaphor datasets.\\nComplex Word Identification We consider word complexity for the first time and use the data presented in Paetzold and Specia (2016).\\nIt provides annotations for different complexity levels of words.\\nPassive We use data from Krasnowska-Kieraś and Wróblewska (2019) to form a probing dataset assessing knowledge about passive language.\\nSynonym / Antonym Replacement Using the text of the OntoNotes (Weischedel et al., 2013) and Wordnet (Miller, 1995), we form a probing dataset to detect synonym and antonym replacement.\\nSpecifically, the binary classification task is: given two texts (the original and an updated one), was the updated one changed by replacing a word with its synonym or antonym?\\nNegation With this work, we verify for the first time negation based on human annotated datasets (Vahtola et al., 2022; Szarvas et al., 2008; Konstantinova et al., 2012).\\nSpecifically, we form different probing datasets.\\n• Is a text negated or not?\\n• Given two text spans, does the negation within the first one correspond to the second one?\\n• Given a text span, is it the cue or the scope of the negation?\\noLMmpics We form probing datasets addressing different lexical reasoning using the data presented in Talmor et al.\\n(2020).\\nAs they provide multiple choices, we form correct instances by filling the gap with the correct option and wrong ones by filling in the other options.\\nSpecifically, we form dataset for always-never, age comparison, objects comparison, antonym-negation, multi-hop composition property conjunction, taxonomy conjunction, and encyclopedic composition.\\nBridging We rely on the data presented in Pandit and Hou (2021) and form two probing datasets.\\nOne is to verify whether a text is linguistically applicable, considering bridging (antecedent matches anaphora).\\nAnd a second one to verify whether an antecedent and anaphora match.\\nDiscourse Connective Using data from Nie et al.\\n(2019), we form a probing dataset to assess whether a given connective marker matches the discourse of the given text.\\nSentence Order and Next Sentence Prediction Following Narayan et al.\\n(2018), we form two datasets to verify the order of good or badness of a given sentence and whether two sentences occur after each other.\\nDiscourse Representation Theory We use data from Webber et al.\\n(2019) to compose eight probing datasets addressing discourse representation theory:\\n• Four probing dataset predicting the class of a given span.\\nWe distinguish between implicit, explicit, implicit-coarse, and explicit-coarse.\\n• The absolute distance, number of words, between two spans in the text.\\n• Whether the order of two spans is correct or not.\\n• Whether two spans have discourse relation or not.\\n• The specific discourse relation of two spans.\\nRhetorical Structure Theory Using annotations from Carlson et al.\\n(2001); Zeldes (2017), we compose 14 probing datasets addressing rhetorical theory.\\nSpecifically, we compose the following seven types of datasets for both works:\\n• The rhetorical type of a text span, either nucleus or satellite.\\n• The number of children of a text span within the rhetorical tree of the text.\\n• The depth of a text span within the rhetorical tree of the text.\\n• The number of edges between two text spans within the rhetorical tree.\\n• The specific rhetorical relation between two text spans like conclusion.\\n• The relation group of a specific rhetorical relation between two text spans like evaluation for the relation conclusion.\\n• Whether two text spans occur after each other in the rhetorical tree.\\nPhenomena\\n | anaphor agreement | 3 | ✓ | ✓\\n | --- | --- | --- | ---\\n | determiner noun agreement | 10 | ✓ | ✓\\n | irregular forms | 3 | ✓ | ✓\\n | subject-verb agreement | 10 | ✓ | ✓\\n\\nTable 3: Overview of resources and linguistic phenomena mapping for morphology.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n\\nTable 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n\\nTable 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n\\nTable 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n\\nTable 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.\\n | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n\\nTable 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       " 'A.1 Additional Details on the Evolution of Probing Literature\\nWe analyze publication trends by year and venue as shown in Table 2.\\nLess work was published between 2015-2018 (earlier) focusing on LSTMbased (Linzen et al., 2016; Conneau et al., 2018) and static LMs (Köhn, 2015; Linzen et al., 2016;\\nBelinkov et al., 2017; Conneau et al., 2018).\\nWith the release of BERT (Devlin et al., 2019) in 2019, we note increasing attention to analyzing linguistic abilities within LMs, with a peak of 90 papers in 2022.4 Considering the venue, more than half of the relevant work (149 papers) was published at major conferences (ACL and EMNLP), and 68 papers were published at AACL, EACL, NAACL, and COLING.5 In addition, we observe a constant contribution of TACL, various workshops, such as Analyzing and Interpreting Neural Networks for NLP or Representation Learning for NLP.',\n",
       " 'A.2 Experimental Details\\nProbing Hyperparameters Following previous work (Hewitt and Liang, 2019; Voita and Titov, 2020), we use fixed hyperparameters for training the probes: 20 epochs, where we find the best one using dev instances; AdamW (Loshchilov and Hutter, 2019) as optimizer; a batch size of 64; a learning rate of 0.0005; a dropout rate of 0.2; a warmup rate of 10% of the steps; random seeds: [0, 1, 2, 3, 4] Hardware We run all of our experiments using 12 Nvidia RTX A6000 GPUs.\\nEvery GPU provides 48GB of memory and 10752 CUDA Cores.\\nConsidered LMs Table 8 outlines the details of the LMs we evaluate on Holmes in this work.',\n",
       " 'A.3 Linguistic Task Categorization\\nWe show in Table 3, Table 4, Table 7, Table 5, and Table 6 which resources Holmes use to cover morphology, syntax, semantics, reasoning, and discourse phenomena.\\nThis includes 33 works providing the data, the specific linguistic phenomena, or both.\\nFor example, for readability we use the data of Weischedel et al.\\n(2013) and calculated the flesch score (Flesch, 1948).\\n4Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-analysis.\\n5Note that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.\\n |  | earlier 2019 | 2020 | 2021 | 2022 | 2023 | Total\\n | --- | --- | --- | --- | --- | --- | ---\\n | ACL | 2 | 10 | 12 | 9 | 34 | 25 | 92\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | AACL | - | - | - | - | 1 | - | 1\\n | COLING | - | - | 10 | - | 9 | - | 19\\n | EACL | - | - | - | 7 | - | 15 | 22\\n | EMNLP | 2 | 4 | 13 | 17 | 21 | - | 57\\n | NAACL | - | 3 | - | 9 | 14 | - | 26\\n | TACL | 1 | 1 | 2 | 3 | 3 | 1 | 11\\n | Workshops | 4 | 4 | 10 | 10 | 7 | 1 | 36\\n | Other | 1 | 2 | 1 | 1 | 1 | 4 | 10\\n | Probing | 10 | 24 | 48 | 56 | 90 | 46 | 274\\n\\nAll Papers 8,056 3,111 3,822 4,294 5,133 3,647 28,063\\nTable 2: Evolution of probing studies.\\nNote that EMNLP-23 and AACL-23 proceedings were not published when conducting this meta-study.\\nMorphology First, we feature 19 tasks verifying morphology phenomena: Anaphor agreement, determiner noun agreement, subject-verb agreement and irregular forms (Warstadt et al., 2020; Huebner et al., 2021).\\nSyntax The second group of 75 tasks verifies the following syntax phenomena: Part-of-speech tagging and constituent labeling (Weischedel et al., 2013); dependency labeling (Silveira et al., 2014); bigram-shift, tree-depth, top-constituent-task, and sentence-length (Conneau et al., 2018); subject- & object-number, and deoncausative-inchoative alternation based on Klafka and Ettinger (2020); binding, control/raising, negative polarity item licensing, island-effects, argument-structure, ellipsis, and filler-gap (Warstadt et al., 2020; Huebner et al., 2021).\\nSemantics Third, consider 67 tasks covering semantics phenomena: Named-entity labeling and semantic-role labeling (Weischedel et al., 2013); subject- and object-number, tense, semantic odd man out, word content, and coordination inversion (Conneau et al., 2018); semantic relation classification (Hendrickx et al., 2010); semantic proto-roles (Rudinger et al., 2018a); factuality (Rudinger et al., 2018b); genericity (Govindarajan et al., 2019); event structure (Gantt et al., 2022); time (Vashishtha et al., 2019); word sense (White et al., 2016); sentiment analysis (Socher et al., 2013); object- and subject-animacy, objectand subject-gender, verb-tense, and verb-dynamic Klafka and Ettinger (2020); metaphor (Mohler et al., 2016; Birke and Sarkar, 2006; Steen et al., 2010); complex word identification (Paetzold and Specia, 2016); and passive (Krasnowska-Kieraś and Wróblewska, 2019).\\nIn addition, we derive synonym-/antonym-detection task using WordNet (Miller, 1995) and the texts from OntoNotes v5 Weischedel et al.\\n(2013).\\nReasoning Forth, 19 tasks cover reasoning phenomena: Paraphrasticity with negation and antonyms (Vahtola et al., 2022); negation detection (Szarvas et al., 2008; Konstantinova et al., 2012; Morante and Blanco, 2012); negation-span classification (Szarvas et al., 2008; Konstantinova et al., 2012); negation-correspondence (Szarvas et al., 2008; Konstantinova et al., 2012); speculation detection, speculation-span classification, and\\nspeculation-correspondence (Szarvas et al., 2008); and always-never, age comparison, objects comparison, antonym negation, property conjunction, taxonomy connection, encyclopedic composition, and multi-hop composition (Talmor et al., 2020).\\nDiscourse Finally, Holmes embodies 28 task addressing discourse phenomena: Co-reference resolution Weischedel et al.\\n(2013); bridging (Hou, 2018, 2020; Pandit and Hou, 2021); discourse connective (Nie et al., 2019); sentence order and next-sentence prediction (Narayan et al., 2018); discourse correspondence, discourse order, discourse relation, discourse distance, discourse explicit classes, discourse implicit classes (Webber et al., 2019; Kurfalı and Östling, 2021); and rst-count/-depth/-distance/-relation/-relationgroup/-successively/-type (Carlson et al., 2001; Koto et al., 2021; Kurfalı and Östling, 2021; Zeldes, 2017).',\n",
       " 'A.4 Details of Probing Dataset Composition\\nWhenever possible, we rely on established probing datasets and transform instances into a unified format: 1) an input x which is either one or a pair of span(s) or sentence(s), including the string and an optional starting and ending index in the context c when task type is either a span or span-pair classification; 2) an optional textual context c to encode x, for example the sentence in which a span occurs; and 3) a corresponding label y. If given, we use the original train/dev/test splits.\\nHowever, if this division does not exist, we use a 70/10/20 ratio to form these splits.\\nFurthermore, we adapt the design of some tasks to map to our task format.\\nExemplary, for the oLMmpics (Talmor et al., 2020) dataset, we transform the mask-filling tasks into a binary classification where the correct label corresponds to a sentence with a correctly filled mask incorrect to a sentence where the mask was filled wrongly.\\nOnToNotes Following Tenney et al.\\n(2019b,a), we use the OntoNotes (Weischedel et al., 2013) dataset to derive part-of-speech tagging, constituent labeling, named-entity labeling, semantic role, and co-reference resolution probing datasets.\\nFurther, we consider with constituent maximum depth and constituent node length further properties of the constituent tree this dataset OntoNotes.\\nDependency Corpus As in Tenney et al.\\n(2019b,a), we use Universal Dependencies annotations of the English Web Treebank to form a dependency labeling datasets.\\nContext Probes Presented in Klafka and Ettinger (2020), we compose nine datasets to verify information about context words.\\nBLiMP Dataset Using the data presented in the BLiMP benchmark (Warstadt et al., 2020), we derive 67 probing datasets verifying specific phenomena, like island effect, covering morphology, syntax, and semantics.\\nUnlike the original version, we compose a binary classification task for every phenomenon.\\nPrecisely, whether to accept or reject a given sentence, where rejecting means that the given linguistic phenomena is violated.\\nZorro Dataset As for the BLiMP tasks, we convert the 21 distinct Zorro tasks into a binary classification task on whether a sentence accepts or rejects the given linguistic phenomena is violated.\\nSemEval-2010 Task 8 For semantic relation classification we rely on the dataset of Hendrickx et al.\\n(2010).\\nDecompositional Semantics Initiative The Decompositional Semantics Initiative6 provides a large number of datasets to verify semantic phenomena.\\nApart of the common use semantic protoroles (Rudinger et al., 2018a), we use their collection of works to compose probing datasets for factuality (Rudinger et al., 2018b), genericity (Govindarajan et al., 2019), event structure (Vashishtha et al., 2019), time (Vashishtha et al., 2019), and word sense (White et al., 2016).\\nSentiment Analysis We use the commonly used work of Socher et al.\\n(2013) and form a probing dataset targeting sentiment.\\nMetaphor As in Aghazadeh et al.\\n(2022), we use the data from Mohler et al.\\n(2016); Birke and Sarkar (2006); Steen et al.\\n(2010) to form three metaphor datasets.\\nComplex Word Identification We consider word complexity for the first time and use the data presented in Paetzold and Specia (2016).\\nIt provides annotations for different complexity levels of words.\\nPassive We use data from Krasnowska-Kieraś and Wróblewska (2019) to form a probing dataset assessing knowledge about passive language.\\nSynonym / Antonym Replacement Using the text of the OntoNotes (Weischedel et al., 2013) and Wordnet (Miller, 1995), we form a probing dataset to detect synonym and antonym replacement.\\nSpecifically, the binary classification task is: given two texts (the original and an updated one), was the updated one changed by replacing a word with its synonym or antonym?\\nNegation With this work, we verify for the first time negation based on human annotated datasets (Vahtola et al., 2022; Szarvas et al., 2008; Konstantinova et al., 2012).\\nSpecifically, we form different probing datasets.\\n• Is a text negated or not?\\n• Given two text spans, does the negation within the first one correspond to the second one?\\n• Given a text span, is it the cue or the scope of the negation?\\noLMmpics We form probing datasets addressing different lexical reasoning using the data presented in Talmor et al.\\n(2020).\\nAs they provide multiple choices, we form correct instances by filling the gap with the correct option and wrong ones by filling in the other options.\\nSpecifically, we form dataset for always-never, age comparison, objects comparison, antonym-negation, multi-hop composition property conjunction, taxonomy conjunction, and encyclopedic composition.\\nBridging We rely on the data presented in Pandit and Hou (2021) and form two probing datasets.\\nOne is to verify whether a text is linguistically applicable, considering bridging (antecedent matches anaphora).\\nAnd a second one to verify whether an antecedent and anaphora match.\\nDiscourse Connective Using data from Nie et al.\\n(2019), we form a probing dataset to assess whether a given connective marker matches the discourse of the given text.\\nSentence Order and Next Sentence Prediction Following Narayan et al.\\n(2018), we form two datasets to verify the order of good or badness of a given sentence and whether two sentences occur after each other.\\nDiscourse Representation Theory We use data from Webber et al.\\n(2019) to compose eight probing datasets addressing discourse representation theory:\\n• Four probing dataset predicting the class of a given span.\\nWe distinguish between implicit, explicit, implicit-coarse, and explicit-coarse.\\n• The absolute distance, number of words, between two spans in the text.\\n• Whether the order of two spans is correct or not.\\n• Whether two spans have discourse relation or not.\\n• The specific discourse relation of two spans.\\nRhetorical Structure Theory Using annotations from Carlson et al.\\n(2001); Zeldes (2017), we compose 14 probing datasets addressing rhetorical theory.\\nSpecifically, we compose the following seven types of datasets for both works:\\n• The rhetorical type of a text span, either nucleus or satellite.\\n• The number of children of a text span within the rhetorical tree of the text.\\n• The depth of a text span within the rhetorical tree of the text.\\n• The number of edges between two text spans within the rhetorical tree.\\n• The specific rhetorical relation between two text spans like conclusion.\\n• The relation group of a specific rhetorical relation between two text spans like evaluation for the relation conclusion.\\n• Whether two text spans occur after each other in the rhetorical tree.\\nPhenomena\\n | anaphor agreement | 3 | ✓ | ✓\\n | --- | --- | --- | ---\\n | determiner noun agreement | 10 | ✓ | ✓\\n | irregular forms | 3 | ✓ | ✓\\n | subject-verb agreement | 10 | ✓ | ✓\\n\\nTable 3: Overview of resources and linguistic phenomena mapping for morphology.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n\\nTable 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n\\nTable 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n\\nTable 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n\\nTable 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.\\n | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n\\nTable 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       " 'Phenomena\\n | anaphor agreement | 3 | ✓ | ✓\\n | --- | --- | --- | ---\\n | determiner noun agreement | 10 | ✓ | ✓\\n | irregular forms | 3 | ✓ | ✓\\n | subject-verb agreement | 10 | ✓ | ✓\\n\\nTable 3: Overview of resources and linguistic phenomena mapping for morphology.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n\\nTable 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n\\nTable 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n\\nTable 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.\\nPhenomena\\n | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n\\nTable 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.\\n | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n\\nTable 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       " 'Phenomena\\n | argument-structure | 20 |  |  |  | ✓ ✓\\n | --- | --- | --- | --- | --- | ---\\n | bigram-shift | 1 |  |  | ✓ | \\n | binding | 8 |  |  |  | ✓ ✓\\n | case | 1 |  |  |  | ✓\\n | constituent parsing | 2 | 1 | ✓ |  | \\n | control/raising | 5 |  |  |  | ✓ ✓\\n | deoncausative-inchoative alternation | 1 |  |  | ✓ | \\n | dependency parsing |  | 1 | ✓ |  | \\n | ellipsis | 3 |  |  |  | ✓ ✓\\n | filler-gap | 9 |  |  |  | ✓ ✓\\n | island-effects | 10 |  |  |  | ✓ ✓\\n | local attractor | 1 |  |  |  | ✓\\n | object-number | 2 |  |  | ✓ | ✓\\n | part-of-speech |  | 3 | ✓ ✓ |  | \\n | readability | 1 |  | ✓ | ✓ | \\n | sentence-length | 1 |  |  | ✓ | \\n | subject-number | 2 |  |  | ✓ | ✓\\n | top-constituent-task | 1 |  |  | ✓ | \\n | tree-depth | 1 |  |  | ✓ | \\n\\nTable 4: Overview of resources and linguistic phenomena mapping for syntax.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       " 'Phenomena\\n | age comparison | 1 |  |  | ✓\\n | --- | --- | --- | --- | ---\\n | always-never | 1 |  |  | ✓\\n | antonym negation | 1 |  |  | ✓\\n | encyclopedic composition | 1 |  |  | ✓\\n | multi-hop composition | 1 |  |  | ✓\\n | negation | 3 | 1 | 2 | 2 ✓ ✓ ✓ ✓\\n | objects comparison | 1 |  |  | ✓\\n | property conjunction | 1 |  |  | ✓\\n | speculation | 1 | 1 | 1 | ✓\\n | taxonomy connection | 1 |  |  | ✓\\n\\nTable 5: Overview of resources and linguistic phenomena mapping for reasoning.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       " 'Phenomena\\n | bridging | 1 |  | 1 | ✓ |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | co-reference resolution |  |  | 1 ✓ |  |  | \\n | discourse connective | 1 |  |  | ✓ |  | \\n | discourse representation theory |  |  | 8 | ✓ |  | \\n | next-sentence prediction | 1 |  |  | ✓ |  | \\n | rethorical structure theory |  | 6 | 8 |  | ✓ | ✓\\n | sentence order | 1 |  |  | ✓ |  | \\n\\nTable 6: Overview of resources and linguistic phenomena mapping for discourse.\\nIt shows the number of datasets for the phenomena by dataset type.',\n",
       " 'Phenomena\\n | complex word identification |  |  | 1 |  |  |  |  |  |  |  |  | ✓ | \\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | coordination inversion | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | event structure |  |  | 4 | 2 |  |  |  |  | ✓ |  |  |  | \\n | factuality |  |  |  | 1 |  |  |  |  | ✓ |  |  |  | \\n | genericity |  |  | 6 |  |  |  |  |  | ✓ |  |  |  | \\n | metaphor |  |  | 4 |  |  |  |  |  |  |  |  | ✓ ✓ ✓ | \\n | named-entity labeling |  |  | 1 |  | ✓ |  |  |  |  |  |  |  | \\n | negative polarity item licensing | 4 |  |  |  |  |  | ✓ ✓ |  |  |  |  |  | \\n | object-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | object-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | passive | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | quantifiers | 6 |  |  |  |  |  | ✓ |  |  |  |  |  | \\n | semantic relation classification |  | 1 |  |  |  |  |  | ✓ |  |  |  |  | \\n | semantic proto-roles |  |  |  | 20 |  |  |  |  | ✓ |  |  |  | \\n | semantic odd man out | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | semantic-role labeling |  |  |  | 1 | ✓ |  |  |  |  |  |  |  | \\n | sentiment analysis | 1 |  |  |  |  |  |  |  |  |  |  | ✓ | \\n | subject-animacy | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | subject-gender | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | synonym-/antonym-detection | 1 |  |  |  |  |  |  |  |  |  |  |  | ✓\\n | tense | 2 |  |  |  |  | ✓ ✓ |  |  |  |  |  |  | \\n | time |  |  | 1 |  |  |  |  |  |  | ✓ |  |  | \\n | verb-dynamic | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word content | 1 |  |  |  |  | ✓ |  |  |  |  |  |  | \\n | word sense |  |  | 1 |  |  |  |  |  |  |  | ✓ |  | \\n\\nTable 7: Overview of resources and linguistic phenomena mapping for semantics.\\nIt shows the number of datasets for the phenomena by dataset type.\\n | Model | Citation | Size | Pre-Training Objective | Pre-Training Data | Huggingface Tag\\n | --- | --- | --- | --- | --- | ---\\n | Encoder-Only Language Models\\n | ALBERT | Lan et al. (2020) | 10 million | MLM+SOP | 16GB | albert-base-v2\\n | BERT | Tenney et al. (2019a) | 110 million | MLM+NSP | 16GB | bert-base-uncased\\n | DeBERTa | He et al. (2021) | 100 million | MLM | 80GB | microsoft/deberta-base\\n | DeBERTa-v3 | He et al. (2023) | 86 million | MLM+DISC | 160GB | microsoft/deberta-v3-base\\n | ELECTRA | Clark et al. (2020) | 110 million | MLM | 16GB | google/electra-base-discriminator\\n | RoBERTa | Liu et al. (2019) | 110 million | MLM+DISC | 160GB | roberta-base\\n | Decoder-Only Language Models\\n | GPT2 | Radford et al. (2019) | 117 million | LM | 40GB | gpt2\\n | Pythia-70m | Biderman et al. (2023) | 70 million | LM | 300 billion tokens | EleutherAI/pythia-70m\\n | Pythia-160m | Biderman et al. (2023) | 160 million | LM | 300 billion tokens | EleutherAI/pythia-160m\\n | Pythia-410m | Biderman et al. (2023) | 410 million | LM | 300 billion tokens | EleutherAI/pythia-410m\\n | Pythia-1b | Biderman et al. (2023) | 1 billion | LM | 300 billion tokens | EleutherAI/pythia-1B\\n | Pythia-1.4b | Biderman et al. (2023) | 1.4 billion | LM | 300 billion tokens | EleutherAI/pythia-1.4B\\n | Pythia-2.8b | Biderman et al. (2023) | 2.8 billion | LM | 300 billion tokens | EleutherAI/pythia-2.8B\\n | Pythia-6.9b | Biderman et al. (2023) | 6.9 billion | LM | 300 billion tokens | EleutherAI/pythia-6.9B\\n | Pythia-12b | Biderman et al. (2023) | 12 billion | LM | 300 billion tokens | EleutherAI/pythia-12B\\n | Pythia-70m-dedup | Biderman et al. (2023) | 70 million | LM | 207 billion tokens | EleutherAI/pythia-70m-deduped\\n | Pythia-160m-dedup | Biderman et al. (2023) | 160 million | LM | 207 billion tokens | EleutherAI/pythia-160m-deduped\\n | Pythia-410m-dedup | Biderman et al. (2023) | 410 million | LM | 207 billion tokens | EleutherAI/pythia-410m-deduped\\n | Pythia-1b-dedup | Biderman et al. (2023) | 1 billion | LM | 207 billion tokens | EleutherAI/pythia-1B-deduped\\n | Pythia-1.4b-dedup | Biderman et al. (2023) | 1.4 billion | LM | 207 billion tokens | EleutherAI/pythia-1.4B-deduped\\n | Pythia-2.8b-dedup | Biderman et al. (2023) | 2.8 billion | LM | 207 billion tokens | EleutherAI/pythia-2.8B-deduped\\n | Pythia-6.9b-dedup | Biderman et al. (2023) | 6.9 billion | LM | 207 billion tokens | EleutherAI/pythia-6.9B-deduped\\n | Pythia-12b-dedup | Biderman et al. (2023) | 12 billion | LM | 207 billion tokens | EleutherAI/pythia-12B-deduped\\n | Dolly-v2 | Conover et al. (2023) | 12 billion | LM+IT | 300 billion token + 15K instructions | databricks/dolly-v2-12b\\n | Llama-2-7b | Touvron et al. (2023) | 7 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-7b-hf\\n | Llama-2-13b | Touvron et al. (2023) | 13 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-13b-hf\\n | Llama-2-70b | Touvron et al. (2023) | 70 billion | LM | 2.4 trillion tokens | meta-llama/Llama-2-70b-hf\\n | Llama-2-7b-chat | Touvron et al. (2023) | 7 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-7b-chat-hf\\n | Llama-2-13b-chat | Touvron et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-13b-chat-hf\\n | Llama-2-70b-chat | Touvron et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 27,5K instructions | meta-llama/Llama-2-70b-chat-hf\\n | IBM-Merlinite | Sudalairaj et al. (2024) | 7 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/merlinite-7b\\n | IBM-Labradorite | Sudalairaj et al. (2024) | 13 billion | LM+IT | 2.4 trillion tokens + 1400k instructions | ibm/labradorite-13b\\n | Vicuna-13b-v1.5 | Zheng et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 125k instructions | lmsys/vicuna-13b-v1.5\\n | Orca-2-13b | Mitra et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 817K instructions | microsoft/Orca-2-13b\\n | Wizard-13B-v1.2 | Xu et al. (2023) | 13 billion | LM | unknown | WizardLM/WizardLM-13B-V1.2\\n | Tülu-2-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-13b\\n | Tülu-2-dpo-13b | Wang et al. (2023) | 13 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-13b\\n | Tülu-2-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | allenai/tulu-2-70b\\n | Tülu-2-dpo-70b | Wang et al. (2023) | 70 billion | LM+IT | 2.4 trillion tokens + 330k instructions | tulu-2-dpo-70b\\n | Mistral-7b | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Mistral-7b-Inst | Jiang et al. (2023) | 7 billion | LM | unknown | mistralai/Mistral-7B-Instruct-v0.1\\n | Mixtral-8x7b | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mixtral-8x7B-v0.1\\n | Mixtral-8x7b-Inst | Jiang et al. (2024) | 47 billion | LM | unknown | mistralai/Mistral-7B-v0.1\\n | Encoder-Decoder Language Models\\n | BART | Lewis et al. (2020) | 121 million | DAE | 160GB | google/facebook/bart-base\\n | T5-small | Raffel et al. (2020) | 60 million | DAE | 800GB | google/t5-small-lm-adapt\\n | T5-base | Raffel et al. (2020) | 220 million | DAE | 800GB | google/t5-base-lm-adapt\\n | T5-large | Raffel et al. (2020) | 770 million | DAE | 800GB | google/t5-large-lm-adapt\\n | T5-xl | Raffel et al. (2020) | 3 billion | DAE | 800GB | google/t5-xl-lm-adapt\\n | T5-xxl | Raffel et al. (2020) | 11 billion | DAE | 800GB | google/t5-xxl-lm-adapt\\n | FLAN-T5-small | Raffel et al. (2020) | 60 million | DAE+IT | 800GB + 1.8k tasks | google/t5-small-lm-adapt\\n | FLAN-T5-base | Raffel et al. (2020) | 220 million | DAE+IT | 800GB + 1.8k tasks | google/t5-base-lm-adapt\\n | FLAN-T5-large | Raffel et al. (2020) | 770 million | DAE+IT | 800GB + 1.8k tasks | google/t5-large-lm-adapt\\n | FLAN-T5-xl | Raffel et al. (2020) | 3 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xl-lm-adapt\\n | FLAN-T5-xxl | Raffel et al. (2020) | 11 billion | DAE+IT | 800GB + 1.8k tasks | google/t5-xxl-lm-adapt\\n | TK-Instruct | Wang et al. (2022) | 11 billion billion | DAE+IT | 800GB + 1.6k tasks | allenai/tk-instruct-11b-def\\n | UL2 | Tay et al. (2023) | 20 billion | DAE | 800GB | google/ul2\\n | FLAN-UL2 | Tay et al. (2023) | 20 billion | DAE+IT | 800GB + 100k instructions | google/flan-ul2\\n | Static Language Models\\n | Glove-6B | Pennington et al. (2014) | - | WP | 6 billion tokens | glove.6B.300d\\n | Glove-840B | Pennington et al. (2014) | - | WP | 840 billion tokens | glove.840B.300d\\n\\nTable 8: Overview of the evaluated LMS covering the corresponding citation, model size, model architecture, pre-training objective & data, and the Huggingface model tag.\\nRegarding the pre-training objective, we distinguish between masked language modeling (MLM), sentence order prediction (SOP), next sentence prediction (NSP), next word prediction (LM), instruction fine-tuning (IT), word denoising (DAE), and word probabilities from word co-occurrences (WP).\\nFor pre-training data, we report known numbers, either as the size of the corpora in gigabytes (GB), the number of pre-training tokens, the number of instructions for fine-tuning, or the number of tasks for instruction fine-tuning.',\n",
       " 'Stylus: Automatic Adapter Selection for Diffusion Models',\n",
       " 'Michael Luo UC Berkeley\\nmichael.luo@berkeley.edu jegonzal@berkeley.edu wong.justin@berkeley.edu zhifengc@google.com brandon@btrabucco.com rsalakhu@cs.cmu.edu huangyp@google.com istoica@berkeley.edu',\n",
       " 'Joseph E. Gonzalez UC Berkeley',\n",
       " 'Justin Wong UC Berkeley',\n",
       " 'Zhifeng Chen Google Deepmind',\n",
       " 'Brandon Trabucco CMU MLD',\n",
       " 'Ruslan Salakhutdinov CMU MLD',\n",
       " 'Yanping Huang Google Deepmind',\n",
       " 'Ion Stoica UC Berkeley\\nStylus\\nSD\\n\\n | v1.5 |  | \\n | --- | --- | ---\\n | “This LoRA generates huskies…” |  |  |  | “This LoRA generates snowboards…”\\n | --- | --- | --- | --- | ---\\n |  | Wooden dish rack on a counter holding plates, saucers, a bowl, mugs and glasses. | \\n | A boy holding an umbrella on the edge of a cliff.\\n | Figure 1. Adapter Selection. Given a user-provided prompt, our method identifies highly relevant adapters (e.g. Low-Rank Adaptation, LoRA) that are closely aligned with the prompt’s context and at least one of the prompt’s keywords. Composing relevant adapters into Sta- ble Diffusion improves visual fidelity, image diversity, and textual alignment. Note that these prompts are sampled from MS-COCO [19].\\n | A open field with large elephants standing in it. greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.\\n | Abstract\\n | Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by opensource communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions. To generate high quality images, This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt’s keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts’ keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves\\n | 1. Introduction\\n | In the evolving field of generative image models, finetuned adapters [7, 9] have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different adapters and model checkpoints, fueling the proliferation of creative AI art [24, 43]. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA) [12] emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.\\n | In light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts\\n | Refiner Composer\\n | Below is an ordered list of adapter descriptions based on relevance:\\n | … “This LoRA generates [XYZ]” #0: This LoRA generates huskies… #842: This LoRA introduces the style of Christmas cards… #3: This LoRA generates snowboards… …\\n | #3 #2 #1 #0 LoRA\\n | … #1 #2 #3 #4#0\\n | and the prompt “Two dogs playing in the snow”.\\n | Vector DB\\n | Adapter DB\\n | First, segment the prompt into different keywords. Second, for each keyword, identify the relevant adapters which directly match the keyword and the prompt’s context.\\n | Retriever\\n | Two dogs playing in the snow.\\n | #0 |  | #242 … | #3 | \\n\\n“dogs” “snow” “This LoRA makes snow more powdery…” “This LoRA generates high-quality images of dogs” #72 #72 #1337 #0 #242 Figure 2.\\nStylus algorithm.\\nStylus consists of three stages.\\nThe refiner plugs an adapter’s model card through a VLM to generate textual descriptions of an adapter’s task and then through an encoder to produce the corresponding text embedding.\\nThe retriever fetches candidate adapters that are relevant to the entire user prompt.\\nFinally, the composer prunes and jointly categorizes the remaining adapters based on the prompt’s tasks, which correspond to a set of keywords.\\n(see Fig. 1).\\nHowever, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings [16].\\nSpecifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data—a common issue on open-source platforms.\\nFurthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks.\\nFor instance, the prompt “two dogs playing the snow” suggests that there are two tasks: generating images of “dogs” and “snow”.\\nThis necessitates segmenting the prompt into various tasks (i.e. keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems [8].\\nFinally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).\\n100000\\n75000\\n50000\\n | 25000\\n | ---\\n | SD 1.5 | SDXL 1.0 | SD 1.5 | SDXL 1.0 0\\n | --- | --- | --- | ---\\n | We propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the refiner plugs in an adapter’s model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods [16], the retriever scores the relevance of each embedding against the user’s entire prompt to retrieve a set of candidate adapters. Finally, the composer segments the prompt into disjoint tasks, further prunes irrelevant can- didate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that\\n | LoRA | Textual Inversion |  | \\n | Civit AI |  | Hugging Face | \\n\\nFigure 3.\\nNumber of Adapters.\\nCivit AI boasts 100K+ adapters for Stable Diffusion, outpacing that of Hugging Face.\\nLow-Rank Adaptation (LoRA) is the dominant approach for finetuning.\\nintroduce biases detrimental to image generation (§ 4.3).\\nFinally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.\\nTo evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs1, that contains pre-computed adapter documentations and embeddings from Stylus’s refiner.\\nOur results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints—shifting the CLIP-FID Pareto curve towards greater efficiency and achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators.\\nAs a system, Stylus is practical and does not present large overheads to the image generation process.\\nFinally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation.\\n1Sourced from https://civitai.com/ [24].',\n",
       " 'Stylus\\nSD\\n\\n | v1.5 |  | \\n | --- | --- | ---\\n | “This LoRA generates huskies…” |  |  |  | “This LoRA generates snowboards…”\\n | --- | --- | --- | --- | ---\\n |  | Wooden dish rack on a counter holding plates, saucers, a bowl, mugs and glasses. | \\n | A boy holding an umbrella on the edge of a cliff.\\n | Figure 1. Adapter Selection. Given a user-provided prompt, our method identifies highly relevant adapters (e.g. Low-Rank Adaptation, LoRA) that are closely aligned with the prompt’s context and at least one of the prompt’s keywords. Composing relevant adapters into Sta- ble Diffusion improves visual fidelity, image diversity, and textual alignment. Note that these prompts are sampled from MS-COCO [19].\\n | A open field with large elephants standing in it. greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.\\n | Abstract\\n | Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by opensource communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions. To generate high quality images, This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt’s keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts’ keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves\\n | 1. Introduction\\n | In the evolving field of generative image models, finetuned adapters [7, 9] have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different adapters and model checkpoints, fueling the proliferation of creative AI art [24, 43]. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA) [12] emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.\\n | In light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts\\n | Refiner Composer\\n | Below is an ordered list of adapter descriptions based on relevance:\\n | … “This LoRA generates [XYZ]” #0: This LoRA generates huskies… #842: This LoRA introduces the style of Christmas cards… #3: This LoRA generates snowboards… …\\n | #3 #2 #1 #0 LoRA\\n | … #1 #2 #3 #4#0\\n | and the prompt “Two dogs playing in the snow”.\\n | Vector DB\\n | Adapter DB\\n | First, segment the prompt into different keywords. Second, for each keyword, identify the relevant adapters which directly match the keyword and the prompt’s context.\\n | Retriever\\n | Two dogs playing in the snow.\\n | #0 |  | #242 … | #3 | \\n\\n“dogs” “snow” “This LoRA makes snow more powdery…” “This LoRA generates high-quality images of dogs” #72 #72 #1337 #0 #242 Figure 2.\\nStylus algorithm.\\nStylus consists of three stages.\\nThe refiner plugs an adapter’s model card through a VLM to generate textual descriptions of an adapter’s task and then through an encoder to produce the corresponding text embedding.\\nThe retriever fetches candidate adapters that are relevant to the entire user prompt.\\nFinally, the composer prunes and jointly categorizes the remaining adapters based on the prompt’s tasks, which correspond to a set of keywords.\\n(see Fig. 1).\\nHowever, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings [16].\\nSpecifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data—a common issue on open-source platforms.\\nFurthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks.\\nFor instance, the prompt “two dogs playing the snow” suggests that there are two tasks: generating images of “dogs” and “snow”.\\nThis necessitates segmenting the prompt into various tasks (i.e. keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems [8].\\nFinally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).\\n100000\\n75000\\n50000\\n | 25000\\n | ---\\n | SD 1.5 | SDXL 1.0 | SD 1.5 | SDXL 1.0 0\\n | --- | --- | --- | ---\\n | We propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the refiner plugs in an adapter’s model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods [16], the retriever scores the relevance of each embedding against the user’s entire prompt to retrieve a set of candidate adapters. Finally, the composer segments the prompt into disjoint tasks, further prunes irrelevant can- didate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that\\n | LoRA | Textual Inversion |  | \\n | Civit AI |  | Hugging Face | \\n\\nFigure 3.\\nNumber of Adapters.\\nCivit AI boasts 100K+ adapters for Stable Diffusion, outpacing that of Hugging Face.\\nLow-Rank Adaptation (LoRA) is the dominant approach for finetuning.\\nintroduce biases detrimental to image generation (§ 4.3).\\nFinally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.\\nTo evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs1, that contains pre-computed adapter documentations and embeddings from Stylus’s refiner.\\nOur results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints—shifting the CLIP-FID Pareto curve towards greater efficiency and achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators.\\nAs a system, Stylus is practical and does not present large overheads to the image generation process.\\nFinally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation.\\n1Sourced from https://civitai.com/ [24].',\n",
       " 'SD\\n\\n | v1.5 |  | \\n | --- | --- | ---\\n | “This LoRA generates huskies…” |  |  |  | “This LoRA generates snowboards…”\\n | --- | --- | --- | --- | ---\\n |  | Wooden dish rack on a counter holding plates, saucers, a bowl, mugs and glasses. | \\n | A boy holding an umbrella on the edge of a cliff.\\n | Figure 1. Adapter Selection. Given a user-provided prompt, our method identifies highly relevant adapters (e.g. Low-Rank Adaptation, LoRA) that are closely aligned with the prompt’s context and at least one of the prompt’s keywords. Composing relevant adapters into Sta- ble Diffusion improves visual fidelity, image diversity, and textual alignment. Note that these prompts are sampled from MS-COCO [19].\\n | A open field with large elephants standing in it. greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.\\n | Abstract\\n | Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by opensource communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions. To generate high quality images, This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt’s keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts’ keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves\\n | 1. Introduction\\n | In the evolving field of generative image models, finetuned adapters [7, 9] have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different adapters and model checkpoints, fueling the proliferation of creative AI art [24, 43]. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA) [12] emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.\\n | In light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts\\n | Refiner Composer\\n | Below is an ordered list of adapter descriptions based on relevance:\\n | … “This LoRA generates [XYZ]” #0: This LoRA generates huskies… #842: This LoRA introduces the style of Christmas cards… #3: This LoRA generates snowboards… …\\n | #3 #2 #1 #0 LoRA\\n | … #1 #2 #3 #4#0\\n | and the prompt “Two dogs playing in the snow”.\\n | Vector DB\\n | Adapter DB\\n | First, segment the prompt into different keywords. Second, for each keyword, identify the relevant adapters which directly match the keyword and the prompt’s context.\\n | Retriever\\n | Two dogs playing in the snow.\\n | #0 |  | #242 … | #3 | \\n\\n“dogs” “snow” “This LoRA makes snow more powdery…” “This LoRA generates high-quality images of dogs” #72 #72 #1337 #0 #242 Figure 2.\\nStylus algorithm.\\nStylus consists of three stages.\\nThe refiner plugs an adapter’s model card through a VLM to generate textual descriptions of an adapter’s task and then through an encoder to produce the corresponding text embedding.\\nThe retriever fetches candidate adapters that are relevant to the entire user prompt.\\nFinally, the composer prunes and jointly categorizes the remaining adapters based on the prompt’s tasks, which correspond to a set of keywords.\\n(see Fig. 1).\\nHowever, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings [16].\\nSpecifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data—a common issue on open-source platforms.\\nFurthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks.\\nFor instance, the prompt “two dogs playing the snow” suggests that there are two tasks: generating images of “dogs” and “snow”.\\nThis necessitates segmenting the prompt into various tasks (i.e. keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems [8].\\nFinally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).\\n100000\\n75000\\n50000\\n | 25000\\n | ---\\n | SD 1.5 | SDXL 1.0 | SD 1.5 | SDXL 1.0 0\\n | --- | --- | --- | ---\\n | We propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the refiner plugs in an adapter’s model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods [16], the retriever scores the relevance of each embedding against the user’s entire prompt to retrieve a set of candidate adapters. Finally, the composer segments the prompt into disjoint tasks, further prunes irrelevant can- didate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that\\n | LoRA | Textual Inversion |  | \\n | Civit AI |  | Hugging Face | \\n\\nFigure 3.\\nNumber of Adapters.\\nCivit AI boasts 100K+ adapters for Stable Diffusion, outpacing that of Hugging Face.\\nLow-Rank Adaptation (LoRA) is the dominant approach for finetuning.\\nintroduce biases detrimental to image generation (§ 4.3).\\nFinally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.\\nTo evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs1, that contains pre-computed adapter documentations and embeddings from Stylus’s refiner.\\nOur results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints—shifting the CLIP-FID Pareto curve towards greater efficiency and achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators.\\nAs a system, Stylus is practical and does not present large overheads to the image generation process.\\nFinally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation.\\n1Sourced from https://civitai.com/ [24].',\n",
       " '2. Related Works\\nAdapters.\\nAdapters efficiently fine-tune models on specific tasks with minimal parameter changes, reducing computational and storage requirements while maintaining similar performance to full fine-tuning [7, 9, 12].\\nOur study focuses on retrieving and merging multiple Low-Rank adapters (LoRA), the popular approach within existing open-source communities [24, 25, 43].\\nAdapter composition has emerged as a crucial mechanism for enhancing the capabilities of foundational models across various applications [17, 30, 34, 38, 39].\\nFor large language models (LLM), the linear combination of multiple adapters improves in-domain performance and cross-task generalization [3, 13, 14, 40, 41, 46].\\nIn the image domain, merging LoRAs effectively composes different tasks—concepts, characters, poses, actions, and styles—together, yielding images of high fidelity that closely align with user specifications [21, 47].\\nOur approach advances this further by actively segmenting user prompts into distinct tasks and merging the appropriate adapters for each task.\\nRetrieval-based Methods.\\nRetrieval-based methods, such as retrieval-augmented generation (RAG), significantly improve model responses by adding semantically similar texts from a vast external database [16].\\nThese methods convert text to vector embeddings using text encoders, which are then ranked against a user prompt based on similarity metrics [4, 8, 18, 23, 31, 33].\\nSimilarly, our work draws inspiration from RAG to encode adapters as vector embedings: leveraging visual-language foundational models (VLM) to generate semantic descriptions of adapters, which are then translated into embeddings.\\nA core limitation to RAG is limited precision, retrieving distracting irrelevant documents.\\nThis leads to a”needlein-the-haystack” problem, where more relevant documents are buried further down the list [8].\\nRecent work introduce reranking step; this technique uses cross-encoders to assess both the raw user prompt and the ranked set of raw texts individually, thereby discovering texts based on actual relevance [23, 32].\\nRerankers have been successfully integrated with various LLM-application frameworks [2, 20, 29].\\n3. Our Method: Stylus\\nAdapter selection presents distinct challenges compared to existing methods for retrieving text-based documents, as outlined in Section 2.\\nFirst, computing embeddings for adapters is a novel task, made more difficult without access to training datasets.\\nFurthermore, in the context of image generation, user prompts often specify multiple highly fine-grained tasks.\\nThis challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt.\\nFinally, composing multiple adapters can degrade image quality and inject foreign biases into the model.\\nOur three-stage framework below—Refine, Retrieve, and Compose—addresses the above challenges (Fig. 2).\\n3.1. Refiner\\nThe refiner is a two-stage pipeline designed to generate textual descriptions of an adapter’s task and the corresponding text embeddings for retrieval purposes.\\nThis approach mirrors retrieval-based methods [16], which pre-compute embeddings over an external database of texts.\\nGiven an adapter Ai, the first stage is a vision-language model (VLM) that takes in the adapter’s model card—a set of randomly sampled example images from the model card Ii ∈ {Ii1, Ii2,}, the corresponding prompts Pi ∈ {pi1, pi2,}, and an author-provided description,2 Di—and returns an improved description D∗ i. Optionally, the VLM also recommends the weight for LoRA-based adapters, as the adapter weight is usually specified either in the author’s description Di or the set of prompts Pi, a feature present in popular image generation software [1].\\nIf information cannot be found, the LoRA’s weight is set to 0.8.\\nIn our experiments, these improved descriptions were generated by Gemini Ultra [37] (see § A.1 for prompt).\\nThe second stage uses an embedding model (E) to generate embeddings e = E(D∗) for all adapters.\\nIn our experiments, we create embeddings from OpenAI’s text-embedding-3-large model [18, 26].\\nWe store pre-computed embeddings in a vector database.\\n3.2. Retriever\\nThe retriever fetches the most relevant adapters over the entirety of the user’s prompt using similarity metrics.\\nMathematically, the retriever employs the same embedding model (E) to process the user prompt, s, generating embedding es = E(s).\\nIt then calculates cosine similarity scores between the prompt’s embedding es and the embedding of each adapter in the matrix M. The top K adapters AK (K = 150, in our experiments) are selected based on the highest similarity scores: AK = arg top-Ki\\n(es·Mi ∥es∥∥Mi∥) , where Mi is the ith row of the embedding matrix, representing the ith adapter’s embedding.\\n3.3. Composer\\nThe composer serves a dual purpose: segmenting the prompt into tasks from a prompt’s keywords and assigning retrieved adapters to tasks.\\nThis implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the 2We note that a large set of author descriptions are inaccurate, misleading, or absent.\\nThe refiner helped correct for human errors by using generated images as the ground truth, significantly improving our system.\\nFigure 4.\\nQualitative comparison between Stylus over realistic (left) and cartoon (right) style Stable Diffusion checkpoints.\\nStylus produces highly detailed images that correctly depicts keywords in the context of the prompt.\\nFor the prompt “A graffiti of a corgi on the wall”, our method correctly depicts a spray-painted corgi, whereas the checkpoint generates a realistic dog.\\n |  | RealisticVision-v6 / COCO | RealisticVision-v6 / PartiPrompts | Counterfeit-v3 / COCO | Counterfeit-v3 / PartiPrompts\\n | --- | --- | --- | --- | ---\\n | 80\\n | Stylus SD 1.5 | 68.31% | 67.81% | 62.00% | 58.57%\\n\\n60\\n41.43% 38.00%\\n40\\n32.19% 31.69%\\n20\\n0\\nFigure 5.\\nHuman Evaluation.\\nStylus achieves a higher preference scores (2:1) over different datasets and Stable Diffusion checkpoints.\\nprompt through keyword grounding.\\nFor example, if the prompt is “pandas eating bamboo”, the composer may discard an irrelevant “grizzly bears” adapter and a biased “panda mascots” adapter.\\nMathematically, the composer (C) takes in the prompt (s) and the top K adapters (AK) from the retriever, classifying them over different tasks,\\nT (s) = {t1, t2,, tn}.\\nThis can be expressed as: C(s,AK) = {(ti,Aki) | ti ∈ T (s),Aki ⊆ AK, ∀Aj ∈ Aki, Sim(Aj, ti) ≥ τ} (1), where Aki is the subset of adapters per task ti, Sim(Aj, ti) measures the similarity score between an adapter and a task, and τ is an arbitary threshold.\\nWhile the composer can be trained with human-labeled data [28], we opt for a simpler approach that requires no training—prompting a long-context Large Language Model (LLM).\\nThe LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters.\\nIn our implementation, we choose Gemini 1.5, with a 128K context window, as the composer’s LLM (see App. A.2 for the full prompt).\\nMost importantly, Stylus’s composer parallels reranking, an advanced RAG technique.\\nRerankers employ cross encoders (F) that compare the retriever’s individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores: F(s, D∗).\\nThis prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment.\\nOur experimental ablations (§ 4.3) show that our composer outperforms existing rerankers (Cohere,\\nrerank-english-v2.0) [32].\\n3.4. Masking\\n | The composer identifies tasks assigns each task a set of relevant adapters as: C(Ak, s) = {(t1,Ak1), (t2,Ak2), | ti | from the prompt s Aki, formalized }. Our masking | and\\n | --- | --- | --- | ---\\n | strategy applies a binary mask, mask, Mi, can either be an one hot encoding, all ones, or all zeroes vector. Across all tasks, we perform a cross- product across masks, M1 ×M2 ×M3 × | Mi, | for each task ti. Each , generating | an\\n\\nexponential number of masking schemes.\\nThe combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images (§ 4.2.3).\\nThis approach also curtails the number of final adapters merged into the base model, minimizing the risk of composed adapters introducing undesirable effects to the image [47].\\nFinally, an adapter’s weight (i.e. LoRA), which is extracted from the refiner (§ 3.1), is divided by the total number of adapters (after masking) in its corresponding task.\\nThis solves the problem of image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality (see App. A.4).\\n4. Results\\n4.1. Experimental Setup\\nAdapter Testbed.\\nAdapter selection requires a large database of adapters to properly evaluate its performance.\\n24\\n23\\n22\\n21\\n20\\nSD 1.5\\n19\\nStylus\\n |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n\\nTable 1.\\nEvaluation over different retrieval methods (CFG=6).\\nStylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.\\nHowever, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:\\nRealistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.\\n4.2. Main Experiments\\n4.2.1 Human Evaluation.\\nTo demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.\\nTo conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.\\n4.2.2 Automatic Benchmarks.\\nWe assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.\\n4.2.3 VLM as a Judge\\nWe use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.\\nFigure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.\\n Lose Tie Win\\n100\\n20 29\\n80\\n59\\n60\\n58 52\\n40\\n20\\n41 19 22\\nPrompt Length\\n0\\n | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n\\n(a) VLM as a Judge with GPT-4V\\nFigure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.\\nLose Tie Win\\n100\\n35\\n80\\n60\\n40\\n40\\n20\\n |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n\\nuate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.\\n4.2.4 Diversity per Prompt\\nGiven identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:\\ncalculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.\\nGPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,\\n4FID fails to disentangle image fidelity from diversity [27, 35].\\nStylus\\nReranker\\nRetriever\\nRandom\\nA baby elephant underneath an adult elephant.\\n16s\\n25s\\nA building behind a stop sign and street sign.\\n44s\\nTime (s)\\nA group of friends jumping on a bed.\\nRetriever Composer Load Adapter Image Generation\\n | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n\\nFigure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.\\n | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n\\nFigure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.\\nAs shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.\\n4.3. Ablations\\n4.3.1 Alternative Retrieval-based Methods\\nWe benchmark Stylus’s performance relative to different retrieval methods.\\nFor all baselines below, we select the top three adapters and merge them into the base model.\\nRandom: Adapters are randomly sampled without replacement from StylusDocs.\\nRetriever: The retriever emulates standard RAG pipelines [16, 46], functionally equivalent to Stylus without the composer stage.\\nTop adapters are fetched via cosine similarity over adapter embeddings.\\nConversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion.\\nThe random baseline chooses adapters that are orthogonal to the user prompt.\\nThus, these adapters alter unrelated concepts, which does not affect image generation.\\nIn fact, we observed that the distribution of random policy’s images in Fig. 10 were nearly identical to Stable Diffusion.\\n |  | Stylus | Stylus\\n | --- | --- | ---\\n | Source | SD v1.5 | Source | SD v1.5\\n | --- | --- | --- | ---\\n\\nVolcanoes spewing lava\\nStudio Ghibli Style\\nFiery red Voxel Style Pencil Sketch Van Gogh Style\\n(a) Image Translation\\nStylus chooses relevant adapters that better adapt new styles and elements into existing images.\\nStylus Stylus\\nSD v1.5 SD v1.5\\nSource Source\\nDwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny\\n(b) Inpainting\\nStylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.\\nFigure 12.\\nStylus over different image-to-image tasks.\\n4.3.2 Breakdown of Stylus’s Inference Time\\nThis section breaks down the latency introduced by various components of Stylus.\\nWe note that image generation time is independent of Stylus, as adapter weights are merged into the base model [12].\\nFigure 11 demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts.\\nSpecifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds.\\nThe composer’s large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens.\\nFinally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process.\\nHowever, Stylus’s latency remains consistent across all batch sizes, as the composer and retriever run only once.\\nHence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases.\\n4.3.3 Image-Domain Tasks\\nBeyond text-to-image, Stylus applies across various imageto-image tasks.\\nFig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting, described as follows:\\nImage translation: Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt’s definition.\\nStylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style.\\nWe present examples in Fig 12a.\\nFor a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits.\\nFor a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.\\nInpainting: Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask.\\nStylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity.\\nWe provide further examples in Fig. 12b, demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right).\\n5. Conclusion\\nWe propose Stylus, a new algorithm that automatically selects and composes adapters to generate better images.\\nOur method leverages a three-stage framework that precomputes adapters as lookup embeddings and retrieves most relevant adapters based on prompts’ keywords.\\nTo evaluate Stylus, we develop StylusDocs, a processed dataset featuring 75K adapters and pre-computed adapter embeddings.\\nOur evaluation of Stylus, across automatic metrics, humans, and vision-language models, demonstrate that Stylus achieves better visual fidelity, textual alignment, and image diversity than existing Stable Diffusion checkpoints.\\nAcknowledgement\\nWe thank Lisa Dunlap, Ansh Chaurasia, Siyuan Zhuang, Sijun Tan, Tianjun Zhang, and Shishir Patil for their insightful discussion.\\nWe thank Google Deepmind for funding this project, providing AI infrastructure, and provisioning Gemini endpoints.\\nSky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, SAP, Uber, and VMware.',\n",
       " '3. Our Method: Stylus\\nAdapter selection presents distinct challenges compared to existing methods for retrieving text-based documents, as outlined in Section 2.\\nFirst, computing embeddings for adapters is a novel task, made more difficult without access to training datasets.\\nFurthermore, in the context of image generation, user prompts often specify multiple highly fine-grained tasks.\\nThis challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt.\\nFinally, composing multiple adapters can degrade image quality and inject foreign biases into the model.\\nOur three-stage framework below—Refine, Retrieve, and Compose—addresses the above challenges (Fig. 2).\\n3.1. Refiner\\nThe refiner is a two-stage pipeline designed to generate textual descriptions of an adapter’s task and the corresponding text embeddings for retrieval purposes.\\nThis approach mirrors retrieval-based methods [16], which pre-compute embeddings over an external database of texts.\\nGiven an adapter Ai, the first stage is a vision-language model (VLM) that takes in the adapter’s model card—a set of randomly sampled example images from the model card Ii ∈ {Ii1, Ii2,}, the corresponding prompts Pi ∈ {pi1, pi2,}, and an author-provided description,2 Di—and returns an improved description D∗ i. Optionally, the VLM also recommends the weight for LoRA-based adapters, as the adapter weight is usually specified either in the author’s description Di or the set of prompts Pi, a feature present in popular image generation software [1].\\nIf information cannot be found, the LoRA’s weight is set to 0.8.\\nIn our experiments, these improved descriptions were generated by Gemini Ultra [37] (see § A.1 for prompt).\\nThe second stage uses an embedding model (E) to generate embeddings e = E(D∗) for all adapters.\\nIn our experiments, we create embeddings from OpenAI’s text-embedding-3-large model [18, 26].\\nWe store pre-computed embeddings in a vector database.\\n3.2. Retriever\\nThe retriever fetches the most relevant adapters over the entirety of the user’s prompt using similarity metrics.\\nMathematically, the retriever employs the same embedding model (E) to process the user prompt, s, generating embedding es = E(s).\\nIt then calculates cosine similarity scores between the prompt’s embedding es and the embedding of each adapter in the matrix M. The top K adapters AK (K = 150, in our experiments) are selected based on the highest similarity scores: AK = arg top-Ki\\n(es·Mi ∥es∥∥Mi∥) , where Mi is the ith row of the embedding matrix, representing the ith adapter’s embedding.\\n3.3. Composer\\nThe composer serves a dual purpose: segmenting the prompt into tasks from a prompt’s keywords and assigning retrieved adapters to tasks.\\nThis implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the 2We note that a large set of author descriptions are inaccurate, misleading, or absent.\\nThe refiner helped correct for human errors by using generated images as the ground truth, significantly improving our system.\\nFigure 4.\\nQualitative comparison between Stylus over realistic (left) and cartoon (right) style Stable Diffusion checkpoints.\\nStylus produces highly detailed images that correctly depicts keywords in the context of the prompt.\\nFor the prompt “A graffiti of a corgi on the wall”, our method correctly depicts a spray-painted corgi, whereas the checkpoint generates a realistic dog.\\n |  | RealisticVision-v6 / COCO | RealisticVision-v6 / PartiPrompts | Counterfeit-v3 / COCO | Counterfeit-v3 / PartiPrompts\\n | --- | --- | --- | --- | ---\\n | 80\\n | Stylus SD 1.5 | 68.31% | 67.81% | 62.00% | 58.57%\\n\\n60\\n41.43% 38.00%\\n40\\n32.19% 31.69%\\n20\\n0\\nFigure 5.\\nHuman Evaluation.\\nStylus achieves a higher preference scores (2:1) over different datasets and Stable Diffusion checkpoints.\\nprompt through keyword grounding.\\nFor example, if the prompt is “pandas eating bamboo”, the composer may discard an irrelevant “grizzly bears” adapter and a biased “panda mascots” adapter.\\nMathematically, the composer (C) takes in the prompt (s) and the top K adapters (AK) from the retriever, classifying them over different tasks,\\nT (s) = {t1, t2,, tn}.\\nThis can be expressed as: C(s,AK) = {(ti,Aki) | ti ∈ T (s),Aki ⊆ AK, ∀Aj ∈ Aki, Sim(Aj, ti) ≥ τ} (1), where Aki is the subset of adapters per task ti, Sim(Aj, ti) measures the similarity score between an adapter and a task, and τ is an arbitary threshold.\\nWhile the composer can be trained with human-labeled data [28], we opt for a simpler approach that requires no training—prompting a long-context Large Language Model (LLM).\\nThe LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters.\\nIn our implementation, we choose Gemini 1.5, with a 128K context window, as the composer’s LLM (see App. A.2 for the full prompt).\\nMost importantly, Stylus’s composer parallels reranking, an advanced RAG technique.\\nRerankers employ cross encoders (F) that compare the retriever’s individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores: F(s, D∗).\\nThis prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment.\\nOur experimental ablations (§ 4.3) show that our composer outperforms existing rerankers (Cohere,\\nrerank-english-v2.0) [32].\\n3.4. Masking\\n | The composer identifies tasks assigns each task a set of relevant adapters as: C(Ak, s) = {(t1,Ak1), (t2,Ak2), | ti | from the prompt s Aki, formalized }. Our masking | and\\n | --- | --- | --- | ---\\n | strategy applies a binary mask, mask, Mi, can either be an one hot encoding, all ones, or all zeroes vector. Across all tasks, we perform a cross- product across masks, M1 ×M2 ×M3 × | Mi, | for each task ti. Each , generating | an\\n\\nexponential number of masking schemes.\\nThe combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images (§ 4.2.3).\\nThis approach also curtails the number of final adapters merged into the base model, minimizing the risk of composed adapters introducing undesirable effects to the image [47].\\nFinally, an adapter’s weight (i.e. LoRA), which is extracted from the refiner (§ 3.1), is divided by the total number of adapters (after masking) in its corresponding task.\\nThis solves the problem of image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality (see App. A.4).',\n",
       " '3.1. Refiner\\nThe refiner is a two-stage pipeline designed to generate textual descriptions of an adapter’s task and the corresponding text embeddings for retrieval purposes.\\nThis approach mirrors retrieval-based methods [16], which pre-compute embeddings over an external database of texts.\\nGiven an adapter Ai, the first stage is a vision-language model (VLM) that takes in the adapter’s model card—a set of randomly sampled example images from the model card Ii ∈ {Ii1, Ii2,}, the corresponding prompts Pi ∈ {pi1, pi2,}, and an author-provided description,2 Di—and returns an improved description D∗ i. Optionally, the VLM also recommends the weight for LoRA-based adapters, as the adapter weight is usually specified either in the author’s description Di or the set of prompts Pi, a feature present in popular image generation software [1].\\nIf information cannot be found, the LoRA’s weight is set to 0.8.\\nIn our experiments, these improved descriptions were generated by Gemini Ultra [37] (see § A.1 for prompt).\\nThe second stage uses an embedding model (E) to generate embeddings e = E(D∗) for all adapters.\\nIn our experiments, we create embeddings from OpenAI’s text-embedding-3-large model [18, 26].\\nWe store pre-computed embeddings in a vector database.',\n",
       " '3.2. Retriever\\nThe retriever fetches the most relevant adapters over the entirety of the user’s prompt using similarity metrics.\\nMathematically, the retriever employs the same embedding model (E) to process the user prompt, s, generating embedding es = E(s).\\nIt then calculates cosine similarity scores between the prompt’s embedding es and the embedding of each adapter in the matrix M. The top K adapters AK (K = 150, in our experiments) are selected based on the highest similarity scores: AK = arg top-Ki\\n(es·Mi ∥es∥∥Mi∥) , where Mi is the ith row of the embedding matrix, representing the ith adapter’s embedding.',\n",
       " '3.3. Composer\\nThe composer serves a dual purpose: segmenting the prompt into tasks from a prompt’s keywords and assigning retrieved adapters to tasks.\\nThis implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the 2We note that a large set of author descriptions are inaccurate, misleading, or absent.\\nThe refiner helped correct for human errors by using generated images as the ground truth, significantly improving our system.\\nFigure 4.\\nQualitative comparison between Stylus over realistic (left) and cartoon (right) style Stable Diffusion checkpoints.\\nStylus produces highly detailed images that correctly depicts keywords in the context of the prompt.\\nFor the prompt “A graffiti of a corgi on the wall”, our method correctly depicts a spray-painted corgi, whereas the checkpoint generates a realistic dog.\\n |  | RealisticVision-v6 / COCO | RealisticVision-v6 / PartiPrompts | Counterfeit-v3 / COCO | Counterfeit-v3 / PartiPrompts\\n | --- | --- | --- | --- | ---\\n | 80\\n | Stylus SD 1.5 | 68.31% | 67.81% | 62.00% | 58.57%\\n\\n60\\n41.43% 38.00%\\n40\\n32.19% 31.69%\\n20\\n0\\nFigure 5.\\nHuman Evaluation.\\nStylus achieves a higher preference scores (2:1) over different datasets and Stable Diffusion checkpoints.\\nprompt through keyword grounding.\\nFor example, if the prompt is “pandas eating bamboo”, the composer may discard an irrelevant “grizzly bears” adapter and a biased “panda mascots” adapter.\\nMathematically, the composer (C) takes in the prompt (s) and the top K adapters (AK) from the retriever, classifying them over different tasks,\\nT (s) = {t1, t2,, tn}.\\nThis can be expressed as: C(s,AK) = {(ti,Aki) | ti ∈ T (s),Aki ⊆ AK, ∀Aj ∈ Aki, Sim(Aj, ti) ≥ τ} (1), where Aki is the subset of adapters per task ti, Sim(Aj, ti) measures the similarity score between an adapter and a task, and τ is an arbitary threshold.\\nWhile the composer can be trained with human-labeled data [28], we opt for a simpler approach that requires no training—prompting a long-context Large Language Model (LLM).\\nThe LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters.\\nIn our implementation, we choose Gemini 1.5, with a 128K context window, as the composer’s LLM (see App. A.2 for the full prompt).\\nMost importantly, Stylus’s composer parallels reranking, an advanced RAG technique.\\nRerankers employ cross encoders (F) that compare the retriever’s individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores: F(s, D∗).\\nThis prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment.\\nOur experimental ablations (§ 4.3) show that our composer outperforms existing rerankers (Cohere,\\nrerank-english-v2.0) [32].',\n",
       " '3.4. Masking\\n | The composer identifies tasks assigns each task a set of relevant adapters as: C(Ak, s) = {(t1,Ak1), (t2,Ak2), | ti | from the prompt s Aki, formalized }. Our masking | and\\n | --- | --- | --- | ---\\n | strategy applies a binary mask, mask, Mi, can either be an one hot encoding, all ones, or all zeroes vector. Across all tasks, we perform a cross- product across masks, M1 ×M2 ×M3 × | Mi, | for each task ti. Each , generating | an\\n\\nexponential number of masking schemes.\\nThe combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images (§ 4.2.3).\\nThis approach also curtails the number of final adapters merged into the base model, minimizing the risk of composed adapters introducing undesirable effects to the image [47].\\nFinally, an adapter’s weight (i.e. LoRA), which is extracted from the refiner (§ 3.1), is divided by the total number of adapters (after masking) in its corresponding task.\\nThis solves the problem of image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality (see App. A.4).',\n",
       " '4. Results\\n4.1. Experimental Setup\\nAdapter Testbed.\\nAdapter selection requires a large database of adapters to properly evaluate its performance.\\n24\\n23\\n22\\n21\\n20\\nSD 1.5\\n19\\nStylus\\n |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n\\nTable 1.\\nEvaluation over different retrieval methods (CFG=6).\\nStylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.\\nHowever, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:\\nRealistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.\\n4.2. Main Experiments\\n4.2.1 Human Evaluation.\\nTo demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.\\nTo conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.\\n4.2.2 Automatic Benchmarks.\\nWe assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.\\n4.2.3 VLM as a Judge\\nWe use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.\\nFigure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.\\n Lose Tie Win\\n100\\n20 29\\n80\\n59\\n60\\n58 52\\n40\\n20\\n41 19 22\\nPrompt Length\\n0\\n | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n\\n(a) VLM as a Judge with GPT-4V\\nFigure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.\\nLose Tie Win\\n100\\n35\\n80\\n60\\n40\\n40\\n20\\n |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n\\nuate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.\\n4.2.4 Diversity per Prompt\\nGiven identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:\\ncalculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.\\nGPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,\\n4FID fails to disentangle image fidelity from diversity [27, 35].\\nStylus\\nReranker\\nRetriever\\nRandom\\nA baby elephant underneath an adult elephant.\\n16s\\n25s\\nA building behind a stop sign and street sign.\\n44s\\nTime (s)\\nA group of friends jumping on a bed.\\nRetriever Composer Load Adapter Image Generation\\n | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n\\nFigure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.\\n | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n\\nFigure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.\\nAs shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.\\n4.3. Ablations\\n4.3.1 Alternative Retrieval-based Methods\\nWe benchmark Stylus’s performance relative to different retrieval methods.\\nFor all baselines below, we select the top three adapters and merge them into the base model.\\nRandom: Adapters are randomly sampled without replacement from StylusDocs.\\nRetriever: The retriever emulates standard RAG pipelines [16, 46], functionally equivalent to Stylus without the composer stage.\\nTop adapters are fetched via cosine similarity over adapter embeddings.\\nConversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion.\\nThe random baseline chooses adapters that are orthogonal to the user prompt.\\nThus, these adapters alter unrelated concepts, which does not affect image generation.\\nIn fact, we observed that the distribution of random policy’s images in Fig. 10 were nearly identical to Stable Diffusion.\\n |  | Stylus | Stylus\\n | --- | --- | ---\\n | Source | SD v1.5 | Source | SD v1.5\\n | --- | --- | --- | ---\\n\\nVolcanoes spewing lava\\nStudio Ghibli Style\\nFiery red Voxel Style Pencil Sketch Van Gogh Style\\n(a) Image Translation\\nStylus chooses relevant adapters that better adapt new styles and elements into existing images.\\nStylus Stylus\\nSD v1.5 SD v1.5\\nSource Source\\nDwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny\\n(b) Inpainting\\nStylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.\\nFigure 12.\\nStylus over different image-to-image tasks.\\n4.3.2 Breakdown of Stylus’s Inference Time\\nThis section breaks down the latency introduced by various components of Stylus.\\nWe note that image generation time is independent of Stylus, as adapter weights are merged into the base model [12].\\nFigure 11 demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts.\\nSpecifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds.\\nThe composer’s large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens.\\nFinally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process.\\nHowever, Stylus’s latency remains consistent across all batch sizes, as the composer and retriever run only once.\\nHence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases.\\n4.3.3 Image-Domain Tasks\\nBeyond text-to-image, Stylus applies across various imageto-image tasks.\\nFig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting, described as follows:\\nImage translation: Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt’s definition.\\nStylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style.\\nWe present examples in Fig 12a.\\nFor a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits.\\nFor a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.\\nInpainting: Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask.\\nStylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity.\\nWe provide further examples in Fig. 12b, demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right).',\n",
       " '4.1. Experimental Setup\\nAdapter Testbed.\\nAdapter selection requires a large database of adapters to properly evaluate its performance.\\n24\\n23\\n22\\n21\\n20\\nSD 1.5\\n19\\nStylus\\n |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n\\nTable 1.\\nEvaluation over different retrieval methods (CFG=6).\\nStylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.\\nHowever, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:\\nRealistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.',\n",
       " 'SD 1.5\\n19',\n",
       " 'Stylus\\n |  |  | 26.0 26.2 26.4 26.6 CLIP Score (ViT-L/14) |  | 26.8 27.0 | 27.2 | \\n | --- | --- | --- | --- | --- | --- | ---\\n | Figure 6. Clip/FID Pareto Curve for COCO. We observe Stylus can improve visual fidelity (FID) and/or textual alignment (CLIP) over a range of guidance values (CFG): [1, 1.5, 2, 3, 4, 6, 9, 12].\\n |  |  | CLIP (∆) |  | FID (∆) |  | \\n |  | Stylus | 27.25 (+0.03) |  | 22.05 (-1.91) |  | \\n |  | Reranker | 25.48 (-1.74) |  | 22.81 (-1.15) |  | \\n |  | Retriever-only | 24.93 (-2.29) |  | 24.68 (+0.72) |  | \\n |  | Random | 26.34 (-0.88) |  | 24.39 (+0.43) |  | \\n |  | SD v1.5 | 27.22 |  | 23.96 |  | \\n\\nTable 1.\\nEvaluation over different retrieval methods (CFG=6).\\nStylus outperforms existing retrieval-based methods, attains the best FID score, and similar CLIP score to Stable Diffusion.\\nHowever, existing methods [13, 46] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters.\\nTo bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24, 43].\\nThis dataset contains precomputed OpenAI embeddings [18] and improved adapter descriptions from Gemini Ultra-Vision [37], the output of Stylus’s refiner component (§ 3.1).\\nWe further characterize the distribution of adapters in App.\\nA.3.\\nGeneration Details.\\nWe assess Stylus against StableDiffusion-v1.5 [34] as the baseline model.\\nAcross experiments, we employ two well-known checkpoints:\\nRealistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images.\\nOur image generation process integrates directly with Stable-Diffusion WebUI [1] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22].\\nTo replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15] and denoising strength set to 0.7.\\nFor images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint’s original style.\\nTo counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint’s style3.',\n",
       " '4.2. Main Experiments\\n4.2.1 Human Evaluation.\\nTo demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.\\nTo conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.\\n4.2.2 Automatic Benchmarks.\\nWe assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.\\n4.2.3 VLM as a Judge\\nWe use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.\\nFigure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.\\n Lose Tie Win\\n100\\n20 29\\n80\\n59\\n60\\n58 52\\n40\\n20\\n41 19 22\\nPrompt Length\\n0\\n | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n\\n(a) VLM as a Judge with GPT-4V\\nFigure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.\\nLose Tie Win\\n100\\n35\\n80\\n60\\n40\\n40\\n20\\n |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n\\nuate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.\\n4.2.4 Diversity per Prompt\\nGiven identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:\\ncalculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.\\nGPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,\\n4FID fails to disentangle image fidelity from diversity [27, 35].',\n",
       " '4.2.1 Human Evaluation.\\nTo demonstrate our method’s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO [19] and PartiPrompts [45], and two checkpoints, which generate realistic and anime-style images respectively.\\nExamples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.\\nTo conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination.\\nThese raters were asked to indicate their preference for Stylus or StableDiffusion-v1.5.\\nIn Fig. 5, users generally showed a preference for Stylus over existing model checkpoints.\\nAlthough preference rates were consistent across datasets, they varied significantly between different checkpoints.\\nAdapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores.',\n",
       " '4.2.2 Automatic Benchmarks.\\nWe assess Stylus using two automatic benchmarks: CLIP [10], which measures the correlation between a generated images’ caption and users’ prompts, and FID [11], which evaluates the diversity and aesthetic quality of image sets.\\nWe evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint.\\nFig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment.\\nThis improvement aligns with our human evaluations, which suggest a correlation between human preferences and the FID scores.',\n",
       " '4.2.3 VLM as a Judge\\nWe use VLM as a Judge to assess two key metrics: textual alignment and visual fidelity, simulating subjective assessments [5].\\nFor visual fidelity, the VLM scores based on disfigured limbs and unrealistic composition of objects.\\nWhen asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented.\\nTo combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie.\\nIn Fig. 8a, we assess eval- 3The debias prompts are “realistic, high quality” for Realistic-Vision-v6 and “anime style, high quality” for Counterfeit-v3, respectively.\\nFigure 7.\\nImage Diversity.\\nGiven the same prompt, our method (left) generates more diverse and comprehensive sets of images than that of existing Stable Diffusion checkpoints (right).\\nStylus’s diversity comes from its masking scheme and the composer LLM’s temperature.\\n Lose Tie Win\\n100\\n20 29\\n80\\n59\\n60\\n58 52\\n40\\n20\\n41 19 22\\nPrompt Length\\n0\\n | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n\\n(a) VLM as a Judge with GPT-4V\\nFigure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.\\nLose Tie Win\\n100\\n35\\n80\\n60\\n40\\n40\\n20\\n |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n\\nuate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.',\n",
       " ' Lose Tie Win\\n100\\n20 29\\n80\\n59\\n60\\n58 52\\n40\\n20\\n41 19 22',\n",
       " 'Prompt Length\\n0\\n | Visual Quality | Textual Alignment |  | \\n | --- | --- | --- | ---\\n | Figure 9. Image diversity (dFID) across prompt length. Stylus achieves higher diversity score than Stable Diffusion when prompt length increases.\\n\\n(a) VLM as a Judge with GPT-4V\\nFigure 8.\\nPreference Win Rate over GPT-4V as a judge.\\nStylus achieves higher preference scores over GPT-4V for visual quality and image diversity.',\n",
       " 'Lose Tie Win\\n100\\n35\\n80\\n60\\n40\\n40\\n20\\n |  | 25\\n | --- | ---\\n |  | GPT-4V dFID0 |  |  |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | (b) Image Diversity (GPT-4V, dFID)\\n |  | 1.2 |  | SD v1.5 |  | Stylus | \\n | 1.0\\n | 0.8\\n | 0.6\\n | 0.4\\n | 0.2\\n |  | 0 |  | 5 10 | 15 | 20 | 25\\n\\nuate 100 randomly sampled prompts from the PartiPrompts dataset [45].\\nBarring ties, we find visual fidelity achieves 60% win rate between Stylus and the Stable Diffusion realistic checkpoint, which is conclusively consistent with the 68% win rate from our human evaluation.\\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint.\\nAs most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\\nWe provide the full prompt in Appendix A.5.',\n",
       " '4.2.4 Diversity per Prompt\\nGiven identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes.\\nQualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles.\\nTo quantitatively assess this diversity, we use two metrics:\\ncalculated as the variance of latent embeddings from InceptionV3 [36].\\nMathematically, dFID involves fitting a Normal distribution N (µ,Σ) to the latent features of InceptionV3, with the metric given by the trace of the covariance matrix, dFID = Tr Σ.\\nGPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts.\\nFive images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V’s positional bias [47].\\nSimilar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity [6].\\nFull prompt and additional details are provided in App A.5.\\nFig. 8b displays preference rates and defines a win when Stylus achieves higher dFID or receives a higher score from GPT-4V for a given prompt.\\nAcross 200 prompts, Stylus prevails in approximately 60% and 58% cases for dFID and GPT-4V respectively, excluding ties.\\nFigure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images.\\nAdditional results measuring diversity per keyword are presented in Appendix A.6.\\ndFID: Previous evaluations with FID [11] show that Stylus improves image quality and diversity across prompts4.\\nWe define dFID specifically to evaluate diversity per prompt,\\n4FID fails to disentangle image fidelity from diversity [27, 35].',\n",
       " 'Stylus',\n",
       " 'Reranker',\n",
       " 'Retriever',\n",
       " 'Random\\nA baby elephant underneath an adult elephant.\\n16s\\n25s\\nA building behind a stop sign and street sign.\\n44s\\nTime (s)\\nA group of friends jumping on a bed.\\nRetriever Composer Load Adapter Image Generation\\n | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n\\nFigure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.\\n | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n\\nFigure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.\\nAs shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.',\n",
       " 'Time (s)\\nA group of friends jumping on a bed.\\nRetriever Composer Load Adapter Image Generation\\n | Two kids holding a teddy bear while standing. | Red and yellow train pulling up to a cement platform. | \\n | --- | --- | ---\\n | Reranker: An alternative to Stylus’s composer, the reranker fetches the retriever’s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters’ descriptions and the prompt. We evaluate with Cohere’s reranker endpoint [32].\\n | A woman standing in front of a TV with a remote in her hand. | Suitcases sitting next a chair. | Laptop keyboard has two small furry objects on it.\\n\\nFigure 10.\\nDifferent Retrieval Methods.\\nStylus outperforms all other retrieval methods, which choose adapters than either introduce foreign concepts to the image or override other concepts in the prompt, reducing textual alignment.\\n | Stylus | 1.1s | 8.5s | 2.5s |  |  | \\n | --- | --- | --- | --- | --- | --- | ---\\n | SDv1.5 (BS=1)\\n | SDv1.5 (BS=2)\\n | SDv1.5 (BS=4)\\n |  | 0 |  | 10 | 20 | 30 | 40\\n\\nFigure 11.\\nComparison of Stylus’s inference overheads with Stable Diffusion’s inference time by batch size (BS).\\nAt BS=1, Stylus accounts for 75% of the image generation time, primarily due to the composer processing long context prompts from adapter descriptions.\\nHowever, Stylus’s overhead decreases when batch size increases.\\nAs shown in Tab.\\n1, Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which fall behind the base Stable Diffusion model.\\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion.\\nEach method selects adapters that are similar to the prompt but potentially introduce unrelated biases.\\nIn Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants.\\nFurthermore, both methods incorrectly assign weights to adapters, causing adapters’ tasks to overshadow other tasks within the same prompt.\\nIn Fig. 10, both the reranker and retriever generate images solely focused on singular items—beds, chairs, suitcases, or trains—while ignoring other elements specified in the prompt.\\nWe provide an analysis of failure modes in A.4.',\n",
       " '4.3. Ablations',\n",
       " '4.3.1 Alternative Retrieval-based Methods\\nWe benchmark Stylus’s performance relative to different retrieval methods.\\nFor all baselines below, we select the top three adapters and merge them into the base model.\\nRandom: Adapters are randomly sampled without replacement from StylusDocs.\\nRetriever: The retriever emulates standard RAG pipelines [16, 46], functionally equivalent to Stylus without the composer stage.\\nTop adapters are fetched via cosine similarity over adapter embeddings.\\nConversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion.\\nThe random baseline chooses adapters that are orthogonal to the user prompt.\\nThus, these adapters alter unrelated concepts, which does not affect image generation.\\nIn fact, we observed that the distribution of random policy’s images in Fig. 10 were nearly identical to Stable Diffusion.\\n |  | Stylus | Stylus\\n | --- | --- | ---\\n | Source | SD v1.5 | Source | SD v1.5\\n | --- | --- | --- | ---\\n\\nVolcanoes spewing lava\\nStudio Ghibli Style\\nFiery red Voxel Style Pencil Sketch Van Gogh Style\\n(a) Image Translation\\nStylus chooses relevant adapters that better adapt new styles and elements into existing images.\\nStylus Stylus\\nSD v1.5 SD v1.5\\nSource Source\\nDwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny\\n(b) Inpainting\\nStylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.\\nFigure 12.\\nStylus over different image-to-image tasks.',\n",
       " 'Volcanoes spewing lava',\n",
       " 'Studio Ghibli Style',\n",
       " 'Fiery red Voxel Style Pencil Sketch Van Gogh Style',\n",
       " '(a) Image Translation\\nStylus chooses relevant adapters that better adapt new styles and elements into existing images.\\nStylus Stylus\\nSD v1.5 SD v1.5\\nSource Source\\nDwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny',\n",
       " 'Stylus Stylus',\n",
       " 'SD v1.5 SD v1.5',\n",
       " 'Source Source\\nDwayne Johnson Homer Simpson Morgan Freeman Glass Bunny Burger Robot Bunny',\n",
       " '(b) Inpainting\\nStylus chooses adapters than can better introduce new characters or concepts into the inpainted mask.\\nFigure 12.\\nStylus over different image-to-image tasks.',\n",
       " '4.3.2 Breakdown of Stylus’s Inference Time\\nThis section breaks down the latency introduced by various components of Stylus.\\nWe note that image generation time is independent of Stylus, as adapter weights are merged into the base model [12].\\nFigure 11 demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts.\\nSpecifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds.\\nThe composer’s large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens.\\nFinally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process.\\nHowever, Stylus’s latency remains consistent across all batch sizes, as the composer and retriever run only once.\\nHence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases.',\n",
       " '4.3.3 Image-Domain Tasks\\nBeyond text-to-image, Stylus applies across various imageto-image tasks.\\nFig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting, described as follows:\\nImage translation: Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt’s definition.\\nStylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style.\\nWe present examples in Fig 12a.\\nFor a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits.\\nFor a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.\\nInpainting: Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask.\\nStylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity.\\nWe provide further examples in Fig. 12b, demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right).',\n",
       " '5. Conclusion\\nWe propose Stylus, a new algorithm that automatically selects and composes adapters to generate better images.\\nOur method leverages a three-stage framework that precomputes adapters as lookup embeddings and retrieves most relevant adapters based on prompts’ keywords.\\nTo evaluate Stylus, we develop StylusDocs, a processed dataset featuring 75K adapters and pre-computed adapter embeddings.\\nOur evaluation of Stylus, across automatic metrics, humans, and vision-language models, demonstrate that Stylus achieves better visual fidelity, textual alignment, and image diversity than existing Stable Diffusion checkpoints.\\nAcknowledgement\\nWe thank Lisa Dunlap, Ansh Chaurasia, Siyuan Zhuang, Sijun Tan, Tianjun Zhang, and Shishir Patil for their insightful discussion.\\nWe thank Google Deepmind for funding this project, providing AI infrastructure, and provisioning Gemini endpoints.\\nSky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, SAP, Uber, and VMware.',\n",
       " 'Acknowledgement\\nWe thank Lisa Dunlap, Ansh Chaurasia, Siyuan Zhuang, Sijun Tan, Tianjun Zhang, and Shishir Patil for their insightful discussion.\\nWe thank Google Deepmind for funding this project, providing AI infrastructure, and provisioning Gemini endpoints.\\nSky Computing Lab is supported by gifts from Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, SAP, Uber, and VMware.',\n",
       " 'References\\n[1] AUTOMATIC1111.\\nStable Diffusion Web UI, Aug. 2022.\\n3, 5 [2] Harrison Chase.\\nLangChain, Oct. 2022.\\n3 [3] Alexandra Chronopoulou, Matthew E. Peters, Alexander Fraser, and Jesse Dodge.\\nAdaptersoup: Weight averaging to improve generalization of pretrained language models, 2023.\\n3 [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\nBert: Pre-training of deep bidirectional transformers for language understanding, 2019.\\n3 [5] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto.\\nAlpacafarm: A simulation framework for methods that learn from human feedback.\\nIn\\nA. Oh, T\\nNeumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 30039–30069.\\nCurran Associates, Inc., 2023.\\n5 [6] Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, and Serena Yeung-Levy.\\nDescribing differences in image sets with natural language.\\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n6, 14 [7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or.\\nAn image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\\n1, 3 [8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang.\\nRetrieval-augmented generation for large language models: A survey, 2024.\\n2, 3 [9] David Ha, Andrew Dai, and Quoc V. Le.\\nHypernetworks, 2016.\\n1, 3 [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\\nClipscore: A reference-free evaluation metric for image captioning, 2022.\\n5 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\\nGans trained by a two time-scale update rule converge to a local nash equilibrium, 2018.\\n5, 6 [12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLora: Low-rank adaptation of large language models, 2021.\\n1, 3, 8 [13] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin.\\nLorahub: Efficient cross-task generalization via dynamic lora composition, 2024.\\n3, 5 [14] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.\\nPersonalized soups: Personalized large language model alignment via post-hoc parameter merging, 2023.\\n3 [15] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\\nElucidating the design space of diffusion-based generative models, 2022.\\n5 [16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\\nRetrieval-augmented generation for knowledge-intensive nlp tasks, 2021.\\n2, 3, 7 [17] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi.\\nPlayground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024.\\n3 [18] Jimmy Lin, Ronak Pradeep, Tommaso Teofili, and Jasper Xian.\\nVector search with openai embeddings: Lucene is all you need, 2023.\\n3, 5 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.\\nMicrosoft coco: Common objects in context, 2015.\\n1, 5, 11 [20] Jerry Liu.\\nLlamaIndex, 11 2022.\\n3 [21] Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Antonio Torralba.\\nUnsupervised compositional concepts discovery with text-to-image generative models, 2023.\\n3 [22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\\nDPM-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023.\\n5 [23] Tengyu Ma.\\nVectorize your data to gear up your ai stack., 2023.\\n3 [24] Justin Maier.\\nThe home of open-source generative ai, 2022.\\n1, 2, 3, 5, 11 [25] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.\\nPeft: State-of-the-art parameter-efficient fine-tuning methods.\\nhttps://github.com/huggingface/peft, 2022.\\n3 [26] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz.\\nSfr-embeddingmistral:enhance text retrieval with transfer learning.\\nSalesforce AI Research Blog, 2024.\\n3 [27] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo.\\nReliable fidelity and diversity metrics for generative models.\\nIn Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 7176–7185.\\nPMLR, 2020.\\n6 [28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\\nTraining language models to follow instructions with human feedback, 2022.\\n4\\nMalte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch, Massimiliano Pippi, Mayank Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir Blagojevic, Thomas Stadelmann, Tanay Soni, and Sebastian Lee.\\nHaystack: the end-to-end NLP framework for pragmatic builders, Nov. 2019.\\n3 [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.\\nSdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\\n3 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\\nLearning transferable visual models from natural language supervision, 2021.\\n3 [32] Nils Reimers.\\nSay goodbye to irrelevant search results: Cohere rerank is here, 2023.\\n3, 4, 7 [33] Nils Reimers and Iryna Gurevych.\\nSentence-bert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\\nAssociation for Computational Linguistics, 11 2019.\\n3 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\\nHigh-resolution image synthesis with latent diffusion models, 2022.\\n3, 5 [35] Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly.\\nAssessing generative models via precision and recall.\\nIn Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 5234–5243, 2018.\\n6 [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision.\\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2818–2826.\\nIEEE Computer Society, 2016.\\n6 [37] Gemini Team.\\nGemini: A family of highly capable multimodal models, 2023.\\n3, 5, 11 [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\\nLlama: Open and efficient foundation language models, 2023.\\n3 [39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\nLlama 2: Open foundation and fine-tuned chat models, 2023.\\n3 [40] Hanqing Wang, Bowen Ping, Shuo Wang, Xu Han, Yun Chen, Zhiyuan Liu, and Maosong Sun.\\nLora-flow: Dynamic lora fusion for large language models in generative tasks, 2024.\\n3 [41] Xinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Graham Neubig.\\nEfficient test time adapter ensembling for low-resource language varieties.\\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 730–737, Punta Cana, Dominican Republic, Nov. 2021.\\nAssociation for Computational Linguistics.\\n3 [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\n11, 14 [43] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\\nHuggingface’s transformers: State-of-the-art natural language processing, 2020.\\n1, 3, 5, 11 [44] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models, 2023.\\n11, 14 [45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu.\\nScaling autoregressive models for content-rich text-to-image generation, 2022.\\n5, 6 [46] Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, and Fei Wu.\\nLoraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild, 2024.\\n3, 5, 7 [47] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen.\\nMulti-lora composition for image generation, 2024.\\n3, 4, 6\\nA. Appendix\\n0.5\\nA.1. Details of the Refiner VLM\\n0.4\\nWe provide a complete example input to the refiner’s VLM in Tab.\\n2.\\nThe prompt utilizes Chain-of-Thought (CoT) prompting, which decomposes the VLM’s goal of producing better adapter descriptions into two steps [42, 44].\\nInitially, the VLM categorizes the adapter’s task into one of several topics—such as concepts, styles, characters, or poses.\\nSubsequently, the VLM is prompted to elaborate on why the adapter is associated with a particular topic and how it modifies images within that context.\\nWe found that this two step logical process significantly improved the structure and quality of model responses.\\n0.3\\n0.2\\n0.1\\n0.0\\nCategory\\n(a) Distribution of adapters across categories.\\n0.004\\nA.2. Details of the Composer LLM\\n0.003\\nWe provide a full example prompt of the composer’s LLM component in Tab.\\n3, which is plugged through the Gemini\\n0.002\\n1.5 endpoint [37].\\nOur experiments feed in descriptions of the top 150 adapters into the LLM’s context.\\nUsing a Chain-of-Thought (CoT) approach, the prompt is structured to first identify keywords or tasks, then allocate appropriate adapters to these tasks.\\nIf necessary, it merges keywords for adapters that span multiple tasks [42, 44].\\n0.001\\n0.000\\nTop 500 Adapters sorted by Download Count\\nA.3. Stylus-Bench Characterization\\nThis section describes StylusDocs, which comprises of 76K Low Rank Adapters (LoRAs) from public repositories, including Civit AI and Hugging Face [24, 43].\\nWe excluded NSFW-labeled adapters from the Civit AI dataset, which originally contained over 100K LoRAs.\\nFigure 13 illustrates the distribution of adapters across various semantic categories and their popularity, measured by download counts.\\nA significant majority, 70%, of adapters belong to the character and celebrity category, primarily consisting of anime or game characters.\\nAnother 13% of adapters modify image style, 8% adjust clothing, and 4% represent various concepts (Fig. 13a).\\nThese statistics indicate that our experiments consider a minor proportion of adapters, as the COCO dataset does not feature characters or celebrities [19].\\nDespite this, Stylus outperforms base Stable Diffusion.\\nFurthermore, the popularity of adapters follows a Pareto distribution, where the top adapters receive exponentially more downloads than the others (Fig. 13a).\\nHowever, the top adapter accounts for only 0.5% of total downloads, which suggests that the distribution is long-tailed.\\nA.4. Failure Modes\\nWe detail different failure modes that were discovered while developing Stylus.\\nImage saturation.\\nThe quality of image generation is highly depend on adapters’ weights.\\nIf the assigned weight is above the recommended value, the adapter negatively impacts image generation, leading to a growing number of visual inconsistencies and artifacts.\\nIn Fig. 14a, assigning a high weight to a “James Bond” LoRA increases images exposure and introducing significant visual tearing.\\nStylus mitigates over-saturation with its refiner component, which extract the right adapter weights from the adapter’s model card.\\nLastly, Stylus uniformly weights adapters based on their associated tasks, ensuring that similar adapters do not significantly impact their corresponding tasks.\\n0.005\\n(b) Top 500 adapters ranked by percentage of downloads.\\nFigure 13.\\nWorkload Characterization of StylusDocs.\\n(a) Most adapters are categorized as characters or celebrities.\\n(b) Adapter popularity exhibits a power-law distribution, with the top adapters receiving exponentially more downloads than the others.\\nTask Blocking.\\nComposing adapters presents the risk of overwriting existing concepts or tasks specified in the prompt and other selected adapters.\\nWe illustrate several examples in Figure 2—a train LoRA overrides the toy train concept (left), a park bench LoRA masks a person in an orange blanket (middle), and a fancy cake LoRA erases the image of a man eating the cake (right).\\nTask blocking typically arises from two main issues: the adapter weight set too high or too many adapters merged into the base model.\\nStylus addresses this by reducing an adapter’s weight with uniform weighting per task, while the masking scheme reduces the number of selected adapters.\\nAlthough Stylus does not completely solve task blocking, it offers simple heuristics to mitigate the issue.\\nImage Prompts\\nPrompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.\\nPrompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>\\nModel Card Description\\n• Title: Dwayne”The Rock” Johnson (LoRA)\\n• Tags: Celebrity, Photorealistic, Hollywood, Celeb\\n• Trigger Words: Th3R0ck\\n• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.\\nYour goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:\\n1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.\\n• Some prompts may specify the adapter weight (i.e. <lora:NAME:WEIGHT>).\\nIf provided, you will need to infer the adapter’s name and weight.\\nPrioritize this weight over the author’s recommended weight.\\n2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.\\n• The model card description may be incorrect, misleading, or incomplete.\\n• The model card may specify the weight of the model adapter, or the recommended range.\\nFind the recommended weight of the adapter (default is 0.8).\\n% Chain-of-Thought Prompting\\nAgain, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.\\nFirst, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:\\n• Do not describe any training or dataset-related details.\\n• Provide additional context from your prior knowledge if there is insufficient information.\\n• Do not hallucinate and repeat text.\\nOutput only english words and sentences.\\nSecond, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.\\nPlease format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.\\nTask Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.\\nLow quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.\\nRetrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth\\nProvided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.\\n% Chain-of-Thought Prompting\\nFirst, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.\\nHere are the requirements for tasks:\\n• Tasks should never introduce new information to the prompt.\\nThe topic must be selected from the prompt’s keywords.\\n• Different tasks must be orthogonal from each other.\\n• All tasks combined must span the entirety of the prompt.\\n• Prioritize choosing narrower tasks.\\nYou may merge tasks if a relevant adapter spans several tasks.\\nSecond, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.\\nHere are the requirements for adapters:\\n• Adapters should only be used at most once across all tasks.\\nIf an adapter is used in one task, it should not be used in another task.\\n• Adapters should not introduce novel concepts or biases to the topic or the prompt.\\nDo not include such adapters.\\n• Adapters cannot encompass a broader scope relative to its assigned task.\\nFor example, if the task is about a “dog”, the adapter cannot be about general “animals”.\\n• Adapters cannot be too narrow in scope relative to its assigned task.\\nFor example, if a task is about pandas, do not choose highly specific pandas such as the character “Po” from Kung Fu Panda.\\nHowever, it is acceptable to choose adapters that modify the style of the task, such as “Red Pandas”.\\n• If an adapter spans multiple tasks, merge these tasks together.\\nFor example, if there is an adapter that is about “fluffy cats”, merge the topics “fluffy” and “cats” together.\\n• Avoid choosing NSFW and anthropormorphic adapters.\\nFinally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.\\nGive me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.\\ncomposer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.\\nRetrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the\\nw=1.0 w=1.5 w=2.0\\n(a) Image Saturation\\nAssigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.\\nStylus\\nSD\\nv1.5\\n(b) Task Blocking\\nAdapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).\\nStylus\\nSD\\nv1.5\\n(c) Task Diversity\\nAdding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).\\nStylus\\nSD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.\\nA.5. VLM as a Judge\\nThe full prompts to GPT-4V as a judge for textual alignment, visual fidelity, and image diversity are specified in Tables 4 and 5.\\nSystem Prompt:\\nYou are a photoshop expert judging which image has better composition quality.\\n | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n\\nUser:\\nThis is IMAGE A. Reply ’ACK’.\\n% Generated Image from Group A\\nAssistant: ACK User:\\nThis is IMAGE B. Reply ’ACK’.\\n% Generated Images from Group B\\nAssistant: ACK User:\\nRate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.\\nImage A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• You must pick a group for ’Better Quality’ / ’Better Alignment’, neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.\\nputting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.\\nVisual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.\\nSystem Prompt:\\nYou are a photoshop expert judging which set of images is more diverse.\\nScoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).\\nDiversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.\\n• Theme Interpretation: The theme can vary based on interpretation.\\nThe theme “it’s raining cats and dogs” can have a literal interpretation as cats and dogs falling from the sky or a figurative interpretation as heavy rain.\\nThe images are diverse, since they show both weather and animals.\\nIf the group only contains images of heavy rain or animals, a diversity score of 1 should be given.\\n• Main Subject: The main subject changes based on the focus across different subjects.\\nA set of images that contains a mix of images of apples and children dressed as different kinds of apples is more diverse than a set with only children dressed as apples.\\nNote the more diverse set has children as the subject for some images and apples as the subject for other images.\\nUser:\\nThis is GROUP A. Reply ’ACK’.\\n% Set of 5 Generated Images from Group A\\nAssistant: ACK User:\\nThis is GROUP B. Reply ’ACK’.\\n% Set of 5 Generated Images from Group B\\nAssistant: ACK User:\\nRate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.\\nGroup A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• Don’t forget to reward different main subjects in the diversity score.\\n• You must pick a group for ’More Diversity,’ neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 5.\\nFull prompt judging diversity using VLM.\\nwith a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.\\nDiversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.\\nA.6. Additional Diversity Scores\\nFig. 15 decomposes dFID scores over the top 100 keywords in the PartiPrompts dataset.\\nWe highlight that the largest differences stem from concepts, appearances, attributes, or styles.\\nFor example, Stylus excels over concepts ranging from animals (“bears”, “sloth”, and ‘squirrel‘) to objects (“microphone”, “box”, and “jacket”). Selected attributes can include but are not limited to: (“white”, “blue”, and “photographic”). Regardless of keyword, Stylus attains higher diversity scores across the board.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5 Parti Prompts Diversity\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nSD 1.5 Stylus\\nFigure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       " 'A. Oh, T\\nNeumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 30039–30069.\\nCurran Associates, Inc., 2023.\\n5 [6] Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, and Serena Yeung-Levy.\\nDescribing differences in image sets with natural language.\\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n6, 14 [7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or.\\nAn image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\\n1, 3 [8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang.\\nRetrieval-augmented generation for large language models: A survey, 2024.\\n2, 3 [9] David Ha, Andrew Dai, and Quoc V. Le.\\nHypernetworks, 2016.\\n1, 3 [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\\nClipscore: A reference-free evaluation metric for image captioning, 2022.\\n5 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\\nGans trained by a two time-scale update rule converge to a local nash equilibrium, 2018.\\n5, 6 [12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLora: Low-rank adaptation of large language models, 2021.\\n1, 3, 8 [13] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin.\\nLorahub: Efficient cross-task generalization via dynamic lora composition, 2024.\\n3, 5 [14] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.\\nPersonalized soups: Personalized large language model alignment via post-hoc parameter merging, 2023.\\n3 [15] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\\nElucidating the design space of diffusion-based generative models, 2022.\\n5 [16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\\nRetrieval-augmented generation for knowledge-intensive nlp tasks, 2021.\\n2, 3, 7 [17] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi.\\nPlayground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024.\\n3 [18] Jimmy Lin, Ronak Pradeep, Tommaso Teofili, and Jasper Xian.\\nVector search with openai embeddings: Lucene is all you need, 2023.\\n3, 5 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.\\nMicrosoft coco: Common objects in context, 2015.\\n1, 5, 11 [20] Jerry Liu.\\nLlamaIndex, 11 2022.\\n3 [21] Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Antonio Torralba.\\nUnsupervised compositional concepts discovery with text-to-image generative models, 2023.\\n3 [22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\\nDPM-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023.\\n5 [23] Tengyu Ma.\\nVectorize your data to gear up your ai stack., 2023.\\n3 [24] Justin Maier.\\nThe home of open-source generative ai, 2022.\\n1, 2, 3, 5, 11 [25] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.\\nPeft: State-of-the-art parameter-efficient fine-tuning methods.\\nhttps://github.com/huggingface/peft, 2022.\\n3 [26] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz.\\nSfr-embeddingmistral:enhance text retrieval with transfer learning.\\nSalesforce AI Research Blog, 2024.\\n3 [27] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo.\\nReliable fidelity and diversity metrics for generative models.\\nIn Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 7176–7185.\\nPMLR, 2020.\\n6 [28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\\nTraining language models to follow instructions with human feedback, 2022.\\n4\\nMalte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch, Massimiliano Pippi, Mayank Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir Blagojevic, Thomas Stadelmann, Tanay Soni, and Sebastian Lee.\\nHaystack: the end-to-end NLP framework for pragmatic builders, Nov. 2019.\\n3 [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.\\nSdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\\n3 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\\nLearning transferable visual models from natural language supervision, 2021.\\n3 [32] Nils Reimers.\\nSay goodbye to irrelevant search results: Cohere rerank is here, 2023.\\n3, 4, 7 [33] Nils Reimers and Iryna Gurevych.\\nSentence-bert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\\nAssociation for Computational Linguistics, 11 2019.\\n3 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\\nHigh-resolution image synthesis with latent diffusion models, 2022.\\n3, 5 [35] Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly.\\nAssessing generative models via precision and recall.\\nIn Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 5234–5243, 2018.\\n6 [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision.\\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2818–2826.\\nIEEE Computer Society, 2016.\\n6 [37] Gemini Team.\\nGemini: A family of highly capable multimodal models, 2023.\\n3, 5, 11 [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\\nLlama: Open and efficient foundation language models, 2023.\\n3 [39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\nLlama 2: Open foundation and fine-tuned chat models, 2023.\\n3 [40] Hanqing Wang, Bowen Ping, Shuo Wang, Xu Han, Yun Chen, Zhiyuan Liu, and Maosong Sun.\\nLora-flow: Dynamic lora fusion for large language models in generative tasks, 2024.\\n3 [41] Xinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Graham Neubig.\\nEfficient test time adapter ensembling for low-resource language varieties.\\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 730–737, Punta Cana, Dominican Republic, Nov. 2021.\\nAssociation for Computational Linguistics.\\n3 [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\n11, 14 [43] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\\nHuggingface’s transformers: State-of-the-art natural language processing, 2020.\\n1, 3, 5, 11 [44] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models, 2023.\\n11, 14 [45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu.\\nScaling autoregressive models for content-rich text-to-image generation, 2022.\\n5, 6 [46] Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, and Fei Wu.\\nLoraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild, 2024.\\n3, 5, 7 [47] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen.\\nMulti-lora composition for image generation, 2024.\\n3, 4, 6',\n",
       " 'A. Appendix\\n0.5\\nA.1. Details of the Refiner VLM\\n0.4\\nWe provide a complete example input to the refiner’s VLM in Tab.\\n2.\\nThe prompt utilizes Chain-of-Thought (CoT) prompting, which decomposes the VLM’s goal of producing better adapter descriptions into two steps [42, 44].\\nInitially, the VLM categorizes the adapter’s task into one of several topics—such as concepts, styles, characters, or poses.\\nSubsequently, the VLM is prompted to elaborate on why the adapter is associated with a particular topic and how it modifies images within that context.\\nWe found that this two step logical process significantly improved the structure and quality of model responses.\\n0.3\\n0.2\\n0.1\\n0.0\\nCategory\\n(a) Distribution of adapters across categories.\\n0.004\\nA.2. Details of the Composer LLM\\n0.003\\nWe provide a full example prompt of the composer’s LLM component in Tab.\\n3, which is plugged through the Gemini\\n0.002\\n1.5 endpoint [37].\\nOur experiments feed in descriptions of the top 150 adapters into the LLM’s context.\\nUsing a Chain-of-Thought (CoT) approach, the prompt is structured to first identify keywords or tasks, then allocate appropriate adapters to these tasks.\\nIf necessary, it merges keywords for adapters that span multiple tasks [42, 44].\\n0.001\\n0.000\\nTop 500 Adapters sorted by Download Count\\nA.3. Stylus-Bench Characterization\\nThis section describes StylusDocs, which comprises of 76K Low Rank Adapters (LoRAs) from public repositories, including Civit AI and Hugging Face [24, 43].\\nWe excluded NSFW-labeled adapters from the Civit AI dataset, which originally contained over 100K LoRAs.\\nFigure 13 illustrates the distribution of adapters across various semantic categories and their popularity, measured by download counts.\\nA significant majority, 70%, of adapters belong to the character and celebrity category, primarily consisting of anime or game characters.\\nAnother 13% of adapters modify image style, 8% adjust clothing, and 4% represent various concepts (Fig. 13a).\\nThese statistics indicate that our experiments consider a minor proportion of adapters, as the COCO dataset does not feature characters or celebrities [19].\\nDespite this, Stylus outperforms base Stable Diffusion.\\nFurthermore, the popularity of adapters follows a Pareto distribution, where the top adapters receive exponentially more downloads than the others (Fig. 13a).\\nHowever, the top adapter accounts for only 0.5% of total downloads, which suggests that the distribution is long-tailed.\\nA.4. Failure Modes\\nWe detail different failure modes that were discovered while developing Stylus.\\nImage saturation.\\nThe quality of image generation is highly depend on adapters’ weights.\\nIf the assigned weight is above the recommended value, the adapter negatively impacts image generation, leading to a growing number of visual inconsistencies and artifacts.\\nIn Fig. 14a, assigning a high weight to a “James Bond” LoRA increases images exposure and introducing significant visual tearing.\\nStylus mitigates over-saturation with its refiner component, which extract the right adapter weights from the adapter’s model card.\\nLastly, Stylus uniformly weights adapters based on their associated tasks, ensuring that similar adapters do not significantly impact their corresponding tasks.\\n0.005\\n(b) Top 500 adapters ranked by percentage of downloads.\\nFigure 13.\\nWorkload Characterization of StylusDocs.\\n(a) Most adapters are categorized as characters or celebrities.\\n(b) Adapter popularity exhibits a power-law distribution, with the top adapters receiving exponentially more downloads than the others.\\nTask Blocking.\\nComposing adapters presents the risk of overwriting existing concepts or tasks specified in the prompt and other selected adapters.\\nWe illustrate several examples in Figure 2—a train LoRA overrides the toy train concept (left), a park bench LoRA masks a person in an orange blanket (middle), and a fancy cake LoRA erases the image of a man eating the cake (right).\\nTask blocking typically arises from two main issues: the adapter weight set too high or too many adapters merged into the base model.\\nStylus addresses this by reducing an adapter’s weight with uniform weighting per task, while the masking scheme reduces the number of selected adapters.\\nAlthough Stylus does not completely solve task blocking, it offers simple heuristics to mitigate the issue.\\nImage Prompts\\nPrompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.\\nPrompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>\\nModel Card Description\\n• Title: Dwayne”The Rock” Johnson (LoRA)\\n• Tags: Celebrity, Photorealistic, Hollywood, Celeb\\n• Trigger Words: Th3R0ck\\n• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.\\nYour goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:\\n1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.\\n• Some prompts may specify the adapter weight (i.e. <lora:NAME:WEIGHT>).\\nIf provided, you will need to infer the adapter’s name and weight.\\nPrioritize this weight over the author’s recommended weight.\\n2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.\\n• The model card description may be incorrect, misleading, or incomplete.\\n• The model card may specify the weight of the model adapter, or the recommended range.\\nFind the recommended weight of the adapter (default is 0.8).\\n% Chain-of-Thought Prompting\\nAgain, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.\\nFirst, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:\\n• Do not describe any training or dataset-related details.\\n• Provide additional context from your prior knowledge if there is insufficient information.\\n• Do not hallucinate and repeat text.\\nOutput only english words and sentences.\\nSecond, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.\\nPlease format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.\\nTask Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.\\nLow quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.\\nRetrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth\\nProvided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.\\n% Chain-of-Thought Prompting\\nFirst, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.\\nHere are the requirements for tasks:\\n• Tasks should never introduce new information to the prompt.\\nThe topic must be selected from the prompt’s keywords.\\n• Different tasks must be orthogonal from each other.\\n• All tasks combined must span the entirety of the prompt.\\n• Prioritize choosing narrower tasks.\\nYou may merge tasks if a relevant adapter spans several tasks.\\nSecond, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.\\nHere are the requirements for adapters:\\n• Adapters should only be used at most once across all tasks.\\nIf an adapter is used in one task, it should not be used in another task.\\n• Adapters should not introduce novel concepts or biases to the topic or the prompt.\\nDo not include such adapters.\\n• Adapters cannot encompass a broader scope relative to its assigned task.\\nFor example, if the task is about a “dog”, the adapter cannot be about general “animals”.\\n• Adapters cannot be too narrow in scope relative to its assigned task.\\nFor example, if a task is about pandas, do not choose highly specific pandas such as the character “Po” from Kung Fu Panda.\\nHowever, it is acceptable to choose adapters that modify the style of the task, such as “Red Pandas”.\\n• If an adapter spans multiple tasks, merge these tasks together.\\nFor example, if there is an adapter that is about “fluffy cats”, merge the topics “fluffy” and “cats” together.\\n• Avoid choosing NSFW and anthropormorphic adapters.\\nFinally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.\\nGive me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.\\ncomposer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.\\nRetrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the\\nw=1.0 w=1.5 w=2.0\\n(a) Image Saturation\\nAssigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.\\nStylus\\nSD\\nv1.5\\n(b) Task Blocking\\nAdapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).\\nStylus\\nSD\\nv1.5\\n(c) Task Diversity\\nAdding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).\\nStylus\\nSD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.\\nA.5. VLM as a Judge\\nThe full prompts to GPT-4V as a judge for textual alignment, visual fidelity, and image diversity are specified in Tables 4 and 5.\\nSystem Prompt:\\nYou are a photoshop expert judging which image has better composition quality.\\n | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n\\nUser:\\nThis is IMAGE A. Reply ’ACK’.\\n% Generated Image from Group A\\nAssistant: ACK User:\\nThis is IMAGE B. Reply ’ACK’.\\n% Generated Images from Group B\\nAssistant: ACK User:\\nRate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.\\nImage A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• You must pick a group for ’Better Quality’ / ’Better Alignment’, neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.\\nputting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.\\nVisual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.\\nSystem Prompt:\\nYou are a photoshop expert judging which set of images is more diverse.\\nScoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).\\nDiversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.\\n• Theme Interpretation: The theme can vary based on interpretation.\\nThe theme “it’s raining cats and dogs” can have a literal interpretation as cats and dogs falling from the sky or a figurative interpretation as heavy rain.\\nThe images are diverse, since they show both weather and animals.\\nIf the group only contains images of heavy rain or animals, a diversity score of 1 should be given.\\n• Main Subject: The main subject changes based on the focus across different subjects.\\nA set of images that contains a mix of images of apples and children dressed as different kinds of apples is more diverse than a set with only children dressed as apples.\\nNote the more diverse set has children as the subject for some images and apples as the subject for other images.\\nUser:\\nThis is GROUP A. Reply ’ACK’.\\n% Set of 5 Generated Images from Group A\\nAssistant: ACK User:\\nThis is GROUP B. Reply ’ACK’.\\n% Set of 5 Generated Images from Group B\\nAssistant: ACK User:\\nRate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.\\nGroup A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• Don’t forget to reward different main subjects in the diversity score.\\n• You must pick a group for ’More Diversity,’ neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 5.\\nFull prompt judging diversity using VLM.\\nwith a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.\\nDiversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.\\nA.6. Additional Diversity Scores\\nFig. 15 decomposes dFID scores over the top 100 keywords in the PartiPrompts dataset.\\nWe highlight that the largest differences stem from concepts, appearances, attributes, or styles.\\nFor example, Stylus excels over concepts ranging from animals (“bears”, “sloth”, and ‘squirrel‘) to objects (“microphone”, “box”, and “jacket”). Selected attributes can include but are not limited to: (“white”, “blue”, and “photographic”). Regardless of keyword, Stylus attains higher diversity scores across the board.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5 Parti Prompts Diversity\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nSD 1.5 Stylus\\nFigure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       " 'A.1. Details of the Refiner VLM\\n0.4\\nWe provide a complete example input to the refiner’s VLM in Tab.\\n2.\\nThe prompt utilizes Chain-of-Thought (CoT) prompting, which decomposes the VLM’s goal of producing better adapter descriptions into two steps [42, 44].\\nInitially, the VLM categorizes the adapter’s task into one of several topics—such as concepts, styles, characters, or poses.\\nSubsequently, the VLM is prompted to elaborate on why the adapter is associated with a particular topic and how it modifies images within that context.\\nWe found that this two step logical process significantly improved the structure and quality of model responses.\\n0.3\\n0.2\\n0.1\\n0.0\\nCategory\\n(a) Distribution of adapters across categories.\\n0.004',\n",
       " 'Category\\n(a) Distribution of adapters across categories.\\n0.004',\n",
       " 'A.2. Details of the Composer LLM\\n0.003\\nWe provide a full example prompt of the composer’s LLM component in Tab.\\n3, which is plugged through the Gemini\\n0.002\\n1.5 endpoint [37].\\nOur experiments feed in descriptions of the top 150 adapters into the LLM’s context.\\nUsing a Chain-of-Thought (CoT) approach, the prompt is structured to first identify keywords or tasks, then allocate appropriate adapters to these tasks.\\nIf necessary, it merges keywords for adapters that span multiple tasks [42, 44].\\n0.001\\n0.000\\nTop 500 Adapters sorted by Download Count',\n",
       " 'A.3. Stylus-Bench Characterization\\nThis section describes StylusDocs, which comprises of 76K Low Rank Adapters (LoRAs) from public repositories, including Civit AI and Hugging Face [24, 43].\\nWe excluded NSFW-labeled adapters from the Civit AI dataset, which originally contained over 100K LoRAs.\\nFigure 13 illustrates the distribution of adapters across various semantic categories and their popularity, measured by download counts.\\nA significant majority, 70%, of adapters belong to the character and celebrity category, primarily consisting of anime or game characters.\\nAnother 13% of adapters modify image style, 8% adjust clothing, and 4% represent various concepts (Fig. 13a).\\nThese statistics indicate that our experiments consider a minor proportion of adapters, as the COCO dataset does not feature characters or celebrities [19].\\nDespite this, Stylus outperforms base Stable Diffusion.\\nFurthermore, the popularity of adapters follows a Pareto distribution, where the top adapters receive exponentially more downloads than the others (Fig. 13a).\\nHowever, the top adapter accounts for only 0.5% of total downloads, which suggests that the distribution is long-tailed.',\n",
       " 'A.4. Failure Modes\\nWe detail different failure modes that were discovered while developing Stylus.\\nImage saturation.\\nThe quality of image generation is highly depend on adapters’ weights.\\nIf the assigned weight is above the recommended value, the adapter negatively impacts image generation, leading to a growing number of visual inconsistencies and artifacts.\\nIn Fig. 14a, assigning a high weight to a “James Bond” LoRA increases images exposure and introducing significant visual tearing.\\nStylus mitigates over-saturation with its refiner component, which extract the right adapter weights from the adapter’s model card.\\nLastly, Stylus uniformly weights adapters based on their associated tasks, ensuring that similar adapters do not significantly impact their corresponding tasks.\\n0.005\\n(b) Top 500 adapters ranked by percentage of downloads.\\nFigure 13.\\nWorkload Characterization of StylusDocs.\\n(a) Most adapters are categorized as characters or celebrities.\\n(b) Adapter popularity exhibits a power-law distribution, with the top adapters receiving exponentially more downloads than the others.\\nTask Blocking.\\nComposing adapters presents the risk of overwriting existing concepts or tasks specified in the prompt and other selected adapters.\\nWe illustrate several examples in Figure 2—a train LoRA overrides the toy train concept (left), a park bench LoRA masks a person in an orange blanket (middle), and a fancy cake LoRA erases the image of a man eating the cake (right).\\nTask blocking typically arises from two main issues: the adapter weight set too high or too many adapters merged into the base model.\\nStylus addresses this by reducing an adapter’s weight with uniform weighting per task, while the masking scheme reduces the number of selected adapters.\\nAlthough Stylus does not completely solve task blocking, it offers simple heuristics to mitigate the issue.\\nImage Prompts\\nPrompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.\\nPrompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>\\nModel Card Description\\n• Title: Dwayne”The Rock” Johnson (LoRA)\\n• Tags: Celebrity, Photorealistic, Hollywood, Celeb\\n• Trigger Words: Th3R0ck\\n• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.\\nYour goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:\\n1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.\\n• Some prompts may specify the adapter weight (i.e. <lora:NAME:WEIGHT>).\\nIf provided, you will need to infer the adapter’s name and weight.\\nPrioritize this weight over the author’s recommended weight.\\n2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.\\n• The model card description may be incorrect, misleading, or incomplete.\\n• The model card may specify the weight of the model adapter, or the recommended range.\\nFind the recommended weight of the adapter (default is 0.8).\\n% Chain-of-Thought Prompting\\nAgain, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.\\nFirst, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:\\n• Do not describe any training or dataset-related details.\\n• Provide additional context from your prior knowledge if there is insufficient information.\\n• Do not hallucinate and repeat text.\\nOutput only english words and sentences.\\nSecond, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.\\nPlease format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.\\nTask Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.\\nLow quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.\\nRetrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth\\nProvided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.\\n% Chain-of-Thought Prompting\\nFirst, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.\\nHere are the requirements for tasks:\\n• Tasks should never introduce new information to the prompt.\\nThe topic must be selected from the prompt’s keywords.\\n• Different tasks must be orthogonal from each other.\\n• All tasks combined must span the entirety of the prompt.\\n• Prioritize choosing narrower tasks.\\nYou may merge tasks if a relevant adapter spans several tasks.\\nSecond, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.\\nHere are the requirements for adapters:\\n• Adapters should only be used at most once across all tasks.\\nIf an adapter is used in one task, it should not be used in another task.\\n• Adapters should not introduce novel concepts or biases to the topic or the prompt.\\nDo not include such adapters.\\n• Adapters cannot encompass a broader scope relative to its assigned task.\\nFor example, if the task is about a “dog”, the adapter cannot be about general “animals”.\\n• Adapters cannot be too narrow in scope relative to its assigned task.\\nFor example, if a task is about pandas, do not choose highly specific pandas such as the character “Po” from Kung Fu Panda.\\nHowever, it is acceptable to choose adapters that modify the style of the task, such as “Red Pandas”.\\n• If an adapter spans multiple tasks, merge these tasks together.\\nFor example, if there is an adapter that is about “fluffy cats”, merge the topics “fluffy” and “cats” together.\\n• Avoid choosing NSFW and anthropormorphic adapters.\\nFinally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.\\nGive me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.\\ncomposer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.\\nRetrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the\\nw=1.0 w=1.5 w=2.0\\n(a) Image Saturation\\nAssigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.\\nStylus\\nSD\\nv1.5\\n(b) Task Blocking\\nAdapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).\\nStylus\\nSD\\nv1.5\\n(c) Task Diversity\\nAdding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).\\nStylus\\nSD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       " 'Image Prompts\\nPrompt 1: Photo of Dwayne Johnson, wearing military clothes and cap, dramatic lighting, <lora:TheRockV3:0.9>.\\nPrompt 2: Photo of Dwayne Johnson, wearing a Superman suit, high quality, <lora:TheRockV3:1>.\\nPrompt 3: Photo of Dwayne Johnson, wearing an Armani tuxedo, <lora:TheRockV3:0.9>',\n",
       " 'Model Card Description\\n• Title: Dwayne”The Rock” Johnson (LoRA)\\n• Tags: Celebrity, Photorealistic, Hollywood, Celeb\\n• Trigger Words: Th3R0ck\\n• Description: Had to make this one, due to Kevin Hart Lora.\\nRecommended lora strength: 0.9.\\n% Author descriptions may be misleading or incomplete.\\nYour goal is improve the description of a model adapter’s task for Stable Diffusion, with images, prompts, and descriptions pulled from popular model repositories.\\nAbove, we have provided the following information and the associated constraints:\\n1. Examples of generated images (from left to right) from the adapter and the corresponding user-provided prompts.\\n• Some prompts may specify the adapter weight (i.e. <lora:NAME:WEIGHT>).\\nIf provided, you will need to infer the adapter’s name and weight.\\nPrioritize this weight over the author’s recommended weight.\\n2. The adapter’s model card from the original author.\\nThis includes the title, tags, trigger words, and description.\\n• The model card description may be incorrect, misleading, or incomplete.\\n• The model card may specify the weight of the model adapter, or the recommended range.\\nFind the recommended weight of the adapter (default is 0.8).\\n% Chain-of-Thought Prompting\\nAgain, your mission is to provide a clear description of the model’s adapter purpose and its impact on the image.\\nTo do so, you should implicitly categorize the model adapter into only one of the following topics: [Concept, Style, Pose, Action, Celebrity/Character, Clothing, Background, Building, Vehicle, Animal, Action].\\nDo not associate an adapter with a topic that is vague or uninteresting.\\nFirst, describe the topic associated with the adapter and explain how this adapter alters the images, based on the common elements observed in the example images.\\nYour requirements are:\\n• Do not describe any training or dataset-related details.\\n• Provide additional context from your prior knowledge if there is insufficient information.\\n• Do not hallucinate and repeat text.\\nOutput only english words and sentences.\\nSecond, recommend an optimal weight for the adapter as a float.\\nDo not specify a range, only give one value.\\nPlease format your output as follows: Example 1: [Description of adapter and its weight] Example 2: [Description of adapter and its weight] Table 2.\\nFull prompt for the refiner VLM to generate better adapter descriptions.\\nTask Diversity.\\nMerging adapters into the base model overwrites the base model’s prior distribution over an adapter’s corresponding tasks.\\nIf an adapter is not finetuned on a diverse set of images, diversity is significantly reduced among different instances of the same task.\\nWe present three examples in Fig. 14c, over different prompts that specify multiple instances of the same task (teddy bears, women, and apples).\\nWe observe that all instances of each task are highly identical with one another.\\nStylus offers no solution to address or mitigate this problem.\\nLow quality adapters.\\nLow quality adapters can significantly degrade the quality of image generation, as shown by corrupted images in Fig. 14d.\\nThis issue typically arises from poor training data or from fine-tuning the adapter for too many epochs.\\nStylus attempts to blacklist such adapters.\\nRetrieved Adapter Descriptions 42: This LoRA is for the concept of dragon, a mythical creature.\\nIt generates images of dragons with a variety of different appearances, including both Western and Eastern styles 120: This LoRA steers the image generation towards a fantasy castle, with a focus on the building and its surroundings.\\nThe castle is depicted as a grand structure, often with towering spires, intricate architecture, and a sense of grandeur 3478: This LoRA is designed to generate images of a Chinese dragon breathing fire.\\nIt generates images of a dragon with a long, serpentine body, covered in scales, with a large head and sharp teeth.\\nThe dragon is breathing fire, with flames coming out of its mouth 1337: This LoRA is designed to generate images of animals breathing fire.\\nIt generates images of animals, such as rabbits, dragons, and frogs, breathing fire.\\nThe fire is shown as a bright, orange-yellow flame that is coming out of the animal’s mouth\\nProvided above are the IDs and descriptions for different model adapters (e.g. LoRA) for Stable Diffusion that may be related to the prompt.\\nYour goal is to fetch adapters that can improve image fidelity.\\nThe prompt is: Dragon breathing fire on a castle.\\n% Chain-of-Thought Prompting\\nFirst, segment the prompt into different tasks—concepts, styles, poses, celebrities, backgrounds, objects, actions, or adjectives—from the prompt’s keywords.\\nHere are the requirements for tasks:\\n• Tasks should never introduce new information to the prompt.\\nThe topic must be selected from the prompt’s keywords.\\n• Different tasks must be orthogonal from each other.\\n• All tasks combined must span the entirety of the prompt.\\n• Prioritize choosing narrower tasks.\\nYou may merge tasks if a relevant adapter spans several tasks.\\nSecond, for each task, provide 0-5 of the most relevant model adapters to the task.\\nFor each adapter, infer an adapter’s main function from its description.\\nThis function must directly match at least one task and the context of the prompt.\\nIf the adapter is indirectly relevant, do not include it.\\nHere are the requirements for adapters:\\n• Adapters should only be used at most once across all tasks.\\nIf an adapter is used in one task, it should not be used in another task.\\n• Adapters should not introduce novel concepts or biases to the topic or the prompt.\\nDo not include such adapters.\\n• Adapters cannot encompass a broader scope relative to its assigned task.\\nFor example, if the task is about a “dog”, the adapter cannot be about general “animals”.\\n• Adapters cannot be too narrow in scope relative to its assigned task.\\nFor example, if a task is about pandas, do not choose highly specific pandas such as the character “Po” from Kung Fu Panda.\\nHowever, it is acceptable to choose adapters that modify the style of the task, such as “Red Pandas”.\\n• If an adapter spans multiple tasks, merge these tasks together.\\nFor example, if there is an adapter that is about “fluffy cats”, merge the topics “fluffy” and “cats” together.\\n• Avoid choosing NSFW and anthropormorphic adapters.\\nFinally, for each selected adapter, provide a strong reason for why this adapter is relevant to the prompt, directly matches the keyword, and improves image quality.\\nGive me the answer only.\\nPlease format your output as follows: Example 1: [Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nExample 2:[Dictionary of tasks to the associated adapter ids and reasons for their selection.]\\nTable 3.\\nFull prompt for the composer LLM.\\ncomposer may classify the adapter into an incorrect task.\\nWe detail three examples in Figure 4.\\nStylus selects an “okapi” (forest giraffe) LoRA, known for its distinctive zebra-like appearance, causing the generated giraffes to adopt the okapi’s skin texture.\\nIn the middle, Stylus selects a flowery vase LoRA, a misinterpretation of the prompt “orange flowers placed in a vase.”\\nOn the right, the composer However, our blacklist is not comprehensive, and as a result, Stylus may still occasionally select low-quality adapters.\\nRetrieval Errors.\\nStylus’s retrieval process involves three stages, each introducing potential errors that can compound in later stages.\\nFor instance, the refiner may return incorrect descriptions of an adapter’s task, while the\\nw=1.0 w=1.5 w=2.0',\n",
       " '(a) Image Saturation\\nAssigning too high of a weight to a “James Bond” adapter leads to significant degradation in visual fidelity.\\nStylus\\nSD\\nv1.5',\n",
       " 'Stylus\\nSD\\nv1.5',\n",
       " 'SD\\nv1.5',\n",
       " '(b) Task Blocking\\nAdapters can block a prompt’s or other adapter’s tasks (i.e. toy trains, person in orange blanket, or man eating cake).\\nStylus\\nSD\\nv1.5',\n",
       " 'Stylus\\nSD\\nv1.5',\n",
       " 'SD\\nv1.5',\n",
       " '(c) Task Diversity\\nAdding an adapter reduces diversity of instances within a single task (i.e. teddy bears, woman, and apples).\\nStylus\\nSD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       " 'Stylus\\nSD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       " 'SD\\nv1.5\\n(d) Low quality adapters.\\nLow quality adapters can significantly impact visual fidelity.\\nWe blacklist such adapters.\\n(e) Retrieval Errors.\\nRetrieval errors can lead to foreign biases in image generation and deliberate misinterpretations of the prompt.\\nFigure 14.\\nCategorization of Different Failure Modes.\\nTo distinguish the two images (or groups of images), the VLM exploits multi-turn prompting: We provide each image (or group of images) labeled with IMAGE/GROUP A or IMAGE/GROUP B. Note that the ACK messages are not generated by the VLM; instead, it is part of VLM’s context window.\\nWe provide the rubric, detailed instructions, reminders, and example model outputs in our prompt.\\nFor scoring, the VLM employs Chain-of-Thought (CoT) prompting to output scores 0-2, similar to VisDiff [6, 42, 44].\\nWe observe that larger ranges (5-10) leads the model towards abstaining from making decisions, as it avoids out- incorrectly chooses a human baby adapter for the prompt “a baby daikon radish in a tutu.\\n”, resulting in images of babies instead of daikons.\\nStylus includes an option to self-repair faulty composer outputs with multi-turn conversations, which can improve adapter selection.',\n",
       " 'A.5. VLM as a Judge\\nThe full prompts to GPT-4V as a judge for textual alignment, visual fidelity, and image diversity are specified in Tables 4 and 5.\\nSystem Prompt:\\nYou are a photoshop expert judging which image has better composition quality.\\n | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n\\nUser:\\nThis is IMAGE A. Reply ’ACK’.\\n% Generated Image from Group A\\nAssistant: ACK User:\\nThis is IMAGE B. Reply ’ACK’.\\n% Generated Images from Group B\\nAssistant: ACK User:\\nRate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.\\nImage A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• You must pick a group for ’Better Quality’ / ’Better Alignment’, neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.\\nputting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.\\nVisual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.\\nSystem Prompt:\\nYou are a photoshop expert judging which set of images is more diverse.\\nScoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).\\nDiversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.\\n• Theme Interpretation: The theme can vary based on interpretation.\\nThe theme “it’s raining cats and dogs” can have a literal interpretation as cats and dogs falling from the sky or a figurative interpretation as heavy rain.\\nThe images are diverse, since they show both weather and animals.\\nIf the group only contains images of heavy rain or animals, a diversity score of 1 should be given.\\n• Main Subject: The main subject changes based on the focus across different subjects.\\nA set of images that contains a mix of images of apples and children dressed as different kinds of apples is more diverse than a set with only children dressed as apples.\\nNote the more diverse set has children as the subject for some images and apples as the subject for other images.\\nUser:\\nThis is GROUP A. Reply ’ACK’.\\n% Set of 5 Generated Images from Group A\\nAssistant: ACK User:\\nThis is GROUP B. Reply ’ACK’.\\n% Set of 5 Generated Images from Group B\\nAssistant: ACK User:\\nRate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.\\nGroup A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• Don’t forget to reward different main subjects in the diversity score.\\n• You must pick a group for ’More Diversity,’ neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 5.\\nFull prompt judging diversity using VLM.\\nwith a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.\\nDiversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       " 'System Prompt:\\nYou are a photoshop expert judging which image has better composition quality.\\n | Scoring: quality), 1 (visually aesthetic but has elements with distor- tion/missing features/extra features), 0 (low visual quality, issues with texture/blur/visual artifacts). | Compositional quality scores can be 2 (very high | Scoring: porates part of the theme but not all), 0 (not aligned). | Alignment scores can be 2 (fully aligned), 1 (incor-\\n | --- | --- | --- | ---\\n | Composition can be broken down into three main aspects:\\n | • Clarity: If the image is blurry, poorly lit, or has poor composition (objects obstructing each other), it gets scores 0.\\n | • Disfigured Parts: This applies to both body parts of humans and animals as well as objects like motorcycles. If the image has a hand that has 6 fingers it gets a 1 for having otherwise normal fingers, but the hand should not have two fingers. If the fingers themselves are disfigured showing lips and teeth warped in, it gets a 0.\\n | • Detail: If the sail of a sailboat’s sail shows dynamic ripples and ornate patterns, this shows detail and should get a score of 2. If it’s monochrome and flat, it gets a score of 1. If it looks like a cartoon and is inconsistent with the environment, give a score of 0.\\n | We provide several examples:\\n | • If the prompt is ’shoes’, and an image is a sock, this is not aligned and gets a score of 0.\\n | • If the prompt is ’shoes without laces’, but the shoes have laces, this is somewhat aligned and gets a score of 1.\\n | • If the prompt is ’a concert without fans’, but there’s fans in the image, pick the images that show fewer fans.\\n',\n",
       " 'User:\\nThis is IMAGE A. Reply ’ACK’.\\n% Generated Image from Group A',\n",
       " 'Assistant: ACK User:\\nThis is IMAGE B. Reply ’ACK’.\\n% Generated Images from Group B',\n",
       " 'Assistant: ACK User:\\nRate the quality of the images in GROUP A and GROUP B. For each image, provide a score and explanation.\\nImage A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• You must pick a group for ’Better Quality’ / ’Better Alignment’, neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.\\nputting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.\\nVisual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.',\n",
       " 'Image A Quality: <SCORE>(<EXPLANATION>) Image B Quality: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• You must pick a group for ’Better Quality’ / ’Better Alignment’, neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 4.\\nFull prompt judging compositional quality (left) or textual alignment (right) using VLM.\\nputting extreme scores.\\nHowever, the score range 0-2 provides the VLM sufficient granularity to express preferences and prompt the model to summarize the key differences.\\nVisual Quality.\\nOur evaluation assesses visual quality through three metrics: clarify, disfigurements, and detail.\\nFirst, the VLM assigns low clarity scores if an image is blurry, poorly lighted, or exhibits poor compositional quality.\\nWe note that LoRAs are trained over specific tasks/concepts; the model determines how to compose different concepts.\\nFor instance, a rhinoceros LoRA combined Textual Alignment.\\nThe VLM scores how well a generated image follows the prompt’s specifications.\\nWe note that prompts with negations (e.g. “concert with no fans” or “harbor with no boats”) fail for both Stylus and the Stable Diffusion checkpoint.\\nHence, we prompted the VLM to assign better scores for images that produced less fans or boats.\\nFurthermore, as adapters can potentially block existing concepts in the image (see Fig. 14b), the VLM allocates partial credit in scenarios where images partially capture the set of keywords in the prompt.',\n",
       " 'System Prompt:\\nYou are a photoshop expert judging which set of images is more diverse.\\nScoring: Diversity scores can be 2 (very diverse), 1 (somewhat diverse), 0 (not diverse).\\nDiversity can be decomposed based on 1) the interpretation of the theme and 2) the main subject.\\n• Theme Interpretation: The theme can vary based on interpretation.\\nThe theme “it’s raining cats and dogs” can have a literal interpretation as cats and dogs falling from the sky or a figurative interpretation as heavy rain.\\nThe images are diverse, since they show both weather and animals.\\nIf the group only contains images of heavy rain or animals, a diversity score of 1 should be given.\\n• Main Subject: The main subject changes based on the focus across different subjects.\\nA set of images that contains a mix of images of apples and children dressed as different kinds of apples is more diverse than a set with only children dressed as apples.\\nNote the more diverse set has children as the subject for some images and apples as the subject for other images.',\n",
       " 'User:\\nThis is GROUP A. Reply ’ACK’.\\n% Set of 5 Generated Images from Group A',\n",
       " 'Assistant: ACK User:\\nThis is GROUP B. Reply ’ACK’.\\n% Set of 5 Generated Images from Group B',\n",
       " 'Assistant: ACK User:\\nRate the diversity of the images in GROUP A and GROUP B. For each group, provide a score and explanation.\\nGroup A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• Don’t forget to reward different main subjects in the diversity score.\\n• You must pick a group for ’More Diversity,’ neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 5.\\nFull prompt judging diversity using VLM.\\nwith a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.\\nDiversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       " 'Group A Diversity: <SCORE>(<EXPLANATION>) Group B Diversity: <SCORE>(<EXPLANATION>) Preference: Group <CHOICE>(<EXPLANATION>)\\n% Prevent VLM from returning neutral results.\\nI’ll make my own judgement using your results, your response is just an opinion as part of a rigorous process.\\nI provide additional requirements below:\\n• Don’t forget to reward different main subjects in the diversity score.\\n• You must pick a group for ’More Diversity,’ neither is not an option.\\n• If it’s a close call, make a choice first then explain why in parenthesis.\\nTable 5.\\nFull prompt judging diversity using VLM.\\nwith a motorcycle LoRA led to images of motorcycles draped with rhinoceros hide.\\nAs such, the VLM assigns partial credit when the model fails to combine concepts in a meaningful way.\\nSecond, the VLM assigns lower scores by judging if an image has disfigured parts.\\nFor instance, diffusion models have trouble accurately depicting a human hand, oftentimes generating extra fingers.\\nFinally, the VLM’s final score depends on the detail of image.\\nWe find that adapters are able to bring greater detail to certain concepts.\\nFor example, an elephant adapter generates elephants with much greater detail than that of the base model.\\nHowever, we note that the VLM is not good at detecting subtleties in detail.\\nDiversity.\\nFor each prompt, we generate five images each for Stylus and the Stable Diffusion checkpoint.\\nThese images are then assessed with a VLM (Visual Language Model, GPT-4V) judge, which rates and ranks them based on diversity.\\nIn Tab.\\n5, we measure diversity through two metrics.\\nThe first metric, theme interpretation, measures di- versity based on the interpretation of the prompt, which is often under-specified.\\nWe find that different thematic interpretations improves model response due to non-ambiguity.\\nThe second metric measures diversity by the variance of focus across different subjects.\\nWe find that many prompts often under-specify which subject is the focus on the image.',\n",
       " 'A.6. Additional Diversity Scores\\nFig. 15 decomposes dFID scores over the top 100 keywords in the PartiPrompts dataset.\\nWe highlight that the largest differences stem from concepts, appearances, attributes, or styles.\\nFor example, Stylus excels over concepts ranging from animals (“bears”, “sloth”, and ‘squirrel‘) to objects (“microphone”, “box”, and “jacket”). Selected attributes can include but are not limited to: (“white”, “blue”, and “photographic”). Regardless of keyword, Stylus attains higher diversity scores across the board.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5 Parti Prompts Diversity\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nSD 1.5 Stylus\\nFigure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       " '2.5 Parti Prompts Diversity\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nSD 1.5 Stylus\\nFigure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       " 'SD 1.5 Stylus\\nFigure 15. dFID for top 100 keywords in PartiPrompts dataset.\\nStylus leads to consistently higher diversity when compared to Stable Diffusion checkpoints, especially for words describing concepts and attributes.',\n",
       " 'Hallucination of Multimodal Large Language Models: A Survey\\nZECHEN BAI, Show Lab, National University of Singapore, Singapore PICHAO WANG, Amazon Prime Video, USA TIANJUN XIAO, AWS Shanghai AI Lab, China TONG HE, AWS Shanghai AI Lab, China ZONGBO HAN, Show Lab, National University of Singapore, Singapore ZHENG ZHANG, AWS Shanghai AI Lab, China MIKE ZHENG SHOU∗, Show Lab, National University of Singapore, Singapore This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks.\\nDespite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications.\\nThis problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies.\\nWe review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue.\\nAdditionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research.\\nBy drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field.\\nThrough our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike.\\nResources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.',\n",
       " 'CCS Concepts: •Computingmethodologies→Computer vision;Natural language processing;Machine\\nlearning.\\nAdditional Key Words and Phrases: Hallucination, Multimodal, Large Language Models, Vision-Language Models.',\n",
       " 'ACM Reference Format:\\nZechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou.\\n2024.\\nHallucination of Multimodal Large Language Models: A Survey.\\nPreprint 1, 1 (April 2024), 30 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX ∗Corresponding Author Authors’ addresses: Zechen Bai, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore, zechenbai@u.nus.edu; Pichao Wang, Amazon Prime Video, Washington, USA, pichaowang@gmail.com; Tianjun Xiao, AWS Shanghai AI Lab, Shanghai, China, tianjux@amazon.com; Tong He, AWS Shanghai AI Lab, Shanghai, China, htong@ amazon.com; Zongbo Han, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore, hanzb1997@gmail.com; Zheng Zhang, AWS Shanghai AI Lab, Shanghai, China, zhaz@amazon.com; Mike Zheng Shou, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore, mike.zheng.shou@gmail.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\\nCopyrights for components of this work owned by others than the author(s) must be honored.\\nAbstracting with credit is permitted.\\nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\\nRequest permissions from permissions@acm.org.\\n© 2024 Copyright held by the owner/author(s).\\nPublication rights licensed to ACM. ACM 0000-0000/2024/4-ART https://doi.org/XXXXXXX.XXXXXXX',\n",
       " 'INTRODUCTION\\nRecently, the emergence of large language models (LLMs) [29, 81, 85, 99, 132] has dominated a wide range of tasks in natural language processing (NLP), achieving unprecedented progress in language understanding [39, 47], generation [128, 140] and reasoning [20, 58, 87, 107, 115].\\nLeveraging the capabilities of robust LLMs, multimodal large language models (MLLMs) [22, 75, 111, 138], sometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\\nMLLMs show promising ability in multimodal tasks, such as image captioning [66], visual question answering [22, 75], etc.\\nHowever, there is a concerning trend associated with the rapid advancement in MLLMs.\\nThese models exhibit an inclination to generate hallucinations [69, 76, 137], resulting in seemingly plausible yet factually spurious content.\\nThe problem of hallucination originates from LLMs themselves.\\nIn the NLP community, the hallucination problem is empirically categorized into two types [44]: 1) factuality hallucination emphasizes the discrepancy between generated content and verifiable real-world facts, typically manifesting as factual inconsistency or fabrication; 2) faithfulness hallucination refers to the divergence of generated content from user instructions or the context provided by the input, as well as self-consistency within generated content.\\nIn contrast to pure LLMs, research efforts of hallucination in MLLMs mainly focus on the discrepancy between generated text response and provided visual content [69, 76, 137], i.e., cross-modal inconsistency.\\nThis difference suggests that studies in LLMs cannot be seemingly transferred to MLLMs.\\nTherefore, there is a growing need to comprehensively survey recent advancements in MLLMs’ hallucination phenomena to inspire new ideas and foster the field’s development.\\nIn the realm of computer vision, object recognition is the core task, including sub-tasks such as object classification [60], detection [27], and segmentation [37], etc.\\nSimilarly, studies on hallucination in MLLMs primarily focus on object hallucination.\\nIn pre-MLLM era, there is a pioneering work on object hallucination in image captioning [90], evaluating object existence by comparing captions and image content.\\nIn MLLMs, object hallucination has been empirically categorized into three categories: 1) category, which identifies nonexistent or incorrect object categories in the given image; 2) attribute, which emphasizes descriptions of the objects’ attributes, such as color, shape, material, etc; and 3) relation, which assesses the relationships among objects, such as human-object interactions or relative positions.\\nNote that some literature may consider objects counting, objects event, etc., as independent hallucination categories; however, in this work, we include them into attribute category.\\nAs numerous studies exist on the underlying causes of hallucinations in LLMs, the unique challenges posed by cutting-edge MLLMs warrant an in-depth investigation.\\nOur analysis specifically targets the unique origins of hallucinations in MLLMs, spanning a spectrum of contributing factors from data, model, training, to the inference stage.\\nIn addition, we provide a comprehensive overview of benchmarks and metrics designed specifically for evaluating hallucinations in MLLMs.\\nThen, we review and discuss recent works tailored to mitigate the problem of hallucination from the viewpoints of the identified causes.\\nThrough our comprehensive survey, we aim to contribute to advancing the field of MLLMs and offer valuable insights that deepen understanding of the opportunities and challenges associated with hallucinations in MLLMs.\\nThis exploration not only enhances our understanding of the limitations of current MLLMs but also offers essential guidance for future research and the development of more robust and trustworthy MLLMs.\\nComparison with existing surveys.\\nIn pursuit of reliable generative AI, hallucination stands out as a major challenge, leading to a series of survey papers on its recent advancements.\\nFor pure LLMs, there are several surveys [44, 129], describing the landscape of hallucination in LLMs.\\nIn contrast, there are very few surveys on hallucination in the field of MLLMs.\\nTo the best of our knowledge, there is only one concurrent work [76], a short survey on the hallucination problem of LVLMs.\\nHowever, our survey distinguishes itself in terms of both taxonomy and scope.\\nWe present a layered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape of this field.\\nAdditionally, our approach does not limit itself to specific model architectures as prescribed in the work of [76], but rather dissects the causes of hallucinations by tracing back to various affecting factors.\\nWe cover a larger range of literature both in terms of paper number and taxonomy structure.\\nFurthermore, our mitigation strategies are intricately linked to the underlying causes, ensuring a cohesive and targeted approach.\\nOrganization of this survey.\\nIn this paper, we present a comprehensive survey of the latest developments regarding hallucinations in MLLMs.\\nThe survey is organized as follows: We begin by providing sufficient context and defining concepts related to LLMs, MLLMs, hallucination, etc.\\nNext, we delve into an in-depth analysis of the factors contributing to hallucinations in MLLMs.\\nFollowing this, we present a set of metrics and benchmarks employed for evaluating hallucinations in MLLMs.\\nWe then elaborate on a range of approaches designed to mitigate hallucinations in MLLMs.\\nFinally, we delve into the challenges and open questions that frame the current limitations and future prospects of this field, offering insights and delineating potential pathways for forthcoming research.\\n2 DEFINITIONS\\n2.1 Large Language Models\\nBefore moving to multimodal large language models, it is essential to introduce the concept of large language models.\\nTypically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets.\\nProminent examples include GPT-3 [8], PaLM [18], LLaMA [99], and GPT-4 [82].\\nThrough scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning[8], Chain-of-Thought prompting[107] and instruction following[86], among others.\\nThe characteristics and behaviors of LLMs are intricately linked to their training processes.\\nLLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF).\\nBelow, we provide a concise overview of each stage to facilitate comprehension.\\nPre-trianing.\\nPre-training serves as a fundamental phase in the learning process of LLMs [134].\\nDuring this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence.\\nBy undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities.\\nThis pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively.\\nSupervised Fine-Tuning.\\nAlthough pre-training equips LLMs with substantial knowledge and skills, it’s important to acknowledge that its primary focus is on optimizing for completion.\\nConsequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user’s objective of obtaining desired responses.\\nTo address this disparity, the concept of Supervised FineTuning (SFT) [125] has been introduced.\\nSFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs.\\nReinforcement Learning from Human Feedback.\\nAlthough SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.\\nAmong the various methods, Reinforcement Learning from Human Feedback\\nData Quantity Insufficient Data e.g. AMBER [103], LLaVA-RLHF [96]\\nData Quality\\nStatistic Bias\\nVision Model\\n | Language Model | Parametric Knowledge | e.g. | VCD [64], Volcano [63]\\n | --- | --- | --- | ---\\n | Cross-modal Interface | Inferior Alignment | e.g. | HACL [52], Halle-Switch [123]\\n | Sequence Supervision | e.g. MOCHa [5], OPERA [45] |  | \\n | Visual Supervision | e.g. Chen et al. [16] |  | \\n | Human Feedback | e.g. RLHF-V [119] |  | \\n | Lose Visual Attention | e.g. OPERA [45], HaELM [104] |  | \\n | CHAIR | CHAIR [90] |  | \\n | POPE | POPE [69] |  | \\n | LLM-based | e.g. GAVIE [73], HaELM [104], HallusionBench [72] |  | \\n | Others | e.g. Faith-Score [55], AMBER [103] |  | \\n | Discriminative Task | e.g. POPE [69], RAH-Bench [16], FGHE [105] |  | \\n | Generative Task | e.g. GAVIE [73], Faith-Score [55] |  | \\n | Introducing | Negative Data | e.g. | LRV-Instruction [73]\\n | Introducing | Counterfactual Data | e.g. | HalluciDoctor [117]\\n | Mitigating Noises and Errors | e.g. ReCaption [105], EOS [120] |  | \\n | Scale-up Resolution | e.g. LLaVA-1.5 [74], InternVL [14], HallE-Switch [123] |  | \\n | Versatile | Vision Encoders | e.g. | VCoder [49], IVE [38]\\n | Dedicated Module | e.g. HallE-Switch [123] |  | \\n | Auxiliary Supervision\\n |  | Noisy Data | e.g. | HalluciDoctor [117], LLaVA-1.5 [74]\\n |  | Lack of Diversity | e.g. | LRV-Instruction [73], HalluciDoctor [117]\\n |  | Detailed descriptions (open question) | e.g. | Chen et al. [16], EOS [120]\\n |  | Frequent Objects | e.g. | POPE [69]\\n |  | Objects Occurrence | e.g. | LURE [137], VCD [64]\\n |  | Information Loss | e.g. | HallusionBench [72], AMBER [103]\\n |  | Feature Bias | e.g. | Tong et al. [98]\\n\\nfrom Data (§3.1) Hallucination from Model (§3.2)\\nHallucination Causes (§3)\\nHallucination Metrics and Benchmarks(§4)\\nHallucination from Training (§3.3) Hallucination from Inference (§3.4)\\nHallucination Metrics\\nHallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)\\n | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n\\nHallucination Mitigation (§5)\\nReinforcement Learning\\nGeneration Intervention\\nPost-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]\\nMitigating Inference-related Hallucinations (§5.4)\\nFig. 1.\\nThe main content flow and categorization of this survey.\\n(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.\\n | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n\\nFig. 2.\\nPopular architecture of multimodal large language model.\\n2.2 Multimodal Large Language Models\\nMLLMs [22, 75, 111, 138] typically refers to a series of models that enable LLMs to perceive and comprehend data from various modalities.\\nAmong them, vision+LLM is particularly prominent, owing to the extensive research on vision-language models (VLMs) [51, 88, 116] prior to LLMs.\\nAs a result, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models (LVLMs).\\nThe goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\" the world via images or videos.\\nCombined with strong reasoning and language generation abilities, MLLMs trigger a series of downstream tasks in multimodal domains, such as image/video captioning and visual question answering.\\nAdditionally, MLLMs serve as the foundation for applications in other fields, such as AI assistants, embodied agents, and robotics.\\nIntegrating the two modalities of vision and language involves primarily two types of approaches.\\nThe first line of work is built upon off-the-shelf pre-trained uni-modal models.\\nSpecifically, these MLLMs usually incorporate a learnable interface between pre-trained visual encoders and LLMs.\\nThe interface extracts and integrates information from visual modalities.\\nSuch interfaces can be further categorized into 1) learnable query-based and 2) projection layer based.\\nLearnable querybased methods, exemplified by Q-Former [66], as used in MiniGPT-4 [138] and Instruct-BLIP [22], utilize a set of learnable query tokens to capture visual signals via cross-attention.\\nProjection layerbased methods, as widely applied in LLaVA [75], Shikra [12], etc., involve training a linear projection layer or a Multi-Layer Perceptron (MLP) module to transform extracted visual features.\\nBoth types of interfaces aim to transform pre-trained visual features into the input space of pre-trained LLMs.\\nAnother line of work is represented by Fuyu-8B [4] and Gemini [97].\\nUnlike previous methods that leverage pre-trained uni-modal models, these works employ end-to-end training from scratch.\\nTaking Fuyu-8B as an example, it does not employ any pre-trained vision encoder.\\nInstead, it directly inputs image patches and employs a linear projection to transform the raw pixels of each patch into embeddings.\\nThe abstracted pipeline is depicted in Fig. 2.\\nMLLMs take input from both visual and textual modalities, learning from multimodal instructions and responses, which leads to remarkable performance across various multimodal tasks.\\nRegarding the training of MLLMs, we provide a concise overview of the training process for interface-based MLLMs.\\nGiven that end-to-end models are closed-source, the training details are unknown.\\nTypically, the training of interface-based MLLMs consists of two stages: 1) pre-training, 2) instruction tuning.\\nPlease describe this image in detail.\\nThe image shows a group of people standing in a park.\\nThere is a large tree in the background with pink blossoms.\\nThe people are all wearing different clothing, including a man in a brown jacket and a woman in a white shirt and jeans.\\nThere are also some people in the background who are not in the foreground of the image.\\nThe people are standing in a grassy area and there are some benches and a fence in the background.\\nThe sky is clear and there are some clouds in the distance.\\nWhat is the girl in the picture doing?\\nThe girl in the picture is standing with her arms outstretched, as if she is trying to catch something.\\nIt is not clear what she is trying to catch, but it could be a kite or a frisbee.\\nThe other people in the picture are standing around her, watching what she is doing.\\nIt appears that they are all having a good time together in the park.\\nCategory Hallucination\\nAttribute Hallucination\\nRelation Hallucination\\nFig. 3.\\nThree types of typical hallucination.\\nPre-training.\\nGiven that models from each modality are pre-trained on their respective data, the objective of this pre-training phase is to achieve cross-modal feature alignment.\\nDuring training, both the pre-trained visual encoder and LLM remain frozen, with only the cross-modal interface being trained.\\nSimilar to traditional VLMs training, as exemplified by CLIP [88], web-scale imagetext pairs [92] are utilized for training.\\nGiven that the final output is at the LLM side, the most widely used loss function in this stage is the text generation loss, typically cross-entropy loss, which aligns with the pre-training of LLMs.\\nCertain studies (e.g., [22, 66]) explore the incorporation of contrastive loss and image-text matching loss to further enhance alignment.\\nAfter training, the interface module maps the visual features into the input embedding space of the LLM.\\nInstruction Tuning.\\nSimilar to LLMs, after pre-training, the current model still lacks instruction following ability in the multimodal context.\\nDuring the instruction tuning stage, both machinegenerated datasets [75] and human-annotated QA datasets [48, 59, 80] are utilized to enhance the model’s ability to comprehend and follow multimodal instructions.\\nUnlike pre-training data, the format and quality of instruction tuning data significantly impact the model’s performance.\\nIt is usually in the format of visual content - instruction - response.\\nEmpirical studies also demonstrate that high-quality data significantly enhances the performance of MLLMs.\\nDuring this stage, there are various options for training, such as fine-tuning LLM parameters in full [75], or using techniques like LoRA [41] to tune specific LLM parameters.\\n2.3 Hallucinations in Multimodal Large Language Models\\nHallucination of MLLM generally refers to the phenomenon where the generated text response does not align with the corresponding visual content.\\nState-of-the-art studies in this field primarily focus on object hallucination, given that objects are central to research in computer vision and multimodal contexts.\\nRegarding inconsistency, two typical failure modes are: 1) missing objects, and 2) describing objects that are not present in the image or with incorrect statements.\\nEmpirically, the second mode has been shown to be less preferable to humans.\\nFor example, the LSMDC challenge [91] shows that correctness is more important to human judges than specificity.\\nIn contrast, the coverage of objects is less perceptible to humans.\\nThus, object coverage is not a primary focus in studies of object hallucination.\\nEmpirically, object hallucination can be categorized into three types: object category, object attribute, and object relation.\\nAn example of the three types of hallucination is shown in Fig. 3.\\n• Category.\\nMLLMs identify nonexistent object categories or incorrect categories in the given image.\\nFor example, in Fig. 3, \"some benches and a fence\", \"some clouds\", described in the text response do not exist in the given image.\\n• Attribute.\\nThe object categories identified by MLLMs are accurate, while the descriptions of these objects’ attributes (such as color, shape, material, content, counting, action, etc.) are wrong.\\nIn Fig. 3, \"pink blossoms\" is hallucinated by the MLLM as the color is inaccurate.\\n• Relation.\\nAll objects and their attributes are described correctly, but the relationships among them (such as human-object interactions or relative positions) do not align with the actual image content.\\nIn Fig. 3, \"standing around her, watching\" is a typical example of relation hallucination, as the objects are presented in the image but the relation is inaccurate.\\nIt’s worth noting that some literature may categorize objects counting, objects event, etc., as independent hallucination categories.\\nIn this work, we classify them under the attribute category.\\nThe definition of hallucination types aligns well with the domain of compositional generalization [79, 121] of VLMs, which investigates visio-linguistic generalization and reasoning abilities.\\n3 HALLUCINATION CAUSES\\nHallucinations have multifaceted origins, spanning the entire spectrum of MLLMs’ capability acquisition process.\\nIn this section, we delve into the root causes of hallucinations in MLLMs, primarily categorized into four aspects: Data, Model, Training, and Inference.\\n3.1 Data\\nData stands as the bedrock for MLLMs, enabling them to gain cross-modal understanding and instruction-following capabilities.\\nHowever, it can inadvertently become the source of MLLM hallucinations.\\nThis mainly manifests in three aspects: quantity, quality, and statistical bias.\\n3.1.1 Quantity\\nDeep learning models are data-hungry, especially large models like MLLMs.\\nThe amount of data plays an important role in building robust and reliable MLLMs.\\nCurrently, image-text pair datasets [92] and visual QA [48, 80] data are used for training MLLMs.\\nAlthough these datasets are usually larger than typical datasets in computer vision, they are still far less abundant than the text-only data used for training LLMs in terms of quantity.\\nInsufficient data could potentially lead to problematic cross-modal alignment, resulting in hallucinations [96, 103].\\n3.1.2 Quality\\nGiven the increasing demand for large-scale training data, heuristic data collection methods are employed to efficiently gather vast volumes of data.\\nWhile these methods provide extensive data, they offer no guarantee of quality, thereby increasing the risk of hallucinations.\\nData quality relevant to hallucinations can be further categorized into the following three facets.\\n• Noisy data.\\nAs mentioned in the definition section, training MLLMs involves two stages.\\nThe pre-training stage employs image-text pairs crawled from the web, which contain inaccurate, misaligned, or corrupted data samples.\\nThe noisy data would limit the cross-modal feature alignment [117, 120], which serves as the foundation of MLLMs.\\nAs for the instruction tuning data, prevalent methods, such as LLaVA [75], utilize the advanced GPT-4 [82] model to generate instructions.\\nHowever, ChatGPT is a language model that cannot interpret visual content, leading to the risk of noisy data.\\nMoreover, language models themselves suffer from the issue of hallucination [44], further increasing the risk.\\nLLaVA-1.5 [74] adds human annotated QA data into instruction following and shows improved results, revealing the effect of noisy data.\\n• Lack of diversity.\\nRecent works [73, 117] reveal that the diversity of data also plays a crucial role.\\nFor the data used in the two training stages, instruction tuning data are more likely to have this issue since it is usually in a relatively small amount.\\nOne prominent property is that most instruction following data samples are composed of conversations regarding the image content.\\nWe regard this type of data as positive instruction, as it always faithfully reflects the image content.\\nIn contrast, negative instruction data [73] and reject answering responses [11] are rare in the datasets.\\nGiven such training data, one potential drawback observed by recent studies [69, 73] is that current models tend to answer \"Yes\" for any instructions presented to the model, even when a proper answer should be \"No\", leading to hallucination.\\nThis phenomenon indicates the effect of data diversity.\\n• Detailed descriptions (open question) The impact of the level of detail in textual descriptions on this matter remains an open question.\\nAs discussed in Sec. 2.2, the texts in pre-training data, such as LAION [92], usually describe the salient objects’ overall content.\\nWhile the texts in the instructing tuning stage, such as LLaVA-150k [75], consist of more detailed descriptions.\\nThis LLaVA-150k dataset is generated by GPT-4 based on objects recognized by vision models.\\nOne recent work [16] argues that within the training data, detailed descriptions related to object position, attributes, and non-salient objects are usually absent.\\nThis property results in incomplete cross-modal alignment and deprives the model of grounding ability [62, 126].\\nHowever, another work [120] hypothesizes that the text descriptions in the instruction tuning data contain too much details, exceeding the perception limit of MLLMs.\\nWhen trained with such detailed data, in an attempt to fit the detail level and length distribution of ground truth captions, the model may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThe detail level of the training data remains an open question.\\n3.1.3 Statistic bias.\\nNeural networks, especially large language models, possess an intrinsic tendency to memorize training data, as noted in [23].\\nThe nous (e.g., objects) distribution in the training dataset has strong effects on the behavior of the model.\\nFrequently appeared objects and object co-occurrence are two prominent types of statistical bias, as discussed in [69, 90, 137].\\nFor example, ‘person’ might be one of the most frequently appearing objects in the training data.\\nDuring inference, even if the given image does not contain a person, the model still tends to predict the presence of a person.\\nOn the other hand, object co-occurrence refers to the phenomenon that the model will remember which two objects usually ‘go together’ [90].\\nFor instance, given an image of a kitchen with a refrigerator, MLLMs are prone to answer ‘Yes’ when asked about a microwave, as refrigerators and microwaves frequently appear together in kitchen scenes.\\nBias exists in most datasets.\\nIncreasing the scale of data may alleviate the effect, but cannot fully resolve it, given the long-tail distribution of the real world.\\n3.2 Model\\nCurrently, the architecture of popular MLLMs is composed of several components, usually including pre-trained vision model, pre-trained LLM, and alignment module as we discussed above.\\nSince these models are connected together, instead of end-to-end training from scratch, the error of each module can be accumulated.\\nInferior and problematic output from each module may lead to hallucinations.\\n• Weak vision model.\\nAs mentioned in related works [31, 90, 103], a primary potential reason for hallucination is a weak vision model, which can lead to misclassification or misinterpretation of visual concepts.\\nEven themost powerful visionmodelmay still experience information loss during the encoding process.\\nWeak vision model implies weak perception, which fundamentally undermines the multimodal understanding.\\n• Language model prior.\\nThe modern architecture of MLLMs is imbalanced.\\nUsually, the language model is much larger and stronger than the vision model, leading to a tendency to prioritize language-based information [31, 63, 64, 73, 90].\\nA typical phenomenon is that the knowledge entailed in the language model, also termed as parametric knowledge, can override the visual content.\\nFor example, given an image showing a red banana, which is counter-intuitive in the real world, an MLLM may still respond with \"yellow banana\", as \"banana is yellow\" is a deep-rooted knowledge in the LLM.\\nSuch language/knowledge prior makes the model overlook the visual content and response with hallucination.\\n• Weak alignment interface.\\nThe alignment interface plays an essential role in MLLMs, as it serves as the bridge between the two modalities.\\nA weak alignment interface can easily cause hallucinations.\\nOne potential cause of a weak alignment interface is data, as discussed in earlier sections.\\nApart from that, the interface architecture itself and training loss design also matter [52, 77, 123].\\nRecent work [52] argues that the LLaVA-like linear projection interface preserves most of the information, but lacks supervision on the projected feature.\\nVisualization in [52] reveals that the features after the projection layer remain distinct from the language embeddings.\\nThe distribution gap causes trouble in cross-modal interaction, leading to hallucination.\\nOn the other hand, Q-former-like [66] architecture has diverse supervision on the extracted visual feature, aligning it to the language embedding space.\\nHowever, the use of learnable queries inevitably results in the loss of fine-grained visual information.\\n3.3 Training\\nThe training objective of MLLMs is basically the same as LLMs, i.e, auto-regressive next token prediction loss.\\nThis loss is straightforward yet effective and easy to scale up, showing promising performance in language modeling.\\nHowever, some studies in the field of MLLMs have suggested that the next-token prediction loss might not be suitable for learning visual content due to its complex spatial structure [5, 16].\\nAdditionally, the loss optimizes at the token level, while lacking supervision at the sequence level [5].\\nAnother perspective is that, unlike training LLMs, the RLHF stage is absent in training procedure of MLLMs [96, 119], becoming a potential cause of hallucination.\\n3.4 Inference\\nAs for inference, some works also argues a potential issue in the auto-regressive generation.\\nDuring generation, as the sequence length grows, the self-attention will focus more on the previously generated text tokens, i.e., the attention on the visual content is diluted [45, 102–104].\\nThrough visualizing the attention map during generation [45, 104], it can be observed that the generated content focuses more on previous special tokens, such as punctuation, rather than visual content tokens.\\nThe issue of ’losing attention’ would also lead to the model’s output response being irrelevant to the visual content.\\n4 HALLUCINATION METRICS AND BENCHMARKS\\nIn this section, we present a comprehensive overview of existing hallucination metrics and benchmarks, which are designed to assess the extent of hallucinations generated by existing cutting-edge MLLMs.\\nCurrently, the primary focus of these benchmarks is on evaluating the object hallucination of MLLM-generated content.\\nTab.\\n1 illustrates a summary of related benchmarks.\\nCHAIR [90].\\nAs one of the early works, the metric of CHAIR was proposed to evaluate object hallucination in the traditional image captioning task.\\nThis is achieved by computing what proportion of words generated are actually in the image according to the ground truth sentences and object segmentations.\\nThe computation of the CHAIR metric is straightforward and easy to understand.\\nThe metric has two variants: per-instance (denoted as CHAIR𝑖) and per-sentence\\nTable 1.\\nSummary of most relevant benchmarks and metrics of object hallucination in MLLMs.\\nThe order is based on chronological order on arxiv.\\nIn the metric column, Acc/P/R/F1 denotes Accuracy/Precision/Recall/F1Score.\\n | Benchmark | Venue Underlying Data Source | Size | Task | Type | Metric | Hallucination | Type\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | CHAIR [90] | EMNLP’18 MSCOCO [70] | 5,000 | Gen | CHAIR | ✓ | ✗ | ✗ | ✗\\n | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | Category Attribute Relation Others\\n | POPE [69] | EMNLP’23 MSCOCO [70] | 3,000 | Dis | Acc/P/R/F1 | ✓ | ✗ | ✗ | ✗\\n | MME [113] | arXiv’23 Jun MSCOCO [70] | 1457 | Dis | Acc/Score | ✓ | ✓ | ✗ | ✓\\n | CIEM [42] | NeurIPS-W’23 MSCOCO [70] | 78120 | Dis | Acc | ✓ | ✗ | ✗ | ✗\\n | M-HalDetect [32] | arXiv’23 Aug. MSCOCO [70] | 4,000 | Dis | Reward Model Score | ✓ | ✗ | ✗ | ✗\\n | MMHal-Bench [96] arXiv’23 Sep. Open-Images [61] |  | 96 | Gen | LLM Assessment | ✓ | ✗ | ✗ | ✓\\n | GAVIE [73] | ICLR’24 Visual-Genome [59] | 1,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | NOPE [77] | arXiv’23 Oct. Open-Images [61] | 36,000 | Dis | Acc/METEOR [3] | ✓ | ✗ | ✗ | ✗\\n | HaELM [104] | arXiv’23 Oct. MSCOCO [70] | 5,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | FaithScore [55] | arXiv’23 Nov. MSCOCO [70] | 2,000 | Gen | FaithScore | ✓ | ✓ | ✓ | Obj. Counting\\n | Bingo [21] | arXiv’23 Nov. Unknown | 370 | Gen | Human Assessment | ✗ | ✗ | ✗ | Model Bias\\n | AMBER [103] | arXiv’23 Nov. Web | 15,202 | Dis & Gen | AMBER Score | ✓ | ✓ | ✓ | ✗\\n | RAH-Bench [16] | arXiv’23 Nov. MSCOCO [70] | 3,000 | Dis | False Positive Rate | ✓ | ✓ | ✓ | ✗\\n | HallusionBench [72] | CVPR’24 Unknown | 1,129 | Gen | LLM Assessment | ✗ | ✗ | ✗ | Model Diagnose\\n | CCEval [123] | arXiv’23 Dec. Visual-Genome [59] | 100 | Gen | LLM-based CHAIR | ✓ | ✗ | ✗ | ✗\\n | MERLIM [100] | arXiv’23 Dec. MSCOCO [70] | 31,373 | Dis | Accuracy | ✓ | ✗ | ✓ | Obj. Counting\\n | FGHE [105] | arXiv’23 Dec. MSCOCO [70] | 200 | Dis | Acc/P/R/F | ✓ | ✓ | ✓ | Obj. Behavior\\n | MOCHa [5] | arXiv’23 Dec. Synthetic | 2,000 | Gen | OpenCHAIR [5] | ✓ | ✓ | ✗ | ✗\\n | CorrelationQA [35] | arXiv’24 Feb. Synthetic | 7,308 | Dis | Acc/AccDrop | ✗ | ✗ | ✗ | Model Bias\\n | VQAv2-IDK [11] | arXiv’24 Feb. VQAv2 [30] | 6,624 | Dis | Acc | ✗ | ✗ | ✗ | IK [11]\\n | MHaluBench [13] | arXiv’24 Feb. MSCOCO [70] | 1,860 | Gen | Acc/P/R/F | ✓ | ✓ | ✗ | T2I\\n | VHTest [46] | arXiv’24 Feb. MSCOCO [70] | 1,200 | Dis & Gen | Acc | ✓ | ✓ | ✗ | ✓\\n | Hal-Eavl [53] | arXiv’24 Feb. MSCOCO [70] & LAION [92] | 10,000 Dis & Gen |  | Acc/P/R/F & LLM Assessment | ✓ | ✓ | ✓ | Obj. Event\\n\\n(denoted as CHAIR𝑠):\\n | CHAIR𝑖 = | |{hallucinated objects}| |{all objects mentioned}| ,\\n | CHAIR𝑠 = | |{sentences with hallucinated object}| |{all sentences}|\\n\\nn the paper of CHAIR [90], the range of objects is restricted to the 80 MSCOCO objects. \\nSentence tokenization and synonyms mapping are applied to determine whether a generated sentence contains hallucinated objects.\\n Ground-truth caption and object segmentations both serve as groundtruth objects in the computation\\n. In the MLLM era, this metric is still widely used for assessing the response of MLLM\\nPOPE [69].\\nWhen used in MLLMs, the work of [69] argues that the CHAIR metric can be affected by the instruction designs and the length of generated captions.\\nTherefore, it proposes a new evaluation metric as well as a benchmark, called Pooling-based Object Probing Evaluation (POPE).\\nThe basic idea is to convert the evaluation of hallucination into a binary classification task by prompting MLLMs with simple Yes-or-No short questions about the probing objects (e.g., Is there a car in the image?) Compared to CHAIR, POPE offers increased stability and flexibility.\\nBased on this metric design, it further proposed an evaluation benchmark, drawing 500 images from the MSCOCO dataset.\\nThe questions in the benchmark consist of both positive and negative questions.\\nThe positive questions are formed based on the ground-truth objects, while the negative questions are built from sampling nonexistent objects.\\nThe benchmark is divided into three subsets according to different negative sampling strategy: random, popular, and adversarial.\\nPopular and adversarial sampling are specifically designed to assess frequently appeared objects and object co-occurrence.\\nAs an early representative work, POPE serves as a foundation of object hallucination evaluation.\\nMME [113].\\nMME is a comprehensive evaluation benchmark for MLLMs.\\nIt covers the examination of perception and cognition abilities, encompassing 14 subtasks.\\nRegarding object hallucination, there are four popular object related subtasks in its perception evaluation, including object existence, count, position, color.\\nSimilar to POPE, these tasks are formulated as Yes-or-No tasks.\\nCIEM [42] CIEM is a benchmark to evaluate hallucination of MLLMs.\\nUnlike previous works utilize human annotated objects, CIEM is generated using an automatic pipeline.\\nThe pipeline takes the text description of a specific image as input and utilize advanced LLMs to generate QA pairs.\\nAlthough the LLM-based data generation pipeline is not completely reliable, empirical result shows that the generated data has low error rate, around 5%.\\nMMHal-Bench [96] Comprising 96 image-question pairs, ranging in 8 question categories × 12 object topics, MMHal-Bench is a dedicated benchmark for evaluating hallucination in MLLMs.\\nThe 8 question categories cover various types of hallucination, including object attributes, counting, spatial relations, etc.\\nDuring the evaluation of MMHal-Bench, the GPT-4 model is employed to analyze and rate the responses.\\nGAVIE [73]GPT4-Assisted Visual Instruction Evaluation (GAVIE) is proposed to assess the LMM output in two different aspects: Relevancy to evaluate the instruction-following performance and Accuracy to measure the visual hallucination in the LMM output.\\nIt comprises a benchmark with 1,000 samples and an evaluation approach.\\nGAVIE evaluates the output of MLLMs in an open-ended manner and does not require human-annotated ground-truth answers.\\nThe core idea is to ask the advanced GPT-4 to work as a smart teacher and score the answer by taking image content, human instruction, and model response as input.\\nNOPE [77] This paper proposes to establish a distinction between object hallucination and incorrectness.\\na) Object hallucination refers to a phenomenon in VQA where a VL model’s response includes a non-existent object, despite the ground truth answer being a negative indefinite pronoun (e.g., \"none\", \"no one\", etc).\\nThis is denoted as NegP.\\nb) Incorrectness occurs when a VL model fails to accurately respond to a question with a ground truth answer that is anything other than NegP, denoted as Others.\\nThis paper argues that the existing VQA datasets have a significantly imbalanced distribution, containing too littleNegP data.\\nTherefore, NOPE (Negative Object Presence Evaluation) is proposed in this paper to complement the absent NegP data.\\nDuring evaluation, traditional metrics, including Accuracy and METEOR, are employed.\\nHaELM [104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4 models to assess the quality of theMLLM response.\\nIn contrast, the work of Hallucination Evaluation based on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination detection.\\nIt collects a set of hallucination data generated by a wide range of MLLMs, simulates data using ChatGPT, and trains an LLM based on LLaMA [99].\\nAfter that, the HaELM model becomes proficient in hallucination evaluation, leveraging reference descriptions of images as the basis of assessment.\\nFaithScore [55] Considering the natural forms of interaction between humans and MLLMs, FaithScore aims to evaluate free-form responses to open-ended questions.\\nDifferent from LLM-based overall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate, and analyze the elements in detail.\\nSpecifically, it includes three steps: descriptive sub-sentence identification, atomic fact generation, and fact verification.\\nThe evaluation metric involves finegrained object hallucination categories, including entity, count, color, relation, and other attributes.\\nThe final computation of FaithScore is the ratio of hallucinated content.\\nBingo [21] Bingo (Bias and Interference Challenges in Visual Language Models) is a benchmark specifically designed for assessing and analyzing the limitations of current popular MLLMs, such as GPT-4V [83].\\nIt comprises 190 failure instances, along with 131 success instances as a comparison.\\nThis benchmark reveals that state-of-the-art MLLMs show the phenomenon of bias and interference.\\nBias refers to the model’s susceptibility to generating hallucinatory outputs on specific types of examples, such as OCR bias, region bias, etc.\\nInterference refers to scenarios in which the judgment model can be disrupted, making it more susceptible to hallucination.\\nDue to the small amount of data in this benchmark, the assessment and analysis are mostly conducted by humans.\\nAMBER [103] Upon the application and evaluation of MLLMs, the tasks can be roughly divided into generative tasks and discriminative tasks.\\nFor generative tasks, this paper argues that most existing works rely on additional LLMs, suffering from computational cost.\\nAs for discriminative tasks, the most popular evaluation suite is POPE [69].\\nHowever, POPE lacks fine-grained hallucination types such as attributes and relations.\\nAMBER (An LLM-free Multi-dimensional Benchmark) is proposed to support the evaluation of generative tasks and discriminative tasks, including object existence hallucination, attribute hallucination, and relation hallucination.\\nIt further combines the CHAIR [90] metric in generative tasks and F1 in discriminative tasks to form the AMBER Score as follows:\\nAMBER Score = 𝐴𝑣𝑔(1 − CHAIR, F1).\\n(1)\\nRAH-Bench [16] Relation-Associated Hallucination Benchmark (RAH-Bench) can be regarded as an upgraded version of POPE, containing 3,000 yes-or-no questions with their corresponding images.\\nDifferent from POPE, RAH-Bench further divides the negative questions into three subsets.\\nEach subset contains 500 questions with misleading statements in the different aspects, including:\\n1) categorical hallucination, 2) attribute hallucination, 3) relation hallucination.\\nHallusionBench [72] To diagnose and analyze the potential failure modes of MLLMs, HallusionBench evaluates hallucination from a different perspective.\\nIt consists of 455 visual-question control pairs, with 346 different figures and a total of 1129 questions covering diverse topics and formats.\\nThe questions are divided into two categories: Visual Dependent and Visual Supplement.\\nThe Visual Dependent questions are defined as questions that do not have an affirmative answer without the visual context.\\nThis setting aims to evaluate visual commonsense knowledge and visual reasoning skills.\\nThe Visual Supplement questions can be answered without the visual input; the visual component merely provides supplemental information or corrections.\\nThis setting is designed to evaluate visual reasoning ability and the balance between parametric memory (language prior) and image context.\\nThis division provides a new perspective for understanding and diagnosing MLLMs.\\nCCEval [123] CCEval focuses on the hallucination evaluation of detailed captions.\\nTraditional caption-based evaluation benchmarks and metrics, like CHAIR, are known to favor short captions.\\nHowever, short captions often lack detail and contain less information.\\nTo address this issue, CCEval randomly samples 100 images from Visual Genome to form a benchmark.\\nIn evaluation, GPT-4 is utilized to parse the captions generated by MLLMs and extract objects.\\nAdditionally, this work introduces the \"coverage\" metric on top of CHAIR to ensure that the captions are detailed enough.\\nThis metric computes the ratio of objects in the caption that match the ground truth to the total number of ground truth objects.\\nIt additionally records the average number of objects as well as the average length of captions as auxiliary metric.\\nCompared with CHAIR, CCEval employs more diverse objects, as reflected in the source of ground truth (Visual Genome vs. COCO) and caption parsing (GPT-4 vs. rule-based tool).\\nMERLIM [100] MERLIM (Multi-modal Evaluation benchmaRk for Large Image-language Models) is a test-bed aimed at empirically evaluating MLLMs on core computer vision tasks, including object recognition, instance counting, and identifying object-to-object relationships.\\nMERLIM contains over 279K image-question pairs, and has a strong focus on detecting cross-modal hallucinations.\\nInterestingly, when organizing the data, a set of edited images is intentionally added.\\nBased on the original image, an inpainting strategy is employed to remove one object instance in the image.\\nWith this original-edited image pair, one can compare the output of the target MLLM and identify the hallucinated objects that lack visual grounding.\\nFGHE [105] Fine-Grained Object Hallucination Evaluation (FGHE) follows a binary classification approach similar to POPE to evaluate MLLMs.\\nHowever, unlike POPE, FGHE requires a different set of binary questions to measure fine-grained hallucination.\\nThe FGHE dataset consists of 50 images and 200 binary questions divided into three categories: (a) multiple-object questions, which verify the relationships between multiple objects in the image; (b) attribute questions, which verify attributes of objects in the image; and (c) behavior questions, which verify behaviors or objects in the image.\\nThe questions are manually defined by human annotators on a subset of 50 images from the validation set of the MSCOCO dataset.\\nSimilar to POPE, the Accuracy, Precision, Recall, and F1 score are employed as the evaluation metrics.\\nOpenCHAIR [5] The traditional CHAIR metric relies on the closed list of 80 objects in the MS-COCO dataset, limiting its application.\\nTo measure object hallucination in the open-vocabulary settings, OpenCHAIR expands CHAIR by relaxing the strong reliance on the closed vocabulary.\\nThe ’open-vocabulary’ manifests in two ways.\\nFirstly, when building the benchmark, it organizes a dataset consisting of synthetic images with corresponding captions, which include diverse, openvocabulary objects using a text-to-image diffusion model.\\nSecondly, during computing the metric, CHAIR checks if words or their synonyms (as given by fixed vocabulary lists) are found in groundtruth annotations.\\nIn contrast, OpenCHAIR extracts concrete objects from a predicted caption and identifies hallucinated objects from this list by querying an LLM.\\nSimilar to CHAIR, the final metric computation is based on the hallucination rate.\\nHal-Eval [53] The work of Hal-Eval [53] identifies another type of object hallucination: event hallucination.\\nThis type of hallucination fabricates a fictional target and constructing an entire narrative around it, including its attributes, relationships, and actions.\\nThis effort further completes the definition of hallucination types.\\nIn addition, this work proposes an evaluation benchmark, which encompasses both discriminative and generative evaluation methods.\\nThis is achieved by collecting two evaluation subsets, each tailored to the discriminative and generative evaluation methods, respectively.\\nCorrelationQA [35] CorrelationQA is a dedicated benchmark to quantify the effect of hallucination induced by the spurious visual input.\\nThis type of hallucination usually occurs when providing the MLLM with images that are highly relevant but inconsistent with the answers, causing MLLMs to suffer from hallucination.\\nSuch visual inputs are defined as ’spurious visual inputs’.\\nThis benchmark reveals that most of mainstream MLLMs, including GPT-4V, suffer from hallucination when presented with such spurious visual inputs.\\nThis phenomenon indicates that an image can induce MLLMs to instinctively focus on visual content, resulting in responses that are predominantly based on visual information without proper reasoning and thinking.\\nVQAv2-IDK [11] It has been widely discussed that in the binary QA scenario, MLLMs generally have a bias on answering ’Yes-or-No,’ leading to hallucination.\\nIn a more detailed question and answer scenario, MLLMs generally tend to respond to the user’s question plausibly, even if the desired answer is ’I don’t know’.\\nThe concept is defined as ’I Know (IK)’ hallucination in the work of [11].\\nAccordingly, a new benchmark, VQAv2-IDK, is proposed to specifically evaluate this type of hallucination.\\nVQAv2-IDK is a subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators.\\nIn this benchmark, ’I Know (IK)’ hallucination has been further categorized into four types:\\n• Unanswerable: no one can know.\\n• Don’t know: human may not know, but robot might.\\n• False questions: refers non-existing.\\n• Not sure: ambiguous to answer.\\nThis benchmark opens a new track for the study of hallucination in MLLMs.\\nMHaluBench [13] This benchmark does not aim to evaluate the MLLMs themselves.\\nInstead, it is intentionally designed to evaluate the hallucination detection tools of MLLMs, i.e., judge whether a tool can successfully detect the hallucination produced by an MLLM.\\nThus, the benchmark consists of hallucinatory examples.\\nSpecifically, the benchmark unifies image-to-text tasks and the text-to-image tasks into one evaluation suite: cross-modal consistency checking.\\nThe hallucinatory examples are generated using leading MLLMs and image generation models, such as LLaVA [75], MiniGPT-4 [138], DALL-E2 [89], and DALL-E3 [6].\\nDuring evaluation, the benchmark can be used to compare different hallucination detection methods based on their performance.\\nSo far, there are not many dedicated hallucination detection methods.\\nThis work serves as a basis for this direction.\\nVHTest [46] VHTest categorizes visual properties of objects in an image into 1) individual properties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which emerge from comparisons across multiple objects, such as relative size, relative position, and counting.\\nBased on such categorization, the authors further defined 8 visual hallucination modes, providing a very detailed evaluation of hallucination in MLLMs.\\nFurthermore, the collected 1,200 evaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no question\" (YNQ).\\nSuch design enables this benchmark to evaluate both generative and discriminative tasks.\\nComparison of mainstream models We compare the mainstream MLLMs on some representative benchmarks, providing a holistic overview of their performance from different dimensions.\\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks.\\nWe observe that the MLLMs’ performance is not always consistent across different benchmarks.\\nIt indicates that different benchmarks have different evaluation dimensions and emphases.\\nTable 2.\\nComparison of mainstreamMLLMs on generative benchmarks.\\nThe numbers come from the original papers of these benchmarks.\\n | Model | LLM Size CHAIR (On AMBER) ↓ | AMBER Score ↑ | HallusionBench All-Acc ↑ | FaithScore (LLaVA-1k) ↑ | FaithScore (COCO-Cap) ↑ | Hal-Eval In-domain Gen. Acc ↑ | Hal-Eval Out-of-domain Gen. Acc ↑\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B 23.1 | 54.1 | 43.93 | 0.7167 | 0.8546 | 27.3 | 29.5\\n | Multimodal-GPT [28] | 7B - | - | - | 0.5335 | 0.5440 | - | -\\n | InstructBLIP [22] | 7B 10.3 | 86.2 | 45.26 | 0.8091 | 0.9392 | 35.5 | 41.3\\n | GPT-4V [83] | - 4.3 | 92.7 | 65.28 | - | - | - | -\\n | LLaVA (7B) [75] | 7B 13.5 | 69.3 | - | - | - | 23.3 | 26.3\\n | LLaVA (13B) [75] | 13B - | - | - | 0.8360 | 0.8729 | - | -\\n | MiniGPT-4 (7B) [138] | 7B - | - | 35.78 | 0.5713 | 0.6359 | 61.4 | 50.1\\n | MiniGPT-4 (13B) [138] | 13B 15.9 | 76.7 | - | - | - | - | -\\n | mPLUG-Owl2 [112] | 7B 10.6 | 84.0 | 47.30 | - | - | - | -\\n | LLaVA-1.5 (7B) [74] | 7B 8.6 | 82.9 | - | - | - | 44.6 | 46.4\\n | LLaVA-1.5 (13B) [74] | 13B - | - | 46.94 | 0.8566 | 0.9425 | - | -\\n | CogVLM [106] | 7B 7.9 | 86.1 | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B - | - | 39.15 | - | - | - | -\\n | Open-Flamingo [1] | 9B - | - | 38.44 | - | - | - | -\\n | LRV-Instruction [73] | - - | - | 42.78 | - | - | - | -\\n\\n5 HALLUCINATION MITIGATION\\nIn this section, we present a comprehensive review of contemporary methods aimed at mitigating hallucinations in MLLMs.\\nBased on the properties and perspectives of these methods, we systematically categorize them into four groups.\\nSpecifically, we investigate approaches addressing hallucination from Data, Model, Training, and Inference.\\nTable 3.\\nComparison of mainstream MLLMs on discriminative benchmarks.\\nThe numbers come from the original papers of these benchmarks.\\n | Model | LLM Size | MME Existence Score ↑ | MME Count Score ↑ | MME Position Score ↑ | MME Color Score ↑ | POPE Random F1-Score ↑ | POPE Random F1-Score ↑ | POPE Adversarial F1-Score ↑ | RAH-Bench F1 Score ↑ | AMBER Dis. F1-Score ↑ | AMBER Score ↑ | Hal-Eval In-domain Event. F1 ↑ | Hal-Eval Out-of-domain Event. F1 ↑\\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B | 120.00 | 50.00 | 50.00 | 55.00 | 68.06 | 66.79 | 66.82 | 69.3 | 31.2 | 54.1 | 47 | 46.6\\n | ImageBind-LLM [34] | 7B | 128.33 | 60.00 | 46.67 | 73.33 | - | - | - | - | - | - | - | -\\n | InstructBLIP [22] (7B) | 7B | - | - | - | - | - | - | - | 89.1 | 82.6 | 86.2 | 66.2 | 66.6\\n | InstructBLIP [22] (13B) | 13B | 185.00 | 143.33 | 66.67 | 153.33 | 89.29 | 83.45 | 78.45 | 84.7 | - | - | - | -\\n | VisualGLM-6B [25] | 6B | 85.00 | 50.00 | 48.33 | 55.00 | - | - | - | - | - | - | - | -\\n | Multimodal-GPT [28] | 7B | 61.67 | 55.00 | 58.33 | 68.33 | 66.68 | 66.67 | 66.67 | - | - | - | - | -\\n | PandaGPT [95] | 7B | 70.00 | 50.00 | 50.00 | 50.00 | - | - | - | - | - | - | - | -\\n | LaVIN [78] | 13B | 185.00 | 88.33 | 63.33 | 75.00 | - | - | - | - | - | - | - | -\\n | Cheetor [67] | 7B | 180.00 | 96.67 | 80.00 | 116.67 | - | - | - | - | - | - | - | -\\n | GPT-4V [83] | - | 190.00 | 160.00 | 95.00 | 150.00 | - | - | - | - | 89.6 | 92.7 | - | -\\n | LLaVA [75] (7B) | 7B | - | - | - | - | - | - | - | 73.3 | 32.0 | 69.3 | 35.1 | 14.0\\n | LLaVA [75] (13B) | 13B | 185.00 | 155.00 | 133.33 | 170.00 | 68.65 | 67.72 | 66.98 | 71.8 | - | - | - | -\\n | LRV-Instruction [73] | 7B | 165.00 | 111.67 | 86.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Lynx [122] | 7B | 195.00 | 151.67 | 90.00 | 170.00 | - | - | - | - | - | - | - | -\\n | MMICL [130] | 11B | 170.00 | 160.00 | 81.67 | 156.67 | - | - | - | - | - | - | - | -\\n | Muffin [118] | 13B | 195.00 | 163.33 | 66.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Otter [65] | 7B | 195.00 | 88.33 | 86.67 | 113.33 | - | - | - | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B | 158.33 | 150.00 | 128.33 | 170.00 | - | - | - | - | - | - | - | -\\n | SPHINX [71] | 13B | 195.00 | 160.00 | 153.33 | 160.00 | - | - | - | - | - | - | - | -\\n | VPGTrans [124] | 7B | 70.00 | 85.00 | 63.33 | 73.33 | - | - | - | - | - | - | - | -\\n | BLIVA [43] | 11B | 180.00 | 138.33 | 81.67 | 180.00 | - | - | - | - | - | - | - | -\\n | InfMLLM [135] | 13B | 195.00 | 145.00 | 170.00 | 195.00 | - | - | - | - | - | - | - | -\\n | LLaMA-Adapter V2 [26] | 7B | 185.00 | 133.33 | 56.67 | 118.33 | - | - | - | - | - | - | - | -\\n | MiniGPT-4 [138] | 13B | 68.33 | 55.00 | 43.33 | 75.00 | 78.86 | 72.21 | 71.37 | - | 69.3 | 76.7 | 48.2 | 53.0\\n | mPLUG-Owl2 [112] | 7B | 185.00 | 155.00 | 88.33 | 150.00 | - | - | - | - | 78.5 | 84.0 | - | -\\n | LLaVA-1.5 [75] | 7B | - | - | - | - | - | - | - | - | 74.4 | 82.9 | 48.9 | 34.2\\n | CogVLM [106] | 7B | 195.00 | 165.00 | 103.33 | 160.00 | - | - | - | - | 80 | 86.1 | - | -\\n\\n5.1 Data\\nAs discussed in the section on hallucination causes 3, data is one of the primary factors inducing hallucination in MLLMs.\\nFor mitigating hallucination, recent works make attempts on data, including introducing negative data [73], introducing counterfactual data [117], and reducing noise and errors in existing dataset [105, 120].\\nLRV-Instruction [73] LRV-Instruction is proposed to address the issue that existing instruction tuning data primarily focus on positive instruction samples, leading the model to consistently answer ’Yes’.\\nLRV-Instruction is designed to include both positive and negative instructions for more robust visual instruction tuning, where the negative instructions include: 1) ’Nonexistent Object Manipulation’: introducing nonexistent objects, activities, attributes, and interactions; 2) ’Existent Object Manipulation’: manipulating existent objects with inconsistent attributes; 3) ’Knowledge Manipulation’: manipulating knowledge in instructions.\\nHalluciDoctor [117] This paper addresses the object hallucination problem in MLLMs by calibrating the instruction-tuning dataset.\\nThe calibration is conducted from two perspectives.\\nFirstly, it develops a hallucination detection pipeline via consistency cross-checking of multiple MLLMs.\\nBased on the detection result, the hallucinated content can be eliminated.\\nSecondly, this work observes that long-tail distribution and object co-occurrence in the training data are two primary factors of hallucination.\\nThus, a counterfactual visual instruction generation strategy is proposed to expand the dataset.\\nUsing the proposed methods, the instruction tuning data can be balanced and experience reduced hallucination.\\nMLLMs trained on the calibrated dataset are shown to be less prone to hallucination.\\nReCaption [105] This work proposes a framework called ReCaption to rewrite the text captions of existing image-text pairs in datasets.\\nThe framework comprises two steps: 1) keyword extraction, which extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which employs an LLM to generate sentences based on the extracted keywords.\\nUltimately, the framework produces a set of high-quality image-caption pairs.\\nExperiment results show that the model trained on the rewritten caption dataset has higher accuracy on certain benchmarks, such as the POPE benchmark [69].\\nDespite the performance improvement, the question of why rewritten captions can reduce hallucination remains an open problem.\\nEOS Decision [120] Previous work [137] provides an observation that hallucination tends to occur with objects positioned later in the generated descriptions.\\nIntuitively, an ideal scenario is that the MLLM can terminate the generation process in a timely manner.\\nThis idea is thoroughly explored in the work of [120] from the perspective of end-of-sequence (EOS) decision.\\nThe key insight is that the training data may exceed the perception limit of the MLLM.\\nWhen trained with such data, the model may attempt to fit the detail level and length distribution of ground truth captions.\\nHowever, it may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThus, the authors explored approaches to enhance the model’s end-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the perception limit.\\nRegarding data, this work proposes a data filtering strategy to eliminate harmful training data that could impair the model’s ability to end sequences.\\n5.2 Model\\n5.2.1 Scale-up Resolution\\nEnhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.\\n5.2.2 Versatile Vision Encoders\\nSeveral studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.\\n5.2.3 Dedicated Module\\nFollowing our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.\\n5.3 Training\\n5.3.1 Auxiliary supervision.\\nThe primary supervision signal of training MLLMs is language modeling loss (implemented as CrossEntropyLoss) in both pre-training and finetuning stage.\\nHowever, such supervision may not be sufficient to process the rich information encoded in the visual content.\\nAccordingly, the work of [16] constructs a fine-grained vision instruction dataset based on Panoptic Scene Graph (PSG), called Relation-Associated Instruction (RAI-30k).\\nIn addition to standard dialogues, each instruction in RAI-30k is associated with a relation annotation in PSG, which includes mask annotations for related instances.\\nWith these additional annotations, it further supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [57], guiding MLLMs to focus on highly-related image content.\\nWith the additional supervision from the mask prediction loss, MLLMs are encouraged to extract features that can better represent these crucial instances, thus generating more accurate responses and mitigating vision hallucination.\\nThe intuitive idea of supervising MLLMs with grounding shows promising performance in mitigating hallucination.\\nAnother line of work analyzes the training loss from the perspective of embedding space distribution.\\nAs introduced earlier, popular MLLMs typically project the encoded vision features into the input space of a specific LLM.\\nA recent work, HACL [52], argues that an ideal projection should blend the distribution of visual and textual embeddings.\\nHowever, despite visual projection, a significant modality gap exists between textual and visual tokens, suggesting that the current learned interfaces are not effective in mapping visual representations into the textual representation space of LLMs.\\nThis issue potentially exacerbates the tendency for MLLMs to generate more hallucinations.\\nTherefore, HACL proposes enhancing the alignment between visual and textual representations through contrastive loss.\\nTexts with hallucinations are used as hard negative examples for image anchors.\\nThe loss pulls representations of non-hallucinating text and visual samples closer while pushing representations of non-hallucinating and hallucinative text apart.\\nExperiment results show that this method not only reduces hallucination but also enhances performance on other popular benchmarks.\\nRecalling the work of EOS Decision [120], to teach the model to terminate the generation process properly, this work also designs a learning objective, termed Selective EOS Supervision, in addition to the data filtering strategy.\\nThis is achieved by simply modifying the Maximum Likelihood Estimation (MLE), enabling the model to mitigate hallucination through learning from regular instruction data.\\n5.3.2 Reinforcement Learning\\nReinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.\\nAutomatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.\\nReinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.\\nA concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.\\nReinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct\\nPreference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.\\nLLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.\\nSimilarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.\\nAnother similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.\\n5.3.3 Unlearning\\nUnlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.\\n5.4 Inference\\n5.4.1 Generation Intervention.\\nContrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.\\nGuided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.\\nSimilarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.\\nHALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.\\nOthers.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.\\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.\\n5.4.2 Post-hoc Correction\\nPost-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].\\nWoodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.\\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.\\nSimilar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.\\nLogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.\\n6 CHALLENGES AND FUTURE DIRECTIONS\\nThe research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.\\n6.1 Data-centric Challenges and Innovations\\nThe reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.\\n6.2 Cross-modal Alignment and Consistency\\nThe key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.\\n6.3 Advancements in Model Architecture\\nDespite recent advancements in model architectures of LLMs and MLLMs, designing effective architectures specifically tailored to hallucination remains a challenge.\\nDeveloping advanced model architectures capable of capturing complex linguistic structures and generating coherent and contextually relevant output based on input visual content is essential for improving the performance of MLLMs.\\nFuture research can explore innovative architectural designs based on identified causes of hallucination.\\nThis includes developing stronger visual perception models, innovative cross-modal interaction modules capable of transferring cross-modal information seamlessly, and novel large language model architectures faithful to input visual content and text instructions, etc.\\n6.4 Establishing Standardized Benchmarks\\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in assessing the degree of hallucination in MLLMs.\\nIn Table 1, it can be observed that there is a variety of evaluation benchmarks, but a lack of unified standards.\\nAmong them, one of the most popular benchmarks might be POPE [69], which employs a ’Yes-or-No’ evaluation protocol.\\nHowever, this binary-QA manner does not align with how humans use MLLMs.\\nAccordingly, some benchmarks specifically evaluate the hallucination of MLLMs in the (free-form) generative context.\\nYet, they often rely on external models, such as vision expert models or other LLMs, which limits their widespread application.\\nMoving forward, future research can investigate standardized benchmarks that are theoretically sound and easy to use.\\nOtherwise, research on methods to mitigate hallucinations may be built on an incorrect foundation.\\n6.5 Reframing Hallucination as a Feature\\nRecently, discussions on social media [56] have suggested that hallucination can be regarded as an inherent feature of LLMs and MLLMs.\\nThe models are like dream machines.\\nHuman users direct their dreams with prompts.\\nThe prompts start the dream, and based on the model’s hazy recollection of its training documents, most of the time the result goes someplace useful.\\nIt’s only when the dreams enter deemed factually incorrect territory that we label them as ’hallucinations’.\\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications presents exciting opportunities for enhancing user experiences and enabling new use cases.\\nAs humans are the end-users of these models, the primary goal is to enrich human user experiences.\\nFuture research may switch the optimization objective from specific cross-modal benchmarks to human experience.\\nFor example, Some content may cause hallucinations but will not affect the user experience, while some content may.\\nAlternatively, integrating hallucination to inspire more creative ideas in real-world applications could also be intriguing.\\n6.6 Enhancing Interpretability and Trust\\nExisting methods for hallucination mitigation are primarily based on empirical observations of specific patterns, such as skipping the ‘\\\\n’ token and penalizing over-trust tokens.\\nHowever, despite the impressive improvements achieved on specific benchmarks, understanding the underlying mechanisms and decision-making processes remains challenging.\\nFuture research should focus on developing techniques for interpreting and explaining the generation process of MLLMs, thereby providing insights into the factors influencing hallucinated content.\\nThis includes investigating methods for visualizing model internals, identifying salient features and linguistic patterns, and tracing the generation process from input to output.\\nEnhancing the interpretability of MLLMs will not only improve our understanding of model behavior but also enable users to better assess hallucinated content in practical applications.\\n6.7 Navigating the Ethical Landscape\\nAs MLLMs become increasingly proficient at generating realistic text, ethical considerations surrounding the use of generated content become paramount.\\nEspecially in the context of hallucination, the generated response may contain severely concerning ethical content, amplifying the importance of the problem.\\nAddressing ethical concerns related to misinformation, bias, privacy, and societal impact is crucial for promoting responsible AI practices in the development and deployment of MLLMs.\\nIn addition to addressing typical object hallucination, future research on MLLM hallucinations should prioritize ethical considerations throughout the entire lifecycle of MLLM development, from data collection and model training to deployment and evaluation.\\n7 CONCLUSION\\nBased on powerful large language models, multimodal large language models demonstrate remarkable performance across various multimodal tasks.\\nHowever, the phenomenon of hallucination presents a significant challenge to the practical applications of MLLMs, giving rise to undeniable concerns about safety, reliability, and trustworthiness.\\nIn this comprehensive survey, we conducted a thorough examination of hallucinations within multimodal large language models, focusing on their underlying causes, evaluation metrics, benchmarks, and mitigation methods.\\nDespite considerable progress, hallucination remains a complex and persistent concern that warrants ongoing investigation.\\nThe challenge of hallucination in multimodal large language models remains compelling, requiring continuous scrutiny and innovation.\\nIn light of these challenges, we have outlined several promising future directions in this burgeoning domain.\\nThrough navigating the intricate landscape of hallucinations, we aim for this survey to serve as a foundational resource for addressing the complexities of hallucination phenomena in MLLMs.\\nWe envision this survey empowering researchers and practitioners to dedicate efforts to advancing research and developing robust solutions in this vital area of study.',\n",
       " '2.1 Large Language Models\\nBefore moving to multimodal large language models, it is essential to introduce the concept of large language models.\\nTypically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets.\\nProminent examples include GPT-3 [8], PaLM [18], LLaMA [99], and GPT-4 [82].\\nThrough scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning[8], Chain-of-Thought prompting[107] and instruction following[86], among others.\\nThe characteristics and behaviors of LLMs are intricately linked to their training processes.\\nLLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF).\\nBelow, we provide a concise overview of each stage to facilitate comprehension.\\nPre-trianing.\\nPre-training serves as a fundamental phase in the learning process of LLMs [134].\\nDuring this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence.\\nBy undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities.\\nThis pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively.\\nSupervised Fine-Tuning.\\nAlthough pre-training equips LLMs with substantial knowledge and skills, it’s important to acknowledge that its primary focus is on optimizing for completion.\\nConsequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user’s objective of obtaining desired responses.\\nTo address this disparity, the concept of Supervised FineTuning (SFT) [125] has been introduced.\\nSFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs.\\nReinforcement Learning from Human Feedback.\\nAlthough SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.\\nAmong the various methods, Reinforcement Learning from Human Feedback\\nData Quantity Insufficient Data e.g. AMBER [103], LLaVA-RLHF [96]\\nData Quality\\nStatistic Bias\\nVision Model\\n | Language Model | Parametric Knowledge | e.g. | VCD [64], Volcano [63]\\n | --- | --- | --- | ---\\n | Cross-modal Interface | Inferior Alignment | e.g. | HACL [52], Halle-Switch [123]\\n | Sequence Supervision | e.g. MOCHa [5], OPERA [45] |  | \\n | Visual Supervision | e.g. Chen et al. [16] |  | \\n | Human Feedback | e.g. RLHF-V [119] |  | \\n | Lose Visual Attention | e.g. OPERA [45], HaELM [104] |  | \\n | CHAIR | CHAIR [90] |  | \\n | POPE | POPE [69] |  | \\n | LLM-based | e.g. GAVIE [73], HaELM [104], HallusionBench [72] |  | \\n | Others | e.g. Faith-Score [55], AMBER [103] |  | \\n | Discriminative Task | e.g. POPE [69], RAH-Bench [16], FGHE [105] |  | \\n | Generative Task | e.g. GAVIE [73], Faith-Score [55] |  | \\n | Introducing | Negative Data | e.g. | LRV-Instruction [73]\\n | Introducing | Counterfactual Data | e.g. | HalluciDoctor [117]\\n | Mitigating Noises and Errors | e.g. ReCaption [105], EOS [120] |  | \\n | Scale-up Resolution | e.g. LLaVA-1.5 [74], InternVL [14], HallE-Switch [123] |  | \\n | Versatile | Vision Encoders | e.g. | VCoder [49], IVE [38]\\n | Dedicated Module | e.g. HallE-Switch [123] |  | \\n | Auxiliary Supervision\\n |  | Noisy Data | e.g. | HalluciDoctor [117], LLaVA-1.5 [74]\\n |  | Lack of Diversity | e.g. | LRV-Instruction [73], HalluciDoctor [117]\\n |  | Detailed descriptions (open question) | e.g. | Chen et al. [16], EOS [120]\\n |  | Frequent Objects | e.g. | POPE [69]\\n |  | Objects Occurrence | e.g. | LURE [137], VCD [64]\\n |  | Information Loss | e.g. | HallusionBench [72], AMBER [103]\\n |  | Feature Bias | e.g. | Tong et al. [98]\\n\\nfrom Data (§3.1) Hallucination from Model (§3.2)\\nHallucination Causes (§3)\\nHallucination Metrics and Benchmarks(§4)\\nHallucination from Training (§3.3) Hallucination from Inference (§3.4)\\nHallucination Metrics\\nHallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)\\n | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n\\nHallucination Mitigation (§5)\\nReinforcement Learning\\nGeneration Intervention\\nPost-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]\\nMitigating Inference-related Hallucinations (§5.4)\\nFig. 1.\\nThe main content flow and categorization of this survey.\\n(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.\\n | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n\\nFig. 2.\\nPopular architecture of multimodal large language model.',\n",
       " 'Data Quantity Insufficient Data e.g. AMBER [103], LLaVA-RLHF [96]',\n",
       " 'Data Quality',\n",
       " 'Statistic Bias',\n",
       " 'Vision Model\\n | Language Model | Parametric Knowledge | e.g. | VCD [64], Volcano [63]\\n | --- | --- | --- | ---\\n | Cross-modal Interface | Inferior Alignment | e.g. | HACL [52], Halle-Switch [123]\\n | Sequence Supervision | e.g. MOCHa [5], OPERA [45] |  | \\n | Visual Supervision | e.g. Chen et al. [16] |  | \\n | Human Feedback | e.g. RLHF-V [119] |  | \\n | Lose Visual Attention | e.g. OPERA [45], HaELM [104] |  | \\n | CHAIR | CHAIR [90] |  | \\n | POPE | POPE [69] |  | \\n | LLM-based | e.g. GAVIE [73], HaELM [104], HallusionBench [72] |  | \\n | Others | e.g. Faith-Score [55], AMBER [103] |  | \\n | Discriminative Task | e.g. POPE [69], RAH-Bench [16], FGHE [105] |  | \\n | Generative Task | e.g. GAVIE [73], Faith-Score [55] |  | \\n | Introducing | Negative Data | e.g. | LRV-Instruction [73]\\n | Introducing | Counterfactual Data | e.g. | HalluciDoctor [117]\\n | Mitigating Noises and Errors | e.g. ReCaption [105], EOS [120] |  | \\n | Scale-up Resolution | e.g. LLaVA-1.5 [74], InternVL [14], HallE-Switch [123] |  | \\n | Versatile | Vision Encoders | e.g. | VCoder [49], IVE [38]\\n | Dedicated Module | e.g. HallE-Switch [123] |  | \\n | Auxiliary Supervision\\n |  | Noisy Data | e.g. | HalluciDoctor [117], LLaVA-1.5 [74]\\n |  | Lack of Diversity | e.g. | LRV-Instruction [73], HalluciDoctor [117]\\n |  | Detailed descriptions (open question) | e.g. | Chen et al. [16], EOS [120]\\n |  | Frequent Objects | e.g. | POPE [69]\\n |  | Objects Occurrence | e.g. | LURE [137], VCD [64]\\n |  | Information Loss | e.g. | HallusionBench [72], AMBER [103]\\n |  | Feature Bias | e.g. | Tong et al. [98]\\n',\n",
       " 'from Data (§3.1) Hallucination from Model (§3.2)',\n",
       " 'Hallucination Causes (§3)',\n",
       " 'Hallucination Metrics and Benchmarks(§4)\\nHallucination from Training (§3.3) Hallucination from Inference (§3.4)\\nHallucination Metrics\\nHallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)\\n | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n',\n",
       " 'Hallucination from Training (§3.3) Hallucination from Inference (§3.4)',\n",
       " 'Hallucination Metrics',\n",
       " 'Hallucination Benchmarks Mitigating Data-related Hallucinations (§5.1) Mitigating Model-related Hallucinations (§5.2) Mitigating Training-related Hallucinations (§5.3)\\n | Visual Supervision | e.g. | Chen | et al. | [16]\\n | --- | --- | --- | --- | ---\\n | Contrastive Loss | e.g. | HACL [52] |  | \\n | Others | e.g. | EOS [120] |  | \\n | Automatic Metric-based | e.g. | MOCHa [5] |  | \\n | RLAIF-based | e.g. | HA-DPO [133], POVID [136] |  | \\n | RLHF-based | e.g. | LLaVA-RLHF [96], RLHF-V [119] |  | \\n | Contrastive Decoding e.g.VCD [64], IBD [139]\\n | Guided Decoding | e.g.MARINE |  | [131], GCD [24] | \\n | Others | e.g.OPERA |  | [45], Skip‘\\\\n’ [36] | \\n',\n",
       " 'Hallucination Mitigation (§5)\\nReinforcement Learning\\nGeneration Intervention\\nPost-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]\\nMitigating Inference-related Hallucinations (§5.4)\\nFig. 1.\\nThe main content flow and categorization of this survey.\\n(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.\\n | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n\\nFig. 2.\\nPopular architecture of multimodal large language model.',\n",
       " 'Reinforcement Learning',\n",
       " 'Generation Intervention',\n",
       " 'Post-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]',\n",
       " 'Mitigating Inference-related Hallucinations (§5.4)\\nFig. 1.\\nThe main content flow and categorization of this survey.\\n(RLHF) [19, 84, 94] emerges as a notable approach for achieving alignment through reinforcement learning.\\nRLHF typically employs a preference model [7], trained to predict preference rankings based on prompts and human-labeled responses.\\nTo better align with human preferences, RLHF optimizes the LLM to generate outputs that maximize rewards provided by the trained preference model, often utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [93].\\nThis integration of human feedback into the training loop has demonstrated effectiveness in enhancing the alignment of LLMs.\\n | Vision Input | Vision Model | LLM\\n | --- | --- | ---\\n | Image Video … | CLIP DINO-v2 Linear … | LLaMA Vicuna ChatGLM Fuyu\\n | Decoding\\n | Greedy | Beam Search | SamplingText Input\\n | Instruction … | Tokenizer BPE | SentencePiece …\\n\\nFig. 2.\\nPopular architecture of multimodal large language model.',\n",
       " '2.2 Multimodal Large Language Models\\nMLLMs [22, 75, 111, 138] typically refers to a series of models that enable LLMs to perceive and comprehend data from various modalities.\\nAmong them, vision+LLM is particularly prominent, owing to the extensive research on vision-language models (VLMs) [51, 88, 116] prior to LLMs.\\nAs a result, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models (LVLMs).\\nThe goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\" the world via images or videos.\\nCombined with strong reasoning and language generation abilities, MLLMs trigger a series of downstream tasks in multimodal domains, such as image/video captioning and visual question answering.\\nAdditionally, MLLMs serve as the foundation for applications in other fields, such as AI assistants, embodied agents, and robotics.\\nIntegrating the two modalities of vision and language involves primarily two types of approaches.\\nThe first line of work is built upon off-the-shelf pre-trained uni-modal models.\\nSpecifically, these MLLMs usually incorporate a learnable interface between pre-trained visual encoders and LLMs.\\nThe interface extracts and integrates information from visual modalities.\\nSuch interfaces can be further categorized into 1) learnable query-based and 2) projection layer based.\\nLearnable querybased methods, exemplified by Q-Former [66], as used in MiniGPT-4 [138] and Instruct-BLIP [22], utilize a set of learnable query tokens to capture visual signals via cross-attention.\\nProjection layerbased methods, as widely applied in LLaVA [75], Shikra [12], etc., involve training a linear projection layer or a Multi-Layer Perceptron (MLP) module to transform extracted visual features.\\nBoth types of interfaces aim to transform pre-trained visual features into the input space of pre-trained LLMs.\\nAnother line of work is represented by Fuyu-8B [4] and Gemini [97].\\nUnlike previous methods that leverage pre-trained uni-modal models, these works employ end-to-end training from scratch.\\nTaking Fuyu-8B as an example, it does not employ any pre-trained vision encoder.\\nInstead, it directly inputs image patches and employs a linear projection to transform the raw pixels of each patch into embeddings.\\nThe abstracted pipeline is depicted in Fig. 2.\\nMLLMs take input from both visual and textual modalities, learning from multimodal instructions and responses, which leads to remarkable performance across various multimodal tasks.\\nRegarding the training of MLLMs, we provide a concise overview of the training process for interface-based MLLMs.\\nGiven that end-to-end models are closed-source, the training details are unknown.\\nTypically, the training of interface-based MLLMs consists of two stages: 1) pre-training, 2) instruction tuning.\\nPlease describe this image in detail.\\nThe image shows a group of people standing in a park.\\nThere is a large tree in the background with pink blossoms.\\nThe people are all wearing different clothing, including a man in a brown jacket and a woman in a white shirt and jeans.\\nThere are also some people in the background who are not in the foreground of the image.\\nThe people are standing in a grassy area and there are some benches and a fence in the background.\\nThe sky is clear and there are some clouds in the distance.\\nWhat is the girl in the picture doing?\\nThe girl in the picture is standing with her arms outstretched, as if she is trying to catch something.\\nIt is not clear what she is trying to catch, but it could be a kite or a frisbee.\\nThe other people in the picture are standing around her, watching what she is doing.\\nIt appears that they are all having a good time together in the park.\\nCategory Hallucination\\nAttribute Hallucination\\nRelation Hallucination\\nFig. 3.\\nThree types of typical hallucination.\\nPre-training.\\nGiven that models from each modality are pre-trained on their respective data, the objective of this pre-training phase is to achieve cross-modal feature alignment.\\nDuring training, both the pre-trained visual encoder and LLM remain frozen, with only the cross-modal interface being trained.\\nSimilar to traditional VLMs training, as exemplified by CLIP [88], web-scale imagetext pairs [92] are utilized for training.\\nGiven that the final output is at the LLM side, the most widely used loss function in this stage is the text generation loss, typically cross-entropy loss, which aligns with the pre-training of LLMs.\\nCertain studies (e.g., [22, 66]) explore the incorporation of contrastive loss and image-text matching loss to further enhance alignment.\\nAfter training, the interface module maps the visual features into the input embedding space of the LLM.\\nInstruction Tuning.\\nSimilar to LLMs, after pre-training, the current model still lacks instruction following ability in the multimodal context.\\nDuring the instruction tuning stage, both machinegenerated datasets [75] and human-annotated QA datasets [48, 59, 80] are utilized to enhance the model’s ability to comprehend and follow multimodal instructions.\\nUnlike pre-training data, the format and quality of instruction tuning data significantly impact the model’s performance.\\nIt is usually in the format of visual content - instruction - response.\\nEmpirical studies also demonstrate that high-quality data significantly enhances the performance of MLLMs.\\nDuring this stage, there are various options for training, such as fine-tuning LLM parameters in full [75], or using techniques like LoRA [41] to tune specific LLM parameters.',\n",
       " '2.3 Hallucinations in Multimodal Large Language Models\\nHallucination of MLLM generally refers to the phenomenon where the generated text response does not align with the corresponding visual content.\\nState-of-the-art studies in this field primarily focus on object hallucination, given that objects are central to research in computer vision and multimodal contexts.\\nRegarding inconsistency, two typical failure modes are: 1) missing objects, and 2) describing objects that are not present in the image or with incorrect statements.\\nEmpirically, the second mode has been shown to be less preferable to humans.\\nFor example, the LSMDC challenge [91] shows that correctness is more important to human judges than specificity.\\nIn contrast, the coverage of objects is less perceptible to humans.\\nThus, object coverage is not a primary focus in studies of object hallucination.\\nEmpirically, object hallucination can be categorized into three types: object category, object attribute, and object relation.\\nAn example of the three types of hallucination is shown in Fig. 3.\\n• Category.\\nMLLMs identify nonexistent object categories or incorrect categories in the given image.\\nFor example, in Fig. 3, \"some benches and a fence\", \"some clouds\", described in the text response do not exist in the given image.\\n• Attribute.\\nThe object categories identified by MLLMs are accurate, while the descriptions of these objects’ attributes (such as color, shape, material, content, counting, action, etc.) are wrong.\\nIn Fig. 3, \"pink blossoms\" is hallucinated by the MLLM as the color is inaccurate.\\n• Relation.\\nAll objects and their attributes are described correctly, but the relationships among them (such as human-object interactions or relative positions) do not align with the actual image content.\\nIn Fig. 3, \"standing around her, watching\" is a typical example of relation hallucination, as the objects are presented in the image but the relation is inaccurate.\\nIt’s worth noting that some literature may categorize objects counting, objects event, etc., as independent hallucination categories.\\nIn this work, we classify them under the attribute category.\\nThe definition of hallucination types aligns well with the domain of compositional generalization [79, 121] of VLMs, which investigates visio-linguistic generalization and reasoning abilities.\\n3 HALLUCINATION CAUSES\\nHallucinations have multifaceted origins, spanning the entire spectrum of MLLMs’ capability acquisition process.\\nIn this section, we delve into the root causes of hallucinations in MLLMs, primarily categorized into four aspects: Data, Model, Training, and Inference.',\n",
       " '3.1 Data\\nData stands as the bedrock for MLLMs, enabling them to gain cross-modal understanding and instruction-following capabilities.\\nHowever, it can inadvertently become the source of MLLM hallucinations.\\nThis mainly manifests in three aspects: quantity, quality, and statistical bias.',\n",
       " '3.1.1 Quantity\\nDeep learning models are data-hungry, especially large models like MLLMs.\\nThe amount of data plays an important role in building robust and reliable MLLMs.\\nCurrently, image-text pair datasets [92] and visual QA [48, 80] data are used for training MLLMs.\\nAlthough these datasets are usually larger than typical datasets in computer vision, they are still far less abundant than the text-only data used for training LLMs in terms of quantity.\\nInsufficient data could potentially lead to problematic cross-modal alignment, resulting in hallucinations [96, 103].',\n",
       " '3.1.2 Quality\\nGiven the increasing demand for large-scale training data, heuristic data collection methods are employed to efficiently gather vast volumes of data.\\nWhile these methods provide extensive data, they offer no guarantee of quality, thereby increasing the risk of hallucinations.\\nData quality relevant to hallucinations can be further categorized into the following three facets.\\n• Noisy data.\\nAs mentioned in the definition section, training MLLMs involves two stages.\\nThe pre-training stage employs image-text pairs crawled from the web, which contain inaccurate, misaligned, or corrupted data samples.\\nThe noisy data would limit the cross-modal feature alignment [117, 120], which serves as the foundation of MLLMs.\\nAs for the instruction tuning data, prevalent methods, such as LLaVA [75], utilize the advanced GPT-4 [82] model to generate instructions.\\nHowever, ChatGPT is a language model that cannot interpret visual content, leading to the risk of noisy data.\\nMoreover, language models themselves suffer from the issue of hallucination [44], further increasing the risk.\\nLLaVA-1.5 [74] adds human annotated QA data into instruction following and shows improved results, revealing the effect of noisy data.\\n• Lack of diversity.\\nRecent works [73, 117] reveal that the diversity of data also plays a crucial role.\\nFor the data used in the two training stages, instruction tuning data are more likely to have this issue since it is usually in a relatively small amount.\\nOne prominent property is that most instruction following data samples are composed of conversations regarding the image content.\\nWe regard this type of data as positive instruction, as it always faithfully reflects the image content.\\nIn contrast, negative instruction data [73] and reject answering responses [11] are rare in the datasets.\\nGiven such training data, one potential drawback observed by recent studies [69, 73] is that current models tend to answer \"Yes\" for any instructions presented to the model, even when a proper answer should be \"No\", leading to hallucination.\\nThis phenomenon indicates the effect of data diversity.\\n• Detailed descriptions (open question) The impact of the level of detail in textual descriptions on this matter remains an open question.\\nAs discussed in Sec. 2.2, the texts in pre-training data, such as LAION [92], usually describe the salient objects’ overall content.\\nWhile the texts in the instructing tuning stage, such as LLaVA-150k [75], consist of more detailed descriptions.\\nThis LLaVA-150k dataset is generated by GPT-4 based on objects recognized by vision models.\\nOne recent work [16] argues that within the training data, detailed descriptions related to object position, attributes, and non-salient objects are usually absent.\\nThis property results in incomplete cross-modal alignment and deprives the model of grounding ability [62, 126].\\nHowever, another work [120] hypothesizes that the text descriptions in the instruction tuning data contain too much details, exceeding the perception limit of MLLMs.\\nWhen trained with such detailed data, in an attempt to fit the detail level and length distribution of ground truth captions, the model may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThe detail level of the training data remains an open question.\\n3.1.3 Statistic bias.\\nNeural networks, especially large language models, possess an intrinsic tendency to memorize training data, as noted in [23].\\nThe nous (e.g., objects) distribution in the training dataset has strong effects on the behavior of the model.\\nFrequently appeared objects and object co-occurrence are two prominent types of statistical bias, as discussed in [69, 90, 137].\\nFor example, ‘person’ might be one of the most frequently appearing objects in the training data.\\nDuring inference, even if the given image does not contain a person, the model still tends to predict the presence of a person.\\nOn the other hand, object co-occurrence refers to the phenomenon that the model will remember which two objects usually ‘go together’ [90].\\nFor instance, given an image of a kitchen with a refrigerator, MLLMs are prone to answer ‘Yes’ when asked about a microwave, as refrigerators and microwaves frequently appear together in kitchen scenes.\\nBias exists in most datasets.\\nIncreasing the scale of data may alleviate the effect, but cannot fully resolve it, given the long-tail distribution of the real world.',\n",
       " '3.2 Model\\nCurrently, the architecture of popular MLLMs is composed of several components, usually including pre-trained vision model, pre-trained LLM, and alignment module as we discussed above.\\nSince these models are connected together, instead of end-to-end training from scratch, the error of each module can be accumulated.\\nInferior and problematic output from each module may lead to hallucinations.\\n• Weak vision model.\\nAs mentioned in related works [31, 90, 103], a primary potential reason for hallucination is a weak vision model, which can lead to misclassification or misinterpretation of visual concepts.\\nEven themost powerful visionmodelmay still experience information loss during the encoding process.\\nWeak vision model implies weak perception, which fundamentally undermines the multimodal understanding.\\n• Language model prior.\\nThe modern architecture of MLLMs is imbalanced.\\nUsually, the language model is much larger and stronger than the vision model, leading to a tendency to prioritize language-based information [31, 63, 64, 73, 90].\\nA typical phenomenon is that the knowledge entailed in the language model, also termed as parametric knowledge, can override the visual content.\\nFor example, given an image showing a red banana, which is counter-intuitive in the real world, an MLLM may still respond with \"yellow banana\", as \"banana is yellow\" is a deep-rooted knowledge in the LLM.\\nSuch language/knowledge prior makes the model overlook the visual content and response with hallucination.\\n• Weak alignment interface.\\nThe alignment interface plays an essential role in MLLMs, as it serves as the bridge between the two modalities.\\nA weak alignment interface can easily cause hallucinations.\\nOne potential cause of a weak alignment interface is data, as discussed in earlier sections.\\nApart from that, the interface architecture itself and training loss design also matter [52, 77, 123].\\nRecent work [52] argues that the LLaVA-like linear projection interface preserves most of the information, but lacks supervision on the projected feature.\\nVisualization in [52] reveals that the features after the projection layer remain distinct from the language embeddings.\\nThe distribution gap causes trouble in cross-modal interaction, leading to hallucination.\\nOn the other hand, Q-former-like [66] architecture has diverse supervision on the extracted visual feature, aligning it to the language embedding space.\\nHowever, the use of learnable queries inevitably results in the loss of fine-grained visual information.',\n",
       " '3.3 Training\\nThe training objective of MLLMs is basically the same as LLMs, i.e, auto-regressive next token prediction loss.\\nThis loss is straightforward yet effective and easy to scale up, showing promising performance in language modeling.\\nHowever, some studies in the field of MLLMs have suggested that the next-token prediction loss might not be suitable for learning visual content due to its complex spatial structure [5, 16].\\nAdditionally, the loss optimizes at the token level, while lacking supervision at the sequence level [5].\\nAnother perspective is that, unlike training LLMs, the RLHF stage is absent in training procedure of MLLMs [96, 119], becoming a potential cause of hallucination.',\n",
       " '3.4 Inference\\nAs for inference, some works also argues a potential issue in the auto-regressive generation.\\nDuring generation, as the sequence length grows, the self-attention will focus more on the previously generated text tokens, i.e., the attention on the visual content is diluted [45, 102–104].\\nThrough visualizing the attention map during generation [45, 104], it can be observed that the generated content focuses more on previous special tokens, such as punctuation, rather than visual content tokens.\\nThe issue of ’losing attention’ would also lead to the model’s output response being irrelevant to the visual content.\\n4 HALLUCINATION METRICS AND BENCHMARKS\\nIn this section, we present a comprehensive overview of existing hallucination metrics and benchmarks, which are designed to assess the extent of hallucinations generated by existing cutting-edge MLLMs.\\nCurrently, the primary focus of these benchmarks is on evaluating the object hallucination of MLLM-generated content.\\nTab.\\n1 illustrates a summary of related benchmarks.\\nCHAIR [90].\\nAs one of the early works, the metric of CHAIR was proposed to evaluate object hallucination in the traditional image captioning task.\\nThis is achieved by computing what proportion of words generated are actually in the image according to the ground truth sentences and object segmentations.\\nThe computation of the CHAIR metric is straightforward and easy to understand.\\nThe metric has two variants: per-instance (denoted as CHAIR𝑖) and per-sentence\\nTable 1.\\nSummary of most relevant benchmarks and metrics of object hallucination in MLLMs.\\nThe order is based on chronological order on arxiv.\\nIn the metric column, Acc/P/R/F1 denotes Accuracy/Precision/Recall/F1Score.\\n | Benchmark | Venue Underlying Data Source | Size | Task | Type | Metric | Hallucination | Type\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | CHAIR [90] | EMNLP’18 MSCOCO [70] | 5,000 | Gen | CHAIR | ✓ | ✗ | ✗ | ✗\\n | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | Category Attribute Relation Others\\n | POPE [69] | EMNLP’23 MSCOCO [70] | 3,000 | Dis | Acc/P/R/F1 | ✓ | ✗ | ✗ | ✗\\n | MME [113] | arXiv’23 Jun MSCOCO [70] | 1457 | Dis | Acc/Score | ✓ | ✓ | ✗ | ✓\\n | CIEM [42] | NeurIPS-W’23 MSCOCO [70] | 78120 | Dis | Acc | ✓ | ✗ | ✗ | ✗\\n | M-HalDetect [32] | arXiv’23 Aug. MSCOCO [70] | 4,000 | Dis | Reward Model Score | ✓ | ✗ | ✗ | ✗\\n | MMHal-Bench [96] arXiv’23 Sep. Open-Images [61] |  | 96 | Gen | LLM Assessment | ✓ | ✗ | ✗ | ✓\\n | GAVIE [73] | ICLR’24 Visual-Genome [59] | 1,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | NOPE [77] | arXiv’23 Oct. Open-Images [61] | 36,000 | Dis | Acc/METEOR [3] | ✓ | ✗ | ✗ | ✗\\n | HaELM [104] | arXiv’23 Oct. MSCOCO [70] | 5,000 | Gen | LLM Assessment |  | Not Explicitly Stated |  | \\n | FaithScore [55] | arXiv’23 Nov. MSCOCO [70] | 2,000 | Gen | FaithScore | ✓ | ✓ | ✓ | Obj. Counting\\n | Bingo [21] | arXiv’23 Nov. Unknown | 370 | Gen | Human Assessment | ✗ | ✗ | ✗ | Model Bias\\n | AMBER [103] | arXiv’23 Nov. Web | 15,202 | Dis & Gen | AMBER Score | ✓ | ✓ | ✓ | ✗\\n | RAH-Bench [16] | arXiv’23 Nov. MSCOCO [70] | 3,000 | Dis | False Positive Rate | ✓ | ✓ | ✓ | ✗\\n | HallusionBench [72] | CVPR’24 Unknown | 1,129 | Gen | LLM Assessment | ✗ | ✗ | ✗ | Model Diagnose\\n | CCEval [123] | arXiv’23 Dec. Visual-Genome [59] | 100 | Gen | LLM-based CHAIR | ✓ | ✗ | ✗ | ✗\\n | MERLIM [100] | arXiv’23 Dec. MSCOCO [70] | 31,373 | Dis | Accuracy | ✓ | ✗ | ✓ | Obj. Counting\\n | FGHE [105] | arXiv’23 Dec. MSCOCO [70] | 200 | Dis | Acc/P/R/F | ✓ | ✓ | ✓ | Obj. Behavior\\n | MOCHa [5] | arXiv’23 Dec. Synthetic | 2,000 | Gen | OpenCHAIR [5] | ✓ | ✓ | ✗ | ✗\\n | CorrelationQA [35] | arXiv’24 Feb. Synthetic | 7,308 | Dis | Acc/AccDrop | ✗ | ✗ | ✗ | Model Bias\\n | VQAv2-IDK [11] | arXiv’24 Feb. VQAv2 [30] | 6,624 | Dis | Acc | ✗ | ✗ | ✗ | IK [11]\\n | MHaluBench [13] | arXiv’24 Feb. MSCOCO [70] | 1,860 | Gen | Acc/P/R/F | ✓ | ✓ | ✗ | T2I\\n | VHTest [46] | arXiv’24 Feb. MSCOCO [70] | 1,200 | Dis & Gen | Acc | ✓ | ✓ | ✗ | ✓\\n | Hal-Eavl [53] | arXiv’24 Feb. MSCOCO [70] & LAION [92] | 10,000 Dis & Gen |  | Acc/P/R/F & LLM Assessment | ✓ | ✓ | ✓ | Obj. Event\\n\\n(denoted as CHAIR𝑠):\\n | CHAIR𝑖 = | |{hallucinated objects}| |{all objects mentioned}| ,\\n | CHAIR𝑠 = | |{sentences with hallucinated object}| |{all sentences}|\\n\\nn the paper of CHAIR [90], the range of objects is restricted to the 80 MSCOCO objects. \\nSentence tokenization and synonyms mapping are applied to determine whether a generated sentence contains hallucinated objects.\\n Ground-truth caption and object segmentations both serve as groundtruth objects in the computation\\n. In the MLLM era, this metric is still widely used for assessing the response of MLLM\\nPOPE [69].\\nWhen used in MLLMs, the work of [69] argues that the CHAIR metric can be affected by the instruction designs and the length of generated captions.\\nTherefore, it proposes a new evaluation metric as well as a benchmark, called Pooling-based Object Probing Evaluation (POPE).\\nThe basic idea is to convert the evaluation of hallucination into a binary classification task by prompting MLLMs with simple Yes-or-No short questions about the probing objects (e.g., Is there a car in the image?) Compared to CHAIR, POPE offers increased stability and flexibility.\\nBased on this metric design, it further proposed an evaluation benchmark, drawing 500 images from the MSCOCO dataset.\\nThe questions in the benchmark consist of both positive and negative questions.\\nThe positive questions are formed based on the ground-truth objects, while the negative questions are built from sampling nonexistent objects.\\nThe benchmark is divided into three subsets according to different negative sampling strategy: random, popular, and adversarial.\\nPopular and adversarial sampling are specifically designed to assess frequently appeared objects and object co-occurrence.\\nAs an early representative work, POPE serves as a foundation of object hallucination evaluation.\\nMME [113].\\nMME is a comprehensive evaluation benchmark for MLLMs.\\nIt covers the examination of perception and cognition abilities, encompassing 14 subtasks.\\nRegarding object hallucination, there are four popular object related subtasks in its perception evaluation, including object existence, count, position, color.\\nSimilar to POPE, these tasks are formulated as Yes-or-No tasks.\\nCIEM [42] CIEM is a benchmark to evaluate hallucination of MLLMs.\\nUnlike previous works utilize human annotated objects, CIEM is generated using an automatic pipeline.\\nThe pipeline takes the text description of a specific image as input and utilize advanced LLMs to generate QA pairs.\\nAlthough the LLM-based data generation pipeline is not completely reliable, empirical result shows that the generated data has low error rate, around 5%.\\nMMHal-Bench [96] Comprising 96 image-question pairs, ranging in 8 question categories × 12 object topics, MMHal-Bench is a dedicated benchmark for evaluating hallucination in MLLMs.\\nThe 8 question categories cover various types of hallucination, including object attributes, counting, spatial relations, etc.\\nDuring the evaluation of MMHal-Bench, the GPT-4 model is employed to analyze and rate the responses.\\nGAVIE [73]GPT4-Assisted Visual Instruction Evaluation (GAVIE) is proposed to assess the LMM output in two different aspects: Relevancy to evaluate the instruction-following performance and Accuracy to measure the visual hallucination in the LMM output.\\nIt comprises a benchmark with 1,000 samples and an evaluation approach.\\nGAVIE evaluates the output of MLLMs in an open-ended manner and does not require human-annotated ground-truth answers.\\nThe core idea is to ask the advanced GPT-4 to work as a smart teacher and score the answer by taking image content, human instruction, and model response as input.\\nNOPE [77] This paper proposes to establish a distinction between object hallucination and incorrectness.\\na) Object hallucination refers to a phenomenon in VQA where a VL model’s response includes a non-existent object, despite the ground truth answer being a negative indefinite pronoun (e.g., \"none\", \"no one\", etc).\\nThis is denoted as NegP.\\nb) Incorrectness occurs when a VL model fails to accurately respond to a question with a ground truth answer that is anything other than NegP, denoted as Others.\\nThis paper argues that the existing VQA datasets have a significantly imbalanced distribution, containing too littleNegP data.\\nTherefore, NOPE (Negative Object Presence Evaluation) is proposed in this paper to complement the absent NegP data.\\nDuring evaluation, traditional metrics, including Accuracy and METEOR, are employed.\\nHaELM [104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4 models to assess the quality of theMLLM response.\\nIn contrast, the work of Hallucination Evaluation based on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination detection.\\nIt collects a set of hallucination data generated by a wide range of MLLMs, simulates data using ChatGPT, and trains an LLM based on LLaMA [99].\\nAfter that, the HaELM model becomes proficient in hallucination evaluation, leveraging reference descriptions of images as the basis of assessment.\\nFaithScore [55] Considering the natural forms of interaction between humans and MLLMs, FaithScore aims to evaluate free-form responses to open-ended questions.\\nDifferent from LLM-based overall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate, and analyze the elements in detail.\\nSpecifically, it includes three steps: descriptive sub-sentence identification, atomic fact generation, and fact verification.\\nThe evaluation metric involves finegrained object hallucination categories, including entity, count, color, relation, and other attributes.\\nThe final computation of FaithScore is the ratio of hallucinated content.\\nBingo [21] Bingo (Bias and Interference Challenges in Visual Language Models) is a benchmark specifically designed for assessing and analyzing the limitations of current popular MLLMs, such as GPT-4V [83].\\nIt comprises 190 failure instances, along with 131 success instances as a comparison.\\nThis benchmark reveals that state-of-the-art MLLMs show the phenomenon of bias and interference.\\nBias refers to the model’s susceptibility to generating hallucinatory outputs on specific types of examples, such as OCR bias, region bias, etc.\\nInterference refers to scenarios in which the judgment model can be disrupted, making it more susceptible to hallucination.\\nDue to the small amount of data in this benchmark, the assessment and analysis are mostly conducted by humans.\\nAMBER [103] Upon the application and evaluation of MLLMs, the tasks can be roughly divided into generative tasks and discriminative tasks.\\nFor generative tasks, this paper argues that most existing works rely on additional LLMs, suffering from computational cost.\\nAs for discriminative tasks, the most popular evaluation suite is POPE [69].\\nHowever, POPE lacks fine-grained hallucination types such as attributes and relations.\\nAMBER (An LLM-free Multi-dimensional Benchmark) is proposed to support the evaluation of generative tasks and discriminative tasks, including object existence hallucination, attribute hallucination, and relation hallucination.\\nIt further combines the CHAIR [90] metric in generative tasks and F1 in discriminative tasks to form the AMBER Score as follows:\\nAMBER Score = 𝐴𝑣𝑔(1 − CHAIR, F1).\\n(1)\\nRAH-Bench [16] Relation-Associated Hallucination Benchmark (RAH-Bench) can be regarded as an upgraded version of POPE, containing 3,000 yes-or-no questions with their corresponding images.\\nDifferent from POPE, RAH-Bench further divides the negative questions into three subsets.\\nEach subset contains 500 questions with misleading statements in the different aspects, including:\\n1) categorical hallucination, 2) attribute hallucination, 3) relation hallucination.\\nHallusionBench [72] To diagnose and analyze the potential failure modes of MLLMs, HallusionBench evaluates hallucination from a different perspective.\\nIt consists of 455 visual-question control pairs, with 346 different figures and a total of 1129 questions covering diverse topics and formats.\\nThe questions are divided into two categories: Visual Dependent and Visual Supplement.\\nThe Visual Dependent questions are defined as questions that do not have an affirmative answer without the visual context.\\nThis setting aims to evaluate visual commonsense knowledge and visual reasoning skills.\\nThe Visual Supplement questions can be answered without the visual input; the visual component merely provides supplemental information or corrections.\\nThis setting is designed to evaluate visual reasoning ability and the balance between parametric memory (language prior) and image context.\\nThis division provides a new perspective for understanding and diagnosing MLLMs.\\nCCEval [123] CCEval focuses on the hallucination evaluation of detailed captions.\\nTraditional caption-based evaluation benchmarks and metrics, like CHAIR, are known to favor short captions.\\nHowever, short captions often lack detail and contain less information.\\nTo address this issue, CCEval randomly samples 100 images from Visual Genome to form a benchmark.\\nIn evaluation, GPT-4 is utilized to parse the captions generated by MLLMs and extract objects.\\nAdditionally, this work introduces the \"coverage\" metric on top of CHAIR to ensure that the captions are detailed enough.\\nThis metric computes the ratio of objects in the caption that match the ground truth to the total number of ground truth objects.\\nIt additionally records the average number of objects as well as the average length of captions as auxiliary metric.\\nCompared with CHAIR, CCEval employs more diverse objects, as reflected in the source of ground truth (Visual Genome vs. COCO) and caption parsing (GPT-4 vs. rule-based tool).\\nMERLIM [100] MERLIM (Multi-modal Evaluation benchmaRk for Large Image-language Models) is a test-bed aimed at empirically evaluating MLLMs on core computer vision tasks, including object recognition, instance counting, and identifying object-to-object relationships.\\nMERLIM contains over 279K image-question pairs, and has a strong focus on detecting cross-modal hallucinations.\\nInterestingly, when organizing the data, a set of edited images is intentionally added.\\nBased on the original image, an inpainting strategy is employed to remove one object instance in the image.\\nWith this original-edited image pair, one can compare the output of the target MLLM and identify the hallucinated objects that lack visual grounding.\\nFGHE [105] Fine-Grained Object Hallucination Evaluation (FGHE) follows a binary classification approach similar to POPE to evaluate MLLMs.\\nHowever, unlike POPE, FGHE requires a different set of binary questions to measure fine-grained hallucination.\\nThe FGHE dataset consists of 50 images and 200 binary questions divided into three categories: (a) multiple-object questions, which verify the relationships between multiple objects in the image; (b) attribute questions, which verify attributes of objects in the image; and (c) behavior questions, which verify behaviors or objects in the image.\\nThe questions are manually defined by human annotators on a subset of 50 images from the validation set of the MSCOCO dataset.\\nSimilar to POPE, the Accuracy, Precision, Recall, and F1 score are employed as the evaluation metrics.\\nOpenCHAIR [5] The traditional CHAIR metric relies on the closed list of 80 objects in the MS-COCO dataset, limiting its application.\\nTo measure object hallucination in the open-vocabulary settings, OpenCHAIR expands CHAIR by relaxing the strong reliance on the closed vocabulary.\\nThe ’open-vocabulary’ manifests in two ways.\\nFirstly, when building the benchmark, it organizes a dataset consisting of synthetic images with corresponding captions, which include diverse, openvocabulary objects using a text-to-image diffusion model.\\nSecondly, during computing the metric, CHAIR checks if words or their synonyms (as given by fixed vocabulary lists) are found in groundtruth annotations.\\nIn contrast, OpenCHAIR extracts concrete objects from a predicted caption and identifies hallucinated objects from this list by querying an LLM.\\nSimilar to CHAIR, the final metric computation is based on the hallucination rate.\\nHal-Eval [53] The work of Hal-Eval [53] identifies another type of object hallucination: event hallucination.\\nThis type of hallucination fabricates a fictional target and constructing an entire narrative around it, including its attributes, relationships, and actions.\\nThis effort further completes the definition of hallucination types.\\nIn addition, this work proposes an evaluation benchmark, which encompasses both discriminative and generative evaluation methods.\\nThis is achieved by collecting two evaluation subsets, each tailored to the discriminative and generative evaluation methods, respectively.\\nCorrelationQA [35] CorrelationQA is a dedicated benchmark to quantify the effect of hallucination induced by the spurious visual input.\\nThis type of hallucination usually occurs when providing the MLLM with images that are highly relevant but inconsistent with the answers, causing MLLMs to suffer from hallucination.\\nSuch visual inputs are defined as ’spurious visual inputs’.\\nThis benchmark reveals that most of mainstream MLLMs, including GPT-4V, suffer from hallucination when presented with such spurious visual inputs.\\nThis phenomenon indicates that an image can induce MLLMs to instinctively focus on visual content, resulting in responses that are predominantly based on visual information without proper reasoning and thinking.\\nVQAv2-IDK [11] It has been widely discussed that in the binary QA scenario, MLLMs generally have a bias on answering ’Yes-or-No,’ leading to hallucination.\\nIn a more detailed question and answer scenario, MLLMs generally tend to respond to the user’s question plausibly, even if the desired answer is ’I don’t know’.\\nThe concept is defined as ’I Know (IK)’ hallucination in the work of [11].\\nAccordingly, a new benchmark, VQAv2-IDK, is proposed to specifically evaluate this type of hallucination.\\nVQAv2-IDK is a subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators.\\nIn this benchmark, ’I Know (IK)’ hallucination has been further categorized into four types:\\n• Unanswerable: no one can know.\\n• Don’t know: human may not know, but robot might.\\n• False questions: refers non-existing.\\n• Not sure: ambiguous to answer.\\nThis benchmark opens a new track for the study of hallucination in MLLMs.\\nMHaluBench [13] This benchmark does not aim to evaluate the MLLMs themselves.\\nInstead, it is intentionally designed to evaluate the hallucination detection tools of MLLMs, i.e., judge whether a tool can successfully detect the hallucination produced by an MLLM.\\nThus, the benchmark consists of hallucinatory examples.\\nSpecifically, the benchmark unifies image-to-text tasks and the text-to-image tasks into one evaluation suite: cross-modal consistency checking.\\nThe hallucinatory examples are generated using leading MLLMs and image generation models, such as LLaVA [75], MiniGPT-4 [138], DALL-E2 [89], and DALL-E3 [6].\\nDuring evaluation, the benchmark can be used to compare different hallucination detection methods based on their performance.\\nSo far, there are not many dedicated hallucination detection methods.\\nThis work serves as a basis for this direction.\\nVHTest [46] VHTest categorizes visual properties of objects in an image into 1) individual properties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which emerge from comparisons across multiple objects, such as relative size, relative position, and counting.\\nBased on such categorization, the authors further defined 8 visual hallucination modes, providing a very detailed evaluation of hallucination in MLLMs.\\nFurthermore, the collected 1,200 evaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no question\" (YNQ).\\nSuch design enables this benchmark to evaluate both generative and discriminative tasks.\\nComparison of mainstream models We compare the mainstream MLLMs on some representative benchmarks, providing a holistic overview of their performance from different dimensions.\\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks.\\nWe observe that the MLLMs’ performance is not always consistent across different benchmarks.\\nIt indicates that different benchmarks have different evaluation dimensions and emphases.\\nTable 2.\\nComparison of mainstreamMLLMs on generative benchmarks.\\nThe numbers come from the original papers of these benchmarks.\\n | Model | LLM Size CHAIR (On AMBER) ↓ | AMBER Score ↑ | HallusionBench All-Acc ↑ | FaithScore (LLaVA-1k) ↑ | FaithScore (COCO-Cap) ↑ | Hal-Eval In-domain Gen. Acc ↑ | Hal-Eval Out-of-domain Gen. Acc ↑\\n | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B 23.1 | 54.1 | 43.93 | 0.7167 | 0.8546 | 27.3 | 29.5\\n | Multimodal-GPT [28] | 7B - | - | - | 0.5335 | 0.5440 | - | -\\n | InstructBLIP [22] | 7B 10.3 | 86.2 | 45.26 | 0.8091 | 0.9392 | 35.5 | 41.3\\n | GPT-4V [83] | - 4.3 | 92.7 | 65.28 | - | - | - | -\\n | LLaVA (7B) [75] | 7B 13.5 | 69.3 | - | - | - | 23.3 | 26.3\\n | LLaVA (13B) [75] | 13B - | - | - | 0.8360 | 0.8729 | - | -\\n | MiniGPT-4 (7B) [138] | 7B - | - | 35.78 | 0.5713 | 0.6359 | 61.4 | 50.1\\n | MiniGPT-4 (13B) [138] | 13B 15.9 | 76.7 | - | - | - | - | -\\n | mPLUG-Owl2 [112] | 7B 10.6 | 84.0 | 47.30 | - | - | - | -\\n | LLaVA-1.5 (7B) [74] | 7B 8.6 | 82.9 | - | - | - | 44.6 | 46.4\\n | LLaVA-1.5 (13B) [74] | 13B - | - | 46.94 | 0.8566 | 0.9425 | - | -\\n | CogVLM [106] | 7B 7.9 | 86.1 | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B - | - | 39.15 | - | - | - | -\\n | Open-Flamingo [1] | 9B - | - | 38.44 | - | - | - | -\\n | LRV-Instruction [73] | - - | - | 42.78 | - | - | - | -\\n\\n5 HALLUCINATION MITIGATION\\nIn this section, we present a comprehensive review of contemporary methods aimed at mitigating hallucinations in MLLMs.\\nBased on the properties and perspectives of these methods, we systematically categorize them into four groups.\\nSpecifically, we investigate approaches addressing hallucination from Data, Model, Training, and Inference.\\nTable 3.\\nComparison of mainstream MLLMs on discriminative benchmarks.\\nThe numbers come from the original papers of these benchmarks.\\n | Model | LLM Size | MME Existence Score ↑ | MME Count Score ↑ | MME Position Score ↑ | MME Color Score ↑ | POPE Random F1-Score ↑ | POPE Random F1-Score ↑ | POPE Adversarial F1-Score ↑ | RAH-Bench F1 Score ↑ | AMBER Dis. F1-Score ↑ | AMBER Score ↑ | Hal-Eval In-domain Event. F1 ↑ | Hal-Eval Out-of-domain Event. F1 ↑\\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | mPLUG-Owl [111] | 7B | 120.00 | 50.00 | 50.00 | 55.00 | 68.06 | 66.79 | 66.82 | 69.3 | 31.2 | 54.1 | 47 | 46.6\\n | ImageBind-LLM [34] | 7B | 128.33 | 60.00 | 46.67 | 73.33 | - | - | - | - | - | - | - | -\\n | InstructBLIP [22] (7B) | 7B | - | - | - | - | - | - | - | 89.1 | 82.6 | 86.2 | 66.2 | 66.6\\n | InstructBLIP [22] (13B) | 13B | 185.00 | 143.33 | 66.67 | 153.33 | 89.29 | 83.45 | 78.45 | 84.7 | - | - | - | -\\n | VisualGLM-6B [25] | 6B | 85.00 | 50.00 | 48.33 | 55.00 | - | - | - | - | - | - | - | -\\n | Multimodal-GPT [28] | 7B | 61.67 | 55.00 | 58.33 | 68.33 | 66.68 | 66.67 | 66.67 | - | - | - | - | -\\n | PandaGPT [95] | 7B | 70.00 | 50.00 | 50.00 | 50.00 | - | - | - | - | - | - | - | -\\n | LaVIN [78] | 13B | 185.00 | 88.33 | 63.33 | 75.00 | - | - | - | - | - | - | - | -\\n | Cheetor [67] | 7B | 180.00 | 96.67 | 80.00 | 116.67 | - | - | - | - | - | - | - | -\\n | GPT-4V [83] | - | 190.00 | 160.00 | 95.00 | 150.00 | - | - | - | - | 89.6 | 92.7 | - | -\\n | LLaVA [75] (7B) | 7B | - | - | - | - | - | - | - | 73.3 | 32.0 | 69.3 | 35.1 | 14.0\\n | LLaVA [75] (13B) | 13B | 185.00 | 155.00 | 133.33 | 170.00 | 68.65 | 67.72 | 66.98 | 71.8 | - | - | - | -\\n | LRV-Instruction [73] | 7B | 165.00 | 111.67 | 86.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Lynx [122] | 7B | 195.00 | 151.67 | 90.00 | 170.00 | - | - | - | - | - | - | - | -\\n | MMICL [130] | 11B | 170.00 | 160.00 | 81.67 | 156.67 | - | - | - | - | - | - | - | -\\n | Muffin [118] | 13B | 195.00 | 163.33 | 66.67 | 165.00 | - | - | - | - | - | - | - | -\\n | Otter [65] | 7B | 195.00 | 88.33 | 86.67 | 113.33 | - | - | - | - | - | - | - | -\\n | Qwen-VL-Chat [2] | 7B | 158.33 | 150.00 | 128.33 | 170.00 | - | - | - | - | - | - | - | -\\n | SPHINX [71] | 13B | 195.00 | 160.00 | 153.33 | 160.00 | - | - | - | - | - | - | - | -\\n | VPGTrans [124] | 7B | 70.00 | 85.00 | 63.33 | 73.33 | - | - | - | - | - | - | - | -\\n | BLIVA [43] | 11B | 180.00 | 138.33 | 81.67 | 180.00 | - | - | - | - | - | - | - | -\\n | InfMLLM [135] | 13B | 195.00 | 145.00 | 170.00 | 195.00 | - | - | - | - | - | - | - | -\\n | LLaMA-Adapter V2 [26] | 7B | 185.00 | 133.33 | 56.67 | 118.33 | - | - | - | - | - | - | - | -\\n | MiniGPT-4 [138] | 13B | 68.33 | 55.00 | 43.33 | 75.00 | 78.86 | 72.21 | 71.37 | - | 69.3 | 76.7 | 48.2 | 53.0\\n | mPLUG-Owl2 [112] | 7B | 185.00 | 155.00 | 88.33 | 150.00 | - | - | - | - | 78.5 | 84.0 | - | -\\n | LLaVA-1.5 [75] | 7B | - | - | - | - | - | - | - | - | 74.4 | 82.9 | 48.9 | 34.2\\n | CogVLM [106] | 7B | 195.00 | 165.00 | 103.33 | 160.00 | - | - | - | - | 80 | 86.1 | - | -\\n\\n5.1 Data\\nAs discussed in the section on hallucination causes 3, data is one of the primary factors inducing hallucination in MLLMs.\\nFor mitigating hallucination, recent works make attempts on data, including introducing negative data [73], introducing counterfactual data [117], and reducing noise and errors in existing dataset [105, 120].\\nLRV-Instruction [73] LRV-Instruction is proposed to address the issue that existing instruction tuning data primarily focus on positive instruction samples, leading the model to consistently answer ’Yes’.\\nLRV-Instruction is designed to include both positive and negative instructions for more robust visual instruction tuning, where the negative instructions include: 1) ’Nonexistent Object Manipulation’: introducing nonexistent objects, activities, attributes, and interactions; 2) ’Existent Object Manipulation’: manipulating existent objects with inconsistent attributes; 3) ’Knowledge Manipulation’: manipulating knowledge in instructions.\\nHalluciDoctor [117] This paper addresses the object hallucination problem in MLLMs by calibrating the instruction-tuning dataset.\\nThe calibration is conducted from two perspectives.\\nFirstly, it develops a hallucination detection pipeline via consistency cross-checking of multiple MLLMs.\\nBased on the detection result, the hallucinated content can be eliminated.\\nSecondly, this work observes that long-tail distribution and object co-occurrence in the training data are two primary factors of hallucination.\\nThus, a counterfactual visual instruction generation strategy is proposed to expand the dataset.\\nUsing the proposed methods, the instruction tuning data can be balanced and experience reduced hallucination.\\nMLLMs trained on the calibrated dataset are shown to be less prone to hallucination.\\nReCaption [105] This work proposes a framework called ReCaption to rewrite the text captions of existing image-text pairs in datasets.\\nThe framework comprises two steps: 1) keyword extraction, which extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which employs an LLM to generate sentences based on the extracted keywords.\\nUltimately, the framework produces a set of high-quality image-caption pairs.\\nExperiment results show that the model trained on the rewritten caption dataset has higher accuracy on certain benchmarks, such as the POPE benchmark [69].\\nDespite the performance improvement, the question of why rewritten captions can reduce hallucination remains an open problem.\\nEOS Decision [120] Previous work [137] provides an observation that hallucination tends to occur with objects positioned later in the generated descriptions.\\nIntuitively, an ideal scenario is that the MLLM can terminate the generation process in a timely manner.\\nThis idea is thoroughly explored in the work of [120] from the perspective of end-of-sequence (EOS) decision.\\nThe key insight is that the training data may exceed the perception limit of the MLLM.\\nWhen trained with such data, the model may attempt to fit the detail level and length distribution of ground truth captions.\\nHowever, it may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThus, the authors explored approaches to enhance the model’s end-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the perception limit.\\nRegarding data, this work proposes a data filtering strategy to eliminate harmful training data that could impair the model’s ability to end sequences.\\n5.2 Model\\n5.2.1 Scale-up Resolution\\nEnhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.\\n5.2.2 Versatile Vision Encoders\\nSeveral studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.\\n5.2.3 Dedicated Module\\nFollowing our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.\\n5.3 Training\\n5.3.1 Auxiliary supervision.\\nThe primary supervision signal of training MLLMs is language modeling loss (implemented as CrossEntropyLoss) in both pre-training and finetuning stage.\\nHowever, such supervision may not be sufficient to process the rich information encoded in the visual content.\\nAccordingly, the work of [16] constructs a fine-grained vision instruction dataset based on Panoptic Scene Graph (PSG), called Relation-Associated Instruction (RAI-30k).\\nIn addition to standard dialogues, each instruction in RAI-30k is associated with a relation annotation in PSG, which includes mask annotations for related instances.\\nWith these additional annotations, it further supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [57], guiding MLLMs to focus on highly-related image content.\\nWith the additional supervision from the mask prediction loss, MLLMs are encouraged to extract features that can better represent these crucial instances, thus generating more accurate responses and mitigating vision hallucination.\\nThe intuitive idea of supervising MLLMs with grounding shows promising performance in mitigating hallucination.\\nAnother line of work analyzes the training loss from the perspective of embedding space distribution.\\nAs introduced earlier, popular MLLMs typically project the encoded vision features into the input space of a specific LLM.\\nA recent work, HACL [52], argues that an ideal projection should blend the distribution of visual and textual embeddings.\\nHowever, despite visual projection, a significant modality gap exists between textual and visual tokens, suggesting that the current learned interfaces are not effective in mapping visual representations into the textual representation space of LLMs.\\nThis issue potentially exacerbates the tendency for MLLMs to generate more hallucinations.\\nTherefore, HACL proposes enhancing the alignment between visual and textual representations through contrastive loss.\\nTexts with hallucinations are used as hard negative examples for image anchors.\\nThe loss pulls representations of non-hallucinating text and visual samples closer while pushing representations of non-hallucinating and hallucinative text apart.\\nExperiment results show that this method not only reduces hallucination but also enhances performance on other popular benchmarks.\\nRecalling the work of EOS Decision [120], to teach the model to terminate the generation process properly, this work also designs a learning objective, termed Selective EOS Supervision, in addition to the data filtering strategy.\\nThis is achieved by simply modifying the Maximum Likelihood Estimation (MLE), enabling the model to mitigate hallucination through learning from regular instruction data.\\n5.3.2 Reinforcement Learning\\nReinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.\\nAutomatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.\\nReinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.\\nA concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.\\nReinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct\\nPreference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.\\nLLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.\\nSimilarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.\\nAnother similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.\\n5.3.3 Unlearning\\nUnlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.\\n5.4 Inference\\n5.4.1 Generation Intervention.\\nContrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.\\nGuided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.\\nSimilarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.\\nHALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.\\nOthers.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.\\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.\\n5.4.2 Post-hoc Correction\\nPost-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].\\nWoodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.\\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.\\nSimilar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.\\nLogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.\\n6 CHALLENGES AND FUTURE DIRECTIONS\\nThe research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.\\n6.1 Data-centric Challenges and Innovations\\nThe reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.\\n6.2 Cross-modal Alignment and Consistency\\nThe key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.\\n6.3 Advancements in Model Architecture\\nDespite recent advancements in model architectures of LLMs and MLLMs, designing effective architectures specifically tailored to hallucination remains a challenge.\\nDeveloping advanced model architectures capable of capturing complex linguistic structures and generating coherent and contextually relevant output based on input visual content is essential for improving the performance of MLLMs.\\nFuture research can explore innovative architectural designs based on identified causes of hallucination.\\nThis includes developing stronger visual perception models, innovative cross-modal interaction modules capable of transferring cross-modal information seamlessly, and novel large language model architectures faithful to input visual content and text instructions, etc.\\n6.4 Establishing Standardized Benchmarks\\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in assessing the degree of hallucination in MLLMs.\\nIn Table 1, it can be observed that there is a variety of evaluation benchmarks, but a lack of unified standards.\\nAmong them, one of the most popular benchmarks might be POPE [69], which employs a ’Yes-or-No’ evaluation protocol.\\nHowever, this binary-QA manner does not align with how humans use MLLMs.\\nAccordingly, some benchmarks specifically evaluate the hallucination of MLLMs in the (free-form) generative context.\\nYet, they often rely on external models, such as vision expert models or other LLMs, which limits their widespread application.\\nMoving forward, future research can investigate standardized benchmarks that are theoretically sound and easy to use.\\nOtherwise, research on methods to mitigate hallucinations may be built on an incorrect foundation.\\n6.5 Reframing Hallucination as a Feature\\nRecently, discussions on social media [56] have suggested that hallucination can be regarded as an inherent feature of LLMs and MLLMs.\\nThe models are like dream machines.\\nHuman users direct their dreams with prompts.\\nThe prompts start the dream, and based on the model’s hazy recollection of its training documents, most of the time the result goes someplace useful.\\nIt’s only when the dreams enter deemed factually incorrect territory that we label them as ’hallucinations’.\\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications presents exciting opportunities for enhancing user experiences and enabling new use cases.\\nAs humans are the end-users of these models, the primary goal is to enrich human user experiences.\\nFuture research may switch the optimization objective from specific cross-modal benchmarks to human experience.\\nFor example, Some content may cause hallucinations but will not affect the user experience, while some content may.\\nAlternatively, integrating hallucination to inspire more creative ideas in real-world applications could also be intriguing.\\n6.6 Enhancing Interpretability and Trust\\nExisting methods for hallucination mitigation are primarily based on empirical observations of specific patterns, such as skipping the ‘\\\\n’ token and penalizing over-trust tokens.\\nHowever, despite the impressive improvements achieved on specific benchmarks, understanding the underlying mechanisms and decision-making processes remains challenging.\\nFuture research should focus on developing techniques for interpreting and explaining the generation process of MLLMs, thereby providing insights into the factors influencing hallucinated content.\\nThis includes investigating methods for visualizing model internals, identifying salient features and linguistic patterns, and tracing the generation process from input to output.\\nEnhancing the interpretability of MLLMs will not only improve our understanding of model behavior but also enable users to better assess hallucinated content in practical applications.\\n6.7 Navigating the Ethical Landscape\\nAs MLLMs become increasingly proficient at generating realistic text, ethical considerations surrounding the use of generated content become paramount.\\nEspecially in the context of hallucination, the generated response may contain severely concerning ethical content, amplifying the importance of the problem.\\nAddressing ethical concerns related to misinformation, bias, privacy, and societal impact is crucial for promoting responsible AI practices in the development and deployment of MLLMs.\\nIn addition to addressing typical object hallucination, future research on MLLM hallucinations should prioritize ethical considerations throughout the entire lifecycle of MLLM development, from data collection and model training to deployment and evaluation.\\n7 CONCLUSION\\nBased on powerful large language models, multimodal large language models demonstrate remarkable performance across various multimodal tasks.\\nHowever, the phenomenon of hallucination presents a significant challenge to the practical applications of MLLMs, giving rise to undeniable concerns about safety, reliability, and trustworthiness.\\nIn this comprehensive survey, we conducted a thorough examination of hallucinations within multimodal large language models, focusing on their underlying causes, evaluation metrics, benchmarks, and mitigation methods.\\nDespite considerable progress, hallucination remains a complex and persistent concern that warrants ongoing investigation.\\nThe challenge of hallucination in multimodal large language models remains compelling, requiring continuous scrutiny and innovation.\\nIn light of these challenges, we have outlined several promising future directions in this burgeoning domain.\\nThrough navigating the intricate landscape of hallucinations, we aim for this survey to serve as a foundational resource for addressing the complexities of hallucination phenomena in MLLMs.\\nWe envision this survey empowering researchers and practitioners to dedicate efforts to advancing research and developing robust solutions in this vital area of study.',\n",
       " '5.1 Data\\nAs discussed in the section on hallucination causes 3, data is one of the primary factors inducing hallucination in MLLMs.\\nFor mitigating hallucination, recent works make attempts on data, including introducing negative data [73], introducing counterfactual data [117], and reducing noise and errors in existing dataset [105, 120].\\nLRV-Instruction [73] LRV-Instruction is proposed to address the issue that existing instruction tuning data primarily focus on positive instruction samples, leading the model to consistently answer ’Yes’.\\nLRV-Instruction is designed to include both positive and negative instructions for more robust visual instruction tuning, where the negative instructions include: 1) ’Nonexistent Object Manipulation’: introducing nonexistent objects, activities, attributes, and interactions; 2) ’Existent Object Manipulation’: manipulating existent objects with inconsistent attributes; 3) ’Knowledge Manipulation’: manipulating knowledge in instructions.\\nHalluciDoctor [117] This paper addresses the object hallucination problem in MLLMs by calibrating the instruction-tuning dataset.\\nThe calibration is conducted from two perspectives.\\nFirstly, it develops a hallucination detection pipeline via consistency cross-checking of multiple MLLMs.\\nBased on the detection result, the hallucinated content can be eliminated.\\nSecondly, this work observes that long-tail distribution and object co-occurrence in the training data are two primary factors of hallucination.\\nThus, a counterfactual visual instruction generation strategy is proposed to expand the dataset.\\nUsing the proposed methods, the instruction tuning data can be balanced and experience reduced hallucination.\\nMLLMs trained on the calibrated dataset are shown to be less prone to hallucination.\\nReCaption [105] This work proposes a framework called ReCaption to rewrite the text captions of existing image-text pairs in datasets.\\nThe framework comprises two steps: 1) keyword extraction, which extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which employs an LLM to generate sentences based on the extracted keywords.\\nUltimately, the framework produces a set of high-quality image-caption pairs.\\nExperiment results show that the model trained on the rewritten caption dataset has higher accuracy on certain benchmarks, such as the POPE benchmark [69].\\nDespite the performance improvement, the question of why rewritten captions can reduce hallucination remains an open problem.\\nEOS Decision [120] Previous work [137] provides an observation that hallucination tends to occur with objects positioned later in the generated descriptions.\\nIntuitively, an ideal scenario is that the MLLM can terminate the generation process in a timely manner.\\nThis idea is thoroughly explored in the work of [120] from the perspective of end-of-sequence (EOS) decision.\\nThe key insight is that the training data may exceed the perception limit of the MLLM.\\nWhen trained with such data, the model may attempt to fit the detail level and length distribution of ground truth captions.\\nHowever, it may risk expressing details that it cannot discern from the image, and therefore exhibit hallucinations.\\nThus, the authors explored approaches to enhance the model’s end-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the perception limit.\\nRegarding data, this work proposes a data filtering strategy to eliminate harmful training data that could impair the model’s ability to end sequences.',\n",
       " '5.2 Model\\n5.2.1 Scale-up Resolution\\nEnhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.\\n5.2.2 Versatile Vision Encoders\\nSeveral studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.\\n5.2.3 Dedicated Module\\nFollowing our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.',\n",
       " '5.2.1 Scale-up Resolution\\nEnhancing the perception ability of MLLMs has been shown to improve their overall performance and reduce hallucination [14, 74, 75, 123].\\nOne important update when upgrading from LLaVA [75] to LLaVA-1.5 [74] is to scale up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement.\\nQwenVL [2] has shown the effectiveness of gradually enlarging image resolution from 224 × 224 to 448 × 448.\\nInternVL [2] scales up the vision encoder to 6 billion parameters, enabling processing of high-resolution images.\\nRegarding hallucination, HallE-Switch [123] has investigated the impact of vision encoder resolution on its proposed CCEval benchmark.\\nAmong the three studied vision encoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results in lower degrees of hallucination.\\nThese works indicate that scaling up vision resolution is a straightforward yet effective solution.',\n",
       " '5.2.2 Versatile Vision Encoders\\nSeveral studies [38, 49, 98] have investigated vision encoders for MLLMs.\\nTypically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs thanks to its remarkable ability to extract semantic-rich features.\\nHowever, CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT [10].\\nTherefore, recent studies have proposed complementing this information loss by incorporating visual features from other vision encoders.\\nThe work of [98] proposes mixing features from CLIP ViT and DINO ViT.\\nSpecifically, it experimented with additive and interleaved features.\\nBoth settings show that there is a trade-off between the two types of features.\\nA more dedicated mechanism is needed.\\nConcurrently, a visual expert-based model proposed in [38] aims to mitigate the information loss caused by the CLIP image encoder.\\nInstead of merely mixing features, this paper enhances the visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two pivotal modules: multi-task encoders and the structural knowledge enhancement module.\\nThe multitask encoders are dedicated to integrating various types of latent visual information extracted by multiple visual encoders.\\nAdditionally, the structural knowledge enhancement module is designed to utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from visual inputs.\\nFollowing the approach of the structural knowledge enhancement module in [38], another line of research investigates the utilization of vision tool models to enhance the perception of MLLMs.\\nVCoder [49] utilizes additional perception formats, such as segmentation masks and depth maps, to enhance the object identification ability of the MLLM.\\nAnother work [54] ensembles additional object detection and optical-character recognition models into the MLLM architecture.\\nIt also explores various ways to integrate this information, including training-free infusion, LoRA [41] augmented retraining, and LoRA augmented finetuning.',\n",
       " '5.2.3 Dedicated Module\\nFollowing our previous discussion, the parametric knowledge embedded in the LLM is identified as a significant factor leading to hallucination, directing the generation to be based on language knowledge instead of visual content.\\nTo address this issue, the work of [123] proposes training a dedicated \"switch\" module, termed HallE-Switch, which controls the extent of parametric knowledge within detailed captions.\\nThe detailed implementation is inspired by LMswitch [33], which involves adding a control parameter 𝜖 serving as a \"switching value\".\\nThe switch module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets.\\nDuring inference, addressing hallucination can be attempted by tuning the control parameter 𝜖.',\n",
       " '5.3 Training\\n5.3.1 Auxiliary supervision.\\nThe primary supervision signal of training MLLMs is language modeling loss (implemented as CrossEntropyLoss) in both pre-training and finetuning stage.\\nHowever, such supervision may not be sufficient to process the rich information encoded in the visual content.\\nAccordingly, the work of [16] constructs a fine-grained vision instruction dataset based on Panoptic Scene Graph (PSG), called Relation-Associated Instruction (RAI-30k).\\nIn addition to standard dialogues, each instruction in RAI-30k is associated with a relation annotation in PSG, which includes mask annotations for related instances.\\nWith these additional annotations, it further supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [57], guiding MLLMs to focus on highly-related image content.\\nWith the additional supervision from the mask prediction loss, MLLMs are encouraged to extract features that can better represent these crucial instances, thus generating more accurate responses and mitigating vision hallucination.\\nThe intuitive idea of supervising MLLMs with grounding shows promising performance in mitigating hallucination.\\nAnother line of work analyzes the training loss from the perspective of embedding space distribution.\\nAs introduced earlier, popular MLLMs typically project the encoded vision features into the input space of a specific LLM.\\nA recent work, HACL [52], argues that an ideal projection should blend the distribution of visual and textual embeddings.\\nHowever, despite visual projection, a significant modality gap exists between textual and visual tokens, suggesting that the current learned interfaces are not effective in mapping visual representations into the textual representation space of LLMs.\\nThis issue potentially exacerbates the tendency for MLLMs to generate more hallucinations.\\nTherefore, HACL proposes enhancing the alignment between visual and textual representations through contrastive loss.\\nTexts with hallucinations are used as hard negative examples for image anchors.\\nThe loss pulls representations of non-hallucinating text and visual samples closer while pushing representations of non-hallucinating and hallucinative text apart.\\nExperiment results show that this method not only reduces hallucination but also enhances performance on other popular benchmarks.\\nRecalling the work of EOS Decision [120], to teach the model to terminate the generation process properly, this work also designs a learning objective, termed Selective EOS Supervision, in addition to the data filtering strategy.\\nThis is achieved by simply modifying the Maximum Likelihood Estimation (MLE), enabling the model to mitigate hallucination through learning from regular instruction data.\\n5.3.2 Reinforcement Learning\\nReinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.\\nAutomatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.\\nReinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.\\nA concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.\\nReinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct\\nPreference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.\\nLLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.\\nSimilarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.\\nAnother similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.\\n5.3.3 Unlearning\\nUnlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.',\n",
       " '5.3.2 Reinforcement Learning\\nReinforcement learning (RL) is introduced to train MLLMs for mitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based Optimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from Human Feedback.\\nAutomatic Metric-based Optimization.\\nMotivated by the limitation of LLMs (and MLLMs) training, which is unable to optimize at the sequence level, the MOCHa [5] framework is proposed to apply reinforcement learning.\\nThis work aims to improve the accuracy and relevance of image captioning, thereby reducing hallucination.\\nThe framework introduces three metric-based objectives to guide the reinforcement learning process for image captioning: 1) Natural Language Inference (NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2) BERTScore [127] for semantic adequacy, assessing the relevance and richness of the description; and 3) Kullback–Leibler (KL) divergence for regularization, which constrains the model to stay close to its initial policy.\\nThe framework incorporates these objectives into a multi-objective reward function for reinforcement learning.\\nSubsequently, the proximal policy optimization reinforcement learning algorithm is employed to maximize the expected reward.\\nBy promoting the creation of accurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be mitigated.\\nReinforcement Learning from AI Feedback (RLAIF).\\nHA-DPO [133] addresses hallucination as a preference selection problem by training models to prioritize accurate responses over hallucinatory ones.\\nTo achieve this goal, HA-DPO initially constructs a high-quality dataset.\\nSpecifically, it first utilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect whether these descriptions contain hallucinations.\\nIf hallucinations are detected, the descriptions are rewritten.\\nThus, HA-DPO constructs a dataset that includes both accurate descriptions (positive samples) and hallucinatory descriptions (negative samples).\\nHA-DPO then trains the model using these sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions.\\nThis goal is achieved through direction preference optimization (DPO), which optimizes a specific loss function designed to maximize the model’s preference for positive samples while minimizing its preference for negative samples.\\nA concurrent work, Silkie [68], introduces a similar approach of utilizing preference-based reinforcement learning to enhance the faithfulness of MLLMs.\\nSpecifically, it emphasizes the concept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more robust MLLM, i.e., GPT-4V [83].\\nResponses are first generated by models from 12 MLLMs, and then assessed by GPT-4V.\\nThe constructed dataset, termed as VLFeedback, contains preferences distilled from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based methods.\\nThese methods rely on the traditional preference data generation process in LLMs, where both preferred and dispreferred responses may potentially be incorrect.\\nTherefore, this work proposes the Preference Optimization in VLLMwith AI-Generated Dispreferences (POVID) framework, aiming to exclusively generate dispreferred feedback data using AI models.\\nThe dispreferred data is generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and 2) provoking inherent hallucination by introducing noise into MLLMs.\\nIn the DPO optimization framework, the ground-truth multimodal instructions serves as the preferred answers.\\nReinforcement Learning from Human Feedback (RLHF).\\nHalDetect [32] first introduces the MHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.\\nIt then proposes a multimodal reward model to detect hallucinations generated by MLLMs.\\nThe reward model is trained on the M-HalDetect dataset to identify hallucinations in the generated text.\\nTo utilize the trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct\\nPreference Optimization (FDPO).\\nFDPO uses fine-grained preferences from individual examples to directly reduce hallucinations in generated text by enhancing the model’s ability to distinguish between accurate and inaccurate descriptions.\\nLLaVA-RLHF [96] also try to involve human feedback to mitigate hallucination.\\nIt extends the RLHF paradigm from the text domain to the task of vision-language alignment, where human annotators were asked to compare two responses and pinpoint the hallucinated one.\\nThe MLLM is trained to maximize the human reward simulated by an reward model.\\nTo address the potential issue of reward hacking, i.e., achieving high scores from the reward model does not necessarily lead to improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\\nThis algorithm calibrates the reward signals by augmenting them with additional information such as image captions.\\nSimilarly, RLHF-V [119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect human feedback in the form of fine-grained segment-level corrections, providing a clear, dense, and fine-grained human preference.\\n2) at the method level, it proposes dense direct preference optimization (DDPO) that directly optimizes the policy model against dense and fine-grained segment-level preference.\\nAnother similar work, ViGoR [110], also designs a fine-grained reward model to update pretrainedMLLMs, aiming to improve visual grounding and reduce hallucination.\\nThe reward modeling in this work encompasses both human preferences and automatic metrics.\\nSpecifically, it collects human judgment and preferences for the responses generated by MLLMs by asking crowd-workers to provide fine-grained feedback at the sentence level.\\nThe collected human preference data is used to train a reward model.\\nAdditionally, it leverages advanced vision perception models to automatically score the grounding and fidelity of the text generated by an MLLM.\\nBoth sources are combined into a single reward score during the reinforcement learning procedure.',\n",
       " '5.3.3 Unlearning\\nUnlearning refers to a technique designed to induce a model to ’forget’ specific behaviors or data, primarily through the application of gradient ascent methods [9].\\nRecently, unlearning for LLMs has been receiving increasing attention [50], effectively eliminating privacy vulnerabilities in LLMs.\\nIn the context of MLLMs, a recent work [109] introduces the Efficient Fine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the hallucination problem.\\nSpecifically, it utilizes the CLIP model to construct a dataset comprised of both positive samples and negative (hallucinated) samples.\\nThe training loss is applied separately for positive and negative at the sub-sentence level.\\nTo the best of our knowledge, EFUF [109] is the first and only work that applies the unlearning framework to the task of hallucination mitigation, opening up a new path for future research.',\n",
       " '5.4 Inference\\n5.4.1 Generation Intervention.\\nContrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.\\nGuided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.\\nSimilarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.\\nHALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.\\nOthers.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.\\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.\\n5.4.2 Post-hoc Correction\\nPost-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].\\nWoodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.\\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.\\nSimilar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.\\nLogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.\\n6 CHALLENGES AND FUTURE DIRECTIONS\\nThe research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.\\n6.1 Data-centric Challenges and Innovations\\nThe reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.\\n6.2 Cross-modal Alignment and Consistency\\nThe key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.',\n",
       " '5.4.1 Generation Intervention.\\nContrastive Decoding.\\nVCD (Visual Contrastive Decoding) [64] is designed to suppress the statistical biases and language priors in MLLMs during the decoding phase.\\nThe main assumption of VCD is that a distorted visual input would lead to text responses with more biases and priors.\\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD aims to effectively reduce the over-reliance on statistical bias and language priors.\\nSpecifically, the decoding probability distribution is calibrated using the reference (distorted) distribution.\\nFollowing the same idea of contrastive decoding, IBD [139] proposes an image-biased decoding strategy.\\nSpecifically, IBD involves computing a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model, which focuses more on the image information.\\nThe image-based model is created by modifying the attention weight matrix structure within the original model, without altering its parameters.\\nThis approach emphasizes the knowledge of the image-biased model and diminishes that of the original model, which may be text-biased.\\nThus, it encourages the extraction of correct content while suppressing hallucinations resulting from textual over-reliance.\\nGuided Decoding.\\nMARINE [131] proposes a training-free approach.\\nIt employs an additional vision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\\nSpecifically, it innovatively adapts the classifier-free guidance [40] technique to implement guided decoding, showing promising performance in emphasizing the detected objects while reducing hallucination in the text response.\\nSimilarly, GCD [24] devises a CLIP-Guided Decoding (GCD) approach.\\nIt first verifies that CLIPScore [88] can effectively distinguish between hallucinated and non-hallucinated sentences through a series of studies across different models and datasets.\\nBased on this conclusion, it further recalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which designs a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that are less likely to be hallucinated, and 2) guided sentence generation, which generates responses based on this scoring.\\nThis is implemented in a similar way to beam search but at the sentence level.\\nHALC [15] provides a key insight that when decoding a specific token in the MLLM, identifying a token-wise optimal visual context to provide the most informative visual grounding can effectively reduce hallucination.\\nVisual context refers to the visual tokens that can be grounded from the generated text response.\\nAn oracle study showed that decoding from the provided optimal visual contexts eliminates over 84.5% of hallucinations.\\nBased on the insight and observation, the authors designed mechanisms to locate the fine-grained visual information to correct each generated token that might be hallucinating.\\nThis is essentially a visual content-guided decoding strategy.\\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that utilizes a visual matching score to steer the generation of the final outputs, balancing both object hallucination mitigation and text generation quality.\\nOthers.\\nThe work of OPEAR [45] makes an interesting observation that most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all the previous tokens.\\nSuch a partial over-trust inclination results in neglecting image tokens and describing the image content with hallucination.\\nBased on this observation, a decoding method for MLLMs grounded in an Over-trust Penalty and a Retrospection-Allocation strategy is proposed.\\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding process to mitigate the over-trust issue.\\nAdditionally, to handle the hard cases that cannot be addressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed to retrospect the presence of summary tokens in the previously generated tokens and reallocate the token selection if necessary.\\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered by paragraph break ‘\\\\n\\\\n’ [36].\\nBased on this observation, this work proposes two simple methods to reduce hallucination by avoiding generating ‘\\\\n’ during generation.\\nFirst, intuitively, users can design the prompt to instruct the model to output responses within one paragraph, avoiding ‘\\\\n’.\\nBesides, the authors tried to alter the output logits during generation by manually lowering the probability of generating ‘\\\\n’.\\nExperimental results show that this simple strategy can alleviate hallucination on popular benchmarks.',\n",
       " '5.4.2 Post-hoc Correction\\nPost-hoc correction refers to first allowing the MLLM to generate a text response and then identifying and eliminating hallucinating content, resulting in less hallucinated output.\\nThis is usually achieved by grounding on visual content [114], pre-trained revisior [137], and self-revision [63].\\nWoodpecker [114] is an early attempt on hallucination detection and correction.\\nSimilar to how a woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated text.\\nThe key idea of Woodpecker is to extract key concepts from the generated text and validate them using visual content.\\nSubsequently, the hallucinated concepts can be detected and corrected accordingly.\\nSpecifically, it consists of five stages: 1)Key concept extraction identifies the main objects mentioned in the generated sentences; 2) Question formulation asks questions around the extracted objects; 3) Visual knowledge validation answers the formulated questions via expert models; 4) Visual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge base; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence under the guidance of the visual knowledge base.\\nWoodpecker is a training-free method, where each component can be implemented using either hand-crafted rules or off-the-shelf pre-trained models.\\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\\nSpecifically, inspired by denoising autoencoders [101], which are designed to reconstruct clean data from corrupted input, LURE [137] employs a hallucination revisor that aims to transform potentially hallucinatory descriptions into accurate ones.\\nTo train such a revisor model, a dataset has been constructed.\\nEach example in this dataset consists of an image accompanied by a hallucinatory description, with the correct description serving as the target output.\\nThe hallucinatory descriptions are generated by modifying the accurate descriptions using GPT-3.5.\\nThese adjustments are guided by factors related to object hallucination, including co-occurrence, object uncertainty, and object position.\\nAfter that, the authors fine-tuned an MLLM using this dataset to serve as a revisor, which is used as an additional step for rectifying the output of an MLLM during generation.\\nSimilar idea has also been explored in Volcano [63].\\nIt introduces a self-revising mechanism to reduce hallucination.\\nIt consists of four stages: 1) generate initial response; 2) generate feedback for the initial response; 3) revise the response using this feedback; 4) compare the responses before and after revision to decide which one is better.\\nStages 2-4 are repeated iteratively.\\nTo provide better feedback and decision-making, the model is fine-tuned on a curated dataset.\\nThe dataset is organized using ChatGPT.\\nLogicCheckGPT [108] is a more recent self-revising-based hallucination mitigation method.\\nUnlike Volcano [63], which revises the generated response with the help of general feedback, LogicCheckGPT delves into the logical consistency of MLLMs’ responses.\\nSpecifically, the approach can be formulated into two stages: the first stage involves inquiring attributes of objects, followed by inquiring objects based on attributes.\\nWhether their responses can form a logical closed loop serves an indicator of object hallucination.\\nIf the ratio of closed loops to the total number of questions exceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.\\n6 CHALLENGES AND FUTURE DIRECTIONS\\nThe research of hallucination in MLLMs is still at early stage, remaining a variety of research problems to be explored.\\nIn this section, we delve into the challenges and future directions of this pivotal domain.\\n6.1 Data-centric Challenges and Innovations\\nThe reliance of MLLMs on large volumes of data presents significant challenges in terms of data quality, diversity, and bias.\\nIn Sec. 3.1, previous works have identified several core issues that may cause hallucination.\\nIn order to improve the accuracy and reliability of hallucinated content, it is crucial to ensure that MLLMs have access to high-quality and diverse training data.\\nFuture research should focus on developing techniques for data collection, augmentation, and calibration.\\nFirstly, collecting enough data at the initial stage is crucial to address the data scarcity issue and increase data diversity.\\nSecondly, data augmentation is an effective solution to further expand the size of data.\\nFinally, exploring methods for re-calibrating existing datasets is crucial.\\nThis includes eliminating biases, promoting diversity and inclusivity, and mitigating other potential issues that may induce hallucinations.\\n6.2 Cross-modal Alignment and Consistency\\nThe key challenge of multimodal hallucination is the cross-modal consistency issue.\\nEnsuring that generated content remains consistent and contextually relevant to the input modality requires sophisticated techniques for capturing and modeling cross-modal relationships.\\nThe direction of cross-modal alignment encompasses both MLLMs training and hallucination evaluation.\\nRegarding training, future research should explore methods for aligning representations between different modalities.\\nAchieving this goal may involve designing more advanced architectures, introducing additional learning objectives [52], or incorporating diverse supervision signals [16].\\nRegarding evaluation, cross-modal consistency checking has been a long-standing topic, ranging from multimodal understanding [66, 88] to text-to-image generation [13, 17].\\nDrawing on proven experiences from these domains to improve the assessment of MLLM hallucination, or unifying them into an overall framework, may be promising research directions.',\n",
       " '6.3 Advancements in Model Architecture\\nDespite recent advancements in model architectures of LLMs and MLLMs, designing effective architectures specifically tailored to hallucination remains a challenge.\\nDeveloping advanced model architectures capable of capturing complex linguistic structures and generating coherent and contextually relevant output based on input visual content is essential for improving the performance of MLLMs.\\nFuture research can explore innovative architectural designs based on identified causes of hallucination.\\nThis includes developing stronger visual perception models, innovative cross-modal interaction modules capable of transferring cross-modal information seamlessly, and novel large language model architectures faithful to input visual content and text instructions, etc.',\n",
       " '6.4 Establishing Standardized Benchmarks\\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in assessing the degree of hallucination in MLLMs.\\nIn Table 1, it can be observed that there is a variety of evaluation benchmarks, but a lack of unified standards.\\nAmong them, one of the most popular benchmarks might be POPE [69], which employs a ’Yes-or-No’ evaluation protocol.\\nHowever, this binary-QA manner does not align with how humans use MLLMs.\\nAccordingly, some benchmarks specifically evaluate the hallucination of MLLMs in the (free-form) generative context.\\nYet, they often rely on external models, such as vision expert models or other LLMs, which limits their widespread application.\\nMoving forward, future research can investigate standardized benchmarks that are theoretically sound and easy to use.\\nOtherwise, research on methods to mitigate hallucinations may be built on an incorrect foundation.',\n",
       " '6.5 Reframing Hallucination as a Feature\\nRecently, discussions on social media [56] have suggested that hallucination can be regarded as an inherent feature of LLMs and MLLMs.\\nThe models are like dream machines.\\nHuman users direct their dreams with prompts.\\nThe prompts start the dream, and based on the model’s hazy recollection of its training documents, most of the time the result goes someplace useful.\\nIt’s only when the dreams enter deemed factually incorrect territory that we label them as ’hallucinations’.\\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications presents exciting opportunities for enhancing user experiences and enabling new use cases.\\nAs humans are the end-users of these models, the primary goal is to enrich human user experiences.\\nFuture research may switch the optimization objective from specific cross-modal benchmarks to human experience.\\nFor example, Some content may cause hallucinations but will not affect the user experience, while some content may.\\nAlternatively, integrating hallucination to inspire more creative ideas in real-world applications could also be intriguing.',\n",
       " '6.6 Enhancing Interpretability and Trust\\nExisting methods for hallucination mitigation are primarily based on empirical observations of specific patterns, such as skipping the ‘\\\\n’ token and penalizing over-trust tokens.\\nHowever, despite the impressive improvements achieved on specific benchmarks, understanding the underlying mechanisms and decision-making processes remains challenging.\\nFuture research should focus on developing techniques for interpreting and explaining the generation process of MLLMs, thereby providing insights into the factors influencing hallucinated content.\\nThis includes investigating methods for visualizing model internals, identifying salient features and linguistic patterns, and tracing the generation process from input to output.\\nEnhancing the interpretability of MLLMs will not only improve our understanding of model behavior but also enable users to better assess hallucinated content in practical applications.',\n",
       " '6.7 Navigating the Ethical Landscape\\nAs MLLMs become increasingly proficient at generating realistic text, ethical considerations surrounding the use of generated content become paramount.\\nEspecially in the context of hallucination, the generated response may contain severely concerning ethical content, amplifying the importance of the problem.\\nAddressing ethical concerns related to misinformation, bias, privacy, and societal impact is crucial for promoting responsible AI practices in the development and deployment of MLLMs.\\nIn addition to addressing typical object hallucination, future research on MLLM hallucinations should prioritize ethical considerations throughout the entire lifecycle of MLLM development, from data collection and model training to deployment and evaluation.\\n7 CONCLUSION\\nBased on powerful large language models, multimodal large language models demonstrate remarkable performance across various multimodal tasks.\\nHowever, the phenomenon of hallucination presents a significant challenge to the practical applications of MLLMs, giving rise to undeniable concerns about safety, reliability, and trustworthiness.\\nIn this comprehensive survey, we conducted a thorough examination of hallucinations within multimodal large language models, focusing on their underlying causes, evaluation metrics, benchmarks, and mitigation methods.\\nDespite considerable progress, hallucination remains a complex and persistent concern that warrants ongoing investigation.\\nThe challenge of hallucination in multimodal large language models remains compelling, requiring continuous scrutiny and innovation.\\nIn light of these challenges, we have outlined several promising future directions in this burgeoning domain.\\nThrough navigating the intricate landscape of hallucinations, we aim for this survey to serve as a foundational resource for addressing the complexities of hallucination phenomena in MLLMs.\\nWe envision this survey empowering researchers and practitioners to dedicate efforts to advancing research and developing robust solutions in this vital area of study.',\n",
       " 'ACKNOWLEDGMENTS\\nThis project is supported by Mike Zheng Shou’s Start-Up Grant from NUS.',\n",
       " 'REFERENCES\\n[1] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\\n2023.\\nOpenflamingo: An open-source framework for training large autoregressive vision-language models.\\narXiv preprint arXiv:2308.01390 (2023).\\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\\n2023.\\nQwen-vl: A frontier large vision-language model with versatile abilities.\\narXiv preprint arXiv:2308.12966 (2023).\\n[3] Satanjeev Banerjee and Alon Lavie.\\n2005.\\nMETEOR: An automatic metric for MT evaluation with improved correlation with human judgments.\\nIn Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization.\\n65–72.\\n[4] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar.\\n2023.\\nFuyu-8B: A Multimodal Architecture for AI Agents.\\nhttps://www.adept.ai/blog/fuyu-8b [5] Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, and Hadar Averbuch-Elor.\\n2023.\\nMOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations.\\narXiv preprint arXiv:2312.03631 (2023).\\n[6] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al.\\n2023.\\nImproving image generation with better captions.\\nComputer Science.\\nhttps://cdn.\\nopenai.\\ncom/papers/dall-e-3.\\npdf 2, 3 (2023), 8.\\n[7] Ralph Allan Bradley and Milton E Terry.\\n1952.\\nRank analysis of incomplete block designs: I. The method of paired comparisons.\\nBiometrika 39, 3/4 (1952), 324–345.\\nhttps://www.jstor.org/stable/2334029 [8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\\n2020.\\nLanguage Models are Few-Shot Learners.\\nIn Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html [9] Yinzhi Cao and Junfeng Yang.\\n2015.\\nTowards making systems forget with machine unlearning.\\nIn 2015 IEEE symposium on security and privacy.\\nIEEE, 463–480.\\n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\\n2021.\\nEmerging properties in self-supervised vision transformers.\\nIn Proceedings of the IEEE/CVF international conference on computer vision.\\n9650–9660.\\n[11] Sungguk Cha, Jusung Lee, Younghyun Lee, and Cheoljong Yang.\\n2024.\\nVisually Dehallucinative Instruction Generation:\\nKnow What You Don’t Know.\\narXiv preprint arXiv:2402.09717 (2024).\\n[12] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.\\n2023.\\nShikra: Unleashing Multimodal LLM’s Referential Dialogue Magic.\\narXiv preprint arXiv:2306.15195 (2023).\\n[13] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Jinjie Gu, and Huajun Chen.\\n2024.\\nUnified Hallucination Detection for Multimodal Large Language Models.\\narXiv preprint arXiv:2402.03190 (2024).\\n[14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al.\\n2023.\\nInternvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.\\narXiv preprint arXiv:2312.14238 (2023).\\n[15] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou.\\n2024.\\nHALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding.\\narXiv preprint arXiv:2403.00425 (2024).\\n[16] Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, JinqiaoWang, andMing Tang.\\n2023.\\nMitigating Hallucination in Visual Language Models with Visual Supervision.\\narXiv preprint arXiv:2311.16479 (2023).\\n[17] Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi PontTuset, and Su Wang.\\n2023.\\nDavidsonian scene graph: Improving reliability in fine-grained evaluation for text-image generation.\\narXiv preprint arXiv:2310.18235 (2023).\\n[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,\\nLiam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.\\n2023.\\nPaLM: Scaling Language Modeling with Pathways.\\nJ. Mach.\\nLearn.\\nRes.\\n24 (2023), 240:1–240:113. http://jmlr.org/papers/v24/22-1144.html [19] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\\n2017.\\nDeep Reinforcement Learning from Human Preferences.\\nIn Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.).\\n4299–4307.\\nhttps://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html [20] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu.\\n2023.\\nA Survey of Chain of Thought Reasoning: Advances, Frontiers and Future.\\nArXiv preprint abs/2309.15402 (2023).\\nhttps://arxiv.org/abs/2309.15402 [21] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao.\\n2023.\\nHolistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges.\\narXiv preprint arXiv:2311.03287 (2023).\\n[22] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.\\n2023.\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.\\narXiv:2305.06500 [cs.CV] [23] Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al.\\n2023.\\nLanguage modeling is compression.\\narXiv preprint arXiv:2309.10668 (2023).\\n[24] Ailin Deng, Zhirui Chen, and BryanHooi.\\n2024.\\nSeeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding.\\narXiv preprint arXiv:2402.15300 (2024).\\n[25] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\\n2022.\\nGLM: General Language Model Pretraining with Autoregressive Blank Infilling.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).\\n320–335.\\n[26] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al.\\n2023.\\nLlama-adapter v2: Parameter-efficient visual instruction model.\\narXiv preprint arXiv:2304.15010 (2023).\\n[27] Ross Girshick.\\n2015.\\nFast r-cnn.\\nIn Proceedings of the IEEE international conference on computer vision.\\n1440–1448.\\n[28] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.\\n2023.\\nMultiModal-GPT: A Vision and Language Model for Dialogue with Humans.\\narXiv:2305.04790 [cs.CV] [29] Google.\\n2023.\\nBard.\\nhttps://bard.google.com/ [30] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.\\n2017.\\nMaking the v in vqa matter: Elevating the role of image understanding in visual question answering.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition.\\n6904–6913.\\n[31] Tianrui Guan, Fuxiao Liu, Xiyang Wu Ruiqi Xian Zongxia Li, Xiaoyu Liu Xijun Wang, Lichang Chen Furong Huang Yaser Yacoob, and Dinesh Manocha Tianyi Zhou.\\n2023.\\nHALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination & Visual Illusion in Large Vision-Language Models.\\narXiv e-prints (2023), arXiv–2310.\\n[32] Anisha Gunjal, Jihan Yin, and Erhan Bas.\\n2023.\\nDetecting and preventing hallucinations in large vision language models.\\nArXiv preprint abs/2308.06394 (2023).\\nhttps://arxiv.org/abs/2308.06394 [33] Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji.\\n2023.\\nLm-switch: Lightweight language model conditioning in word embedding space.\\narXiv preprint arXiv:2305.12798 (2023).\\n[34] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al.\\n2023.\\nImagebind-llm: Multi-modality instruction tuning.\\narXiv preprint arXiv:2309.03905 (2023).\\n[35] Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, and Tong Zhang.\\n2024.\\nThe Instinctive Bias: Spurious Images lead to Hallucination in MLLMs.\\narXiv preprint arXiv:2402.03757 (2024).\\n[36] Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, and Mike Zheng Shou.\\n2024.\\nSkip \\\\𝑛: A simple method to reduce hallucination in Large Vision-Language Models.\\narXiv preprint arXiv:2402.01345 (2024).\\n[37] KaimingHe, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.\\n2017.\\nMask r-cnn.\\nIn Proceedings of the IEEE international conference on computer vision.\\n2961–2969.\\n[38] Xin He, Longhui Wei, Lingxi Xie, and Qi Tian.\\n2024.\\nIncorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models.\\narXiv preprint arXiv:2401.03105 (2024).\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021.\\nMeasuring Massive Multitask Language Understanding.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nhttps://openreview.net/forum?id=d7KBjmI3GmQ [40] Jonathan Ho and Tim Salimans.\\n2022.\\nClassifier-free diffusion guidance.\\narXiv preprint arXiv:2207.12598 (2022).\\n[41] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021.\\nLora: Low-rank adaptation of large language models.\\narXiv preprint arXiv:2106.09685 (2021).\\n[42] Hongyu Hu, Jiyuan Zhang, Minyi Zhao, and Zhenbang Sun.\\n2023.\\nCiem: Contrastive instruction evaluation method for better instruction tuning.\\narXiv preprint arXiv:2309.02301 (2023).\\n[43] Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, and Zhuowen Tu.\\n2023.\\nBliva: A simple multimodal llm for better handling of text-rich visual questions.\\narXiv preprint arXiv:2308.09936 (2023).\\n[44] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al.\\n2023.\\nA survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.\\narXiv preprint arXiv:2311.05232 (2023).\\n[45] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu.\\n2023.\\nOPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation.\\narXiv preprint arXiv:2311.17911 (2023).\\n[46] Wen Huang, Hongbin Liu, Minxin Guo, and Neil Zhenqiang Gong.\\n2024.\\nVisual Hallucinations of Multi-modal Large Language Models.\\narXiv preprint arXiv:2402.14683 (2024).\\n[47] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al.\\n2023.\\nC-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.\\nArXiv preprint abs/2305.08322 (2023).\\nhttps://arxiv.org/abs/2305.08322 [48] Drew A Hudson and Christopher D Manning.\\n2019.\\nGqa: A new dataset for real-world visual reasoning and compositional question answering.\\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\\n6700–6709.\\n[49] Jitesh Jain, Jianwei Yang, and Humphrey Shi.\\n2023.\\nVcoder: Versatile vision encoders for multimodal large language models.\\narXiv preprint arXiv:2312.14233 (2023).\\n[50] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo.\\n2022.\\nKnowledge unlearning for mitigating privacy risks in language models.\\narXiv preprint arXiv:2210.01504 (2022).\\n[51] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.\\n2021.\\nScaling up visual and vision-language representation learning with noisy text supervision.\\nIn International conference on machine learning.\\nPMLR, 4904–4916.\\n[52] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang.\\n2023.\\nHallucination Augmented Contrastive Learning for Multimodal Large Language Model.\\narXiv preprint arXiv:2312.06968 (2023).\\n[53] Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang.\\n2024.\\nHal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models.\\narXiv preprint arXiv:2402.15721 (2024).\\n[54] Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, and Ying Shen.\\n2024.\\nEnhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study.\\narXiv preprint arXiv:2401.17981 (2024).\\n[55] Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du.\\n2023.\\nFAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models.\\narXiv preprint arXiv:2311.01477 (2023).\\n[56] Andrej Karpathy.\\n2023.\\nOn the \"hallucination problem\".\\nhttps://twitter.com/karpathy/status/1733299213503787018 Access Date: 10 Mar.\\n2024.\\n[57] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.\\n2023.\\nSegment anything.\\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision.\\n4015–4026.\\n[58] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\\n2022.\\nLarge language models are zero-shot reasoners.\\nAdvances in neural information processing systems 35 (2022), 22199–22213.\\n[59] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.\\n2017.\\nVisual genome: Connecting language and vision using crowdsourced dense image annotations.\\nInternational journal of computer vision 123 (2017), 32–73.\\n[60] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\\n2012.\\nImagenet classification with deep convolutional neural networks.\\nAdvances in neural information processing systems 25 (2012).\\n[61] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al.\\n2020.\\nThe open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.\\nInternational journal of computer vision 128, 7 (2020), 1956–1981.\\nXin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.\\n2023.\\nLisa: Reasoning segmentation via large language model.\\narXiv preprint arXiv:2308.00692 (2023).\\n[63] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo.\\n2023.\\nVolcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision.\\narXiv preprint arXiv:2311.07362 (2023).\\n[64] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing.\\n2023.\\nMitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding.\\narXiv preprint arXiv:2311.16922 (2023).\\n[65] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.\\n2023.\\nOtter: A Multi-Modal Model with In-Context Instruction Tuning.\\narXiv preprint arXiv:2305.03726 (2023).\\n[66] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\\n2023.\\nBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\\narXiv preprint arXiv:2301.12597 (2023).\\n[67] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, and Yueting Zhuang.\\n2023.\\nFine-tuning multimodal llms to follow zero-shot demonstrative instructions.\\nIn The Twelfth International Conference on Learning Representations.\\n[68] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong.\\n2023.\\nSilkie: Preference distillation for large visual language models.\\narXiv preprint arXiv:2312.10665 (2023).\\n[69] Yifan Li, Yifan Du, Kun Zhou, JinpengWang, Wayne Xin Zhao, and Ji-RongWen.\\n2023.\\nEvaluating object hallucination in large vision-language models.\\narXiv preprint arXiv:2305.10355 (2023).\\n[70] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.\\n2014.\\nMicrosoft coco: Common objects in context.\\nIn Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13.\\nSpringer, 740–755.\\n[71] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al.\\n2023.\\nSphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.\\narXiv preprint arXiv:2311.07575 (2023).\\n[72] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou.\\n2023.\\nHallusionBench: You See What You Think?\\nOr You Think What You See?\\nAn Image-Context Reasoning Benchmark Challenging for GPT-4V(Ision), LLaVA-1.5, and Other Multi-Modality Models.\\nhttps://arxiv.org/abs/2310.14566 [73] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.\\n2023.\\nMitigating hallucination in large multi-modal models via robust instruction tuning.\\narXiv preprint arXiv:2306.14565 1 (2023).\\n[74] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\n2023.\\nImproved baselines with visual instruction tuning.\\narXiv preprint arXiv:2310.03744 (2023).\\n[75] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\n2023.\\nVisual instruction tuning.\\narXiv preprint arXiv:2304.08485 (2023).\\n[76] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng.\\n2024.\\nA survey on hallucination in large vision-language models.\\narXiv preprint arXiv:2402.00253 (2024).\\n[77] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung.\\n2023.\\nNegative object presence evaluation (nope) to measure object hallucination in vision-language models.\\narXiv preprint arXiv:2310.05338 (2023).\\n[78] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji.\\n2024.\\nCheap and quick: Efficient vision-language instruction tuning for large language models.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n[79] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna.\\n2023.\\nCREPE: Can VisionLanguage Foundation Models Reason Compositionally?.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n10910–10921.\\n[80] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.\\n2019.\\nOcr-vqa: Visual question answering by reading text in images.\\nIn 2019 international conference on document analysis and recognition (ICDAR).\\nIEEE, 947–952.\\n[81] OpenAI.\\n2022.\\nIntroducing chatgpt.\\nhttps://openai.com/blog/chatgpt [82] OpenAI.\\n2023.\\nGPT-4 Technical Report.\\nArXiv preprint abs/2303.08774 (2023).\\nhttps://arxiv.org/abs/2303.08774 [83] OpenAI.\\n2023.\\nGPT-4V(ision) System Card.\\nhttps://cdn.openai.com/papers/GPTV_System_Card.pdf [84] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022.\\nTraining language models to follow instructions with human feedback.\\nIn NeurIPS.\\nhttp://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html [85] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\\n2023.\\nThe RefinedWeb Dataset for Falcon LLM:\\nOutperforming Curated Corpora with Web Data, and Web Data Only.\\nArXiv preprint abs/2306.01116 (2023).\\nhttps://arxiv.org/abs/2306.01116 [86] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.\\n2023.\\nInstruction tuning with gpt-4.\\nArXiv preprint abs/2304.03277 (2023).\\nhttps://arxiv.org/abs/2304.03277 [87] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen.\\n2022.\\nReasoning with language model prompting: A survey.\\nArXiv preprint abs/2212.09597 (2022).\\nhttps://arxiv.org/abs/2212.09597 [88] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\\n2021.\\nLearning transferable visual models from natural language supervision.\\nIn International conference on machine learning.\\nPMLR, 8748–8763.\\n[89] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\\n2022.\\nHierarchical text-conditional image generation with clip latents.\\narXiv preprint arXiv:2204.06125 1, 2 (2022), 3.\\n[90] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko.\\n2018.\\nObject hallucination in image captioning.\\narXiv preprint arXiv:1809.02156 (2018).\\n[91] Anna Rohrbach, Makarand Tapaswi, Atousa Torabi, Tegan Maharaj, Marcus Rohrbach, Sanja Fidler Christopher Pal, and Bernt Schiele.\\n2017.\\nThe Joint Video and Language Understanding Workshop: MovieQA and The Large Scale Movie Description Challenge (LSMDC).\\n[92] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\\n2022.\\nLaion-5b: An open large-scale dataset for training next generation image-text models.\\nAdvances in Neural Information Processing Systems 35 (2022), 25278–25294.\\n[93] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\n2017.\\nProximal Policy Optimization Algorithms.\\nArXiv preprint abs/1707.06347 (2017).\\nhttps://arxiv.org/abs/1707.06347 [94] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano.\\n2020.\\nLearning to summarize with human feedback.\\nIn Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).\\nhttps://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html [95] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai.\\n2023.\\nPandagpt: One model to instruction-follow them all.\\narXiv preprint arXiv:2305.16355 (2023).\\n[96] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al.\\n2023.\\nAligning large multimodal models with factually augmented rlhf.\\narXiv preprint arXiv:2309.14525 (2023).\\n[97] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.\\n2023.\\nGemini: a family of highly capable multimodal models.\\narXiv preprint arXiv:2312.11805 (2023).\\n[98] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie.\\n2024.\\nEyes Wide Shut?\\nExploring the Visual Shortcomings of Multimodal LLMs.\\narXiv preprint arXiv:2401.06209 (2024).\\n[99] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\\n2023.\\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\\nArXiv preprint abs/2307.09288 (2023).\\nhttps://arxiv.org/abs/2307.09288 [100] Andrés Villa, Juan Carlos León Alcázar, Alvaro Soto, and Bernard Ghanem.\\n2023.\\nBehind the Magic, MERLIM:\\nMulti-modal Evaluation Benchmark for Large Image-Language Models.\\narXiv preprint arXiv:2312.02219 (2023).\\n[101] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.\\n2008.\\nExtracting and composing robust features with denoising autoencoders.\\nIn Proceedings of the 25th international conference on Machine learning.\\n1096–1103.\\n[102] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et al.\\n2023.\\nVigc: Visual instruction generation and correction.\\narXiv preprint arXiv:2308.12714 (2023).\\n[103] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang.\\n2023.\\nAn llm-free multi-dimensional benchmark for mllms hallucination evaluation.\\narXiv preprint arXiv:2311.07397 (2023).\\nJunyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al.\\n2023.\\nEvaluation and analysis of hallucination in large vision-language models.\\narXiv preprint arXiv:2308.15126 (2023).\\n[105] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim.\\n2023.\\nMitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites.\\narXiv preprint arXiv:2312.01701 (2023).\\n[106] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al.\\n2023.\\nCogvlm: Visual expert for pretrained language models.\\narXiv preprint arXiv:2311.03079 (2023).\\n[107] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\\n2022.\\nChain-of-thought prompting elicits reasoning in large language models.\\nAdvances in Neural Information Processing Systems 35 (2022), 24824–24837.\\n[108] Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, and Tieniu Tan.\\n2024.\\nLogical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models.\\narXiv preprint arXiv:2402.11622 (2024).\\n[109] Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, and Xinyu Dai.\\n2024.\\nEFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models.\\narXiv preprint arXiv:2402.09801 (2024).\\n[110] Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, and Li Erran Li.\\n2024.\\nViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling.\\narXiv preprint arXiv:2402.06118 (2024).\\n[111] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al.\\n2023. mplug-owl: Modularization empowers large language models with multimodality.\\narXiv preprint arXiv:2304.14178 (2023).\\n[112] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.\\n2023. mplugowl2: Revolutionizing multi-modal large language model with modality collaboration.\\narXiv preprint arXiv:2311.04257 (2023).\\n[113] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.\\n2023.\\nA survey on multimodal large language models.\\narXiv preprint arXiv:2306.13549 (2023).\\n[114] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen.\\n2023.\\nWoodpecker: Hallucination Correction for Multimodal Large Language Models.\\nhttps: //arxiv.org/abs/2310.16045 [115] Fei Yu, Hongbo Zhang, and Benyou Wang.\\n2023.\\nNature language reasoning, a survey.\\nArXiv preprint abs/2303.14725 (2023).\\nhttps://arxiv.org/abs/2303.14725 [116] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.\\n[n.\\nd.].\\nCoca: Contrastive captioners are image-text foundation models.\\narXiv 2022. arXiv preprint arXiv:2205.01917 ([n. d.]).\\n[117] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang.\\n2023.\\nHalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data.\\narXiv preprint arXiv:2311.13614 (2023).\\n[118] Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, Chongyi Wang, Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, et al.\\n2023.\\nReformulating vision-language foundation models and datasets towards universal multimodal assistants.\\narXiv preprint arXiv:2310.00653 (2023).\\n[119] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al.\\n2023.\\nRLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback.\\narXiv preprint arXiv:2312.00849 (2023).\\n[120] Zihao Yue, Liang Zhang, and Qin Jin.\\n2024.\\nLess is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective.\\narXiv preprint arXiv:2402.14545 (2024).\\n[121] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou.\\n2022.\\nWhen and Why VisionLanguage Models Behave like Bags-Of-Words, and What to Do About It?.\\nIn The Eleventh International Conference on Learning Representations.\\n[122] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong.\\n2023.\\nWhat matters in training a gpt4-style language model with multimodal inputs?\\narXiv preprint arXiv:2307.02469 (2023).\\n[123] Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, and Manling Li.\\n2023.\\nHallE-Switch: Controlling Object Hallucination in Large Vision Language Models.\\narXiv e-prints (2023), arXiv–2310.\\n[124] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua.\\n2024.\\nVPGTrans: Transfer visual prompt generator across LLMs.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n[125] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al.\\n2023.\\nInstruction Tuning for Large Language Models: A Survey.\\nArXiv preprint abs/2308.10792 (2023).\\nhttps://arxiv.org/abs/2308.10792 Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.\\n2023.\\nGpt4roi: Instruction tuning large language model on region-of-interest.\\narXiv preprint arXiv:2307.03601 (2023).\\n[127] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.\\n2019.\\nBertscore: Evaluating text generation with bert.\\narXiv preprint arXiv:1904.09675 (2019).\\n[128] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto.\\n2023.\\nBenchmarking large language models for news summarization.\\nArXiv preprint abs/2301.13848 (2023).\\nhttps: //arxiv.org/abs/2301.13848 [129] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, LongyueWang, Anh Tuan Luu,Wei Bi, Freda Shi, and Shuming Shi.\\n2023.\\nSiren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.\\nArXiv preprint abs/2309.01219 (2023).\\nhttps://arxiv.org/abs/2309.01219 [130] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang.\\n2023.\\nMmicl: Empowering vision-language model with multi-modal in-context learning.\\narXiv preprint arXiv:2309.07915 (2023).\\n[131] Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu.\\n2024.\\nMitigating Object Hallucination in Large VisionLanguage Models via Classifier-Free Guidance.\\narXiv preprint arXiv:2402.08680 (2024).\\n[132] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.\\n2023.\\nA survey of large language models.\\nArXiv preprint abs/2303.18223 (2023).\\nhttps://arxiv.org/abs/2303.18223 [133] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He.\\n2023.\\nBeyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization.\\narXiv preprint arXiv:2311.16839 (2023).\\n[134] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al.\\n2023.\\nLima: Less is more for alignment.\\nArXiv preprint abs/2305.11206 (2023).\\nhttps://arxiv.org/abs/2305.11206 [135] Qiang Zhou, Zhibin Wang, Wei Chu, Yinghui Xu, Hao Li, and Yuan Qi.\\n2023.\\nInfMLLM: A Unified Framework for Visual-Language Tasks.\\narXiv:2311.06791 [cs.CV] [136] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao.\\n2024.\\nAligning Modalities in Vision Large Language Models via Preference Fine-tuning.\\narXiv preprint arXiv:2402.11411 (2024).\\n[137] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.\\n2023.\\nAnalyzing and mitigating object hallucination in large vision-language models.\\narXiv preprint arXiv:2310.00754 (2023).\\n[138] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\\n2023.\\nMinigpt-4: Enhancing vision-language understanding with advanced large language models.\\narXiv preprint arXiv:2304.10592 (2023).\\n[139] Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, and Jun Liu.\\n2024.\\nIBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding.\\narXiv preprint arXiv:2402.18476 (2024).\\n[140] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang.\\n2023.\\nMultilingual machine translation with large language models: Empirical results and analysis.\\nArXiv preprint abs/2304.04675 (2023).\\nhttps://arxiv.org/abs/2304.04675']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = []\n",
    "tables = []\n",
    "for doc in docs_sections:\n",
    "    for sec in doc:\n",
    "        context.append(sec[\"text\"])\n",
    "        tables.append(sec[\"tables\"])\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Summary of \"Hallucination of Multimodal Large Language Models: A Survey\"\\n\\nThis paper explores the issue of **hallucination** in **multimodal large language models (MLLMs)**, which are AI systems that combine language processing with other modalities like vision. While MLLMs show great promise in tasks like image captioning and visual question answering, they can generate outputs that are inconsistent with the visual content, known as hallucinations. This raises concerns about their reliability and practicality.\\n\\n**Causes of Hallucination:**\\n\\n* **Data Issues:** Insufficient data, noisy data, lack of diversity in data (e.g., mainly positive instructions), and statistical biases (e.g., frequent objects) can all contribute to hallucinations.\\n* **Model Issues:** Weak vision models, imbalanced architecture favoring language models, and poorly aligned interfaces between modalities can lead to misinterpretations and hallucinations.\\n* **Training Issues:** The standard next-token prediction loss may not be ideal for learning visual information, and the lack of RLHF (reinforcement learning from human feedback) in MLLM training can result in misalignment with human preferences.\\n* **Inference Issues:** As MLLMs generate text, they may lose focus on the visual content over time, relying more on previously generated text and leading to hallucinations.\\n\\n**Evaluation of Hallucinations:**\\n\\nThe paper reviews various benchmarks and metrics used to assess the severity of hallucinations, such as:\\n\\n* **CHAIR**: Measures the proportion of generated words that are actually present in the image.\\n* **POPE**: Evaluates object hallucination through yes/no questions about specific objects.\\n* **AMBER**: Assesses both generative and discriminative tasks, considering object existence, attributes, and relations.\\n* **HallusionBench**: Diagnoses potential failure modes by evaluating visual commonsense knowledge and reasoning.\\n* **FaithScore**: Analyzes free-form responses to open-ended questions, identifying hallucinated entities, attributes, and relations.\\n\\n**Mitigation Strategies:**\\n\\n* **Data-centric**: Introducing negative and counterfactual data, along with refining existing datasets, can help improve data quality and diversity.\\n* **Model-centric**: Scaling up vision resolution, incorporating diverse vision encoders, and adding dedicated modules to control language priors can enhance visual understanding and reduce reliance on language bias.\\n* **Training-centric**: Employing auxiliary supervision signals and reinforcement learning techniques can improve cross-modal alignment and model faithfulness.\\n* **Inference-centric**: Techniques like contrastive decoding and guided decoding can steer generation towards accurate representations of visual content, while post-hoc correction methods can identify and rectify hallucinations after generation.\\n\\n**Challenges and Future Directions:**\\n\\n* Addressing data quality and bias.\\n* Improving cross-modal alignment and consistency.\\n* Developing advanced model architectures.\\n* Establishing standardized benchmarks and metrics.\\n* Exploring the potential of hallucinations as a creative feature.\\n* Enhancing interpretability and building trust in MLLMs.\\n* Navigating ethical considerations and responsible AI development.\\n\\n**Overall, the paper highlights the need for continuous research and innovation to mitigate hallucinations in MLLMs and ensure their reliable and ethical deployment in real-world applications.** \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    ['# Here is a research paper that the user who is an AI Engineer is reading:']+\n",
    "    context +\n",
    "    [\"[END]\\n\\nPlease sumarize it\"]\n",
    ")\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summary of \"Hallucination of Multimodal Large Language Models: A Survey\"\n",
       "\n",
       "This paper explores the issue of **hallucination** in **multimodal large language models (MLLMs)**, which are AI systems that combine language processing with other modalities like vision. While MLLMs show great promise in tasks like image captioning and visual question answering, they can generate outputs that are inconsistent with the visual content, known as hallucinations. This raises concerns about their reliability and practicality.\n",
       "\n",
       "**Causes of Hallucination:**\n",
       "\n",
       "* **Data Issues:** Insufficient data, noisy data, lack of diversity in data (e.g., mainly positive instructions), and statistical biases (e.g., frequent objects) can all contribute to hallucinations.\n",
       "* **Model Issues:** Weak vision models, imbalanced architecture favoring language models, and poorly aligned interfaces between modalities can lead to misinterpretations and hallucinations.\n",
       "* **Training Issues:** The standard next-token prediction loss may not be ideal for learning visual information, and the lack of RLHF (reinforcement learning from human feedback) in MLLM training can result in misalignment with human preferences.\n",
       "* **Inference Issues:** As MLLMs generate text, they may lose focus on the visual content over time, relying more on previously generated text and leading to hallucinations.\n",
       "\n",
       "**Evaluation of Hallucinations:**\n",
       "\n",
       "The paper reviews various benchmarks and metrics used to assess the severity of hallucinations, such as:\n",
       "\n",
       "* **CHAIR**: Measures the proportion of generated words that are actually present in the image.\n",
       "* **POPE**: Evaluates object hallucination through yes/no questions about specific objects.\n",
       "* **AMBER**: Assesses both generative and discriminative tasks, considering object existence, attributes, and relations.\n",
       "* **HallusionBench**: Diagnoses potential failure modes by evaluating visual commonsense knowledge and reasoning.\n",
       "* **FaithScore**: Analyzes free-form responses to open-ended questions, identifying hallucinated entities, attributes, and relations.\n",
       "\n",
       "**Mitigation Strategies:**\n",
       "\n",
       "* **Data-centric**: Introducing negative and counterfactual data, along with refining existing datasets, can help improve data quality and diversity.\n",
       "* **Model-centric**: Scaling up vision resolution, incorporating diverse vision encoders, and adding dedicated modules to control language priors can enhance visual understanding and reduce reliance on language bias.\n",
       "* **Training-centric**: Employing auxiliary supervision signals and reinforcement learning techniques can improve cross-modal alignment and model faithfulness.\n",
       "* **Inference-centric**: Techniques like contrastive decoding and guided decoding can steer generation towards accurate representations of visual content, while post-hoc correction methods can identify and rectify hallucinations after generation.\n",
       "\n",
       "**Challenges and Future Directions:**\n",
       "\n",
       "* Addressing data quality and bias.\n",
       "* Improving cross-modal alignment and consistency.\n",
       "* Developing advanced model architectures.\n",
       "* Establishing standardized benchmarks and metrics.\n",
       "* Exploring the potential of hallucinations as a creative feature.\n",
       "* Enhancing interpretability and building trust in MLLMs.\n",
       "* Navigating ethical considerations and responsible AI development.\n",
       "\n",
       "**Overall, the paper highlights the need for continuous research and innovation to mitigate hallucinations in MLLMs and ensure their reliable and ethical deployment in real-world applications.** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could display each table as HTML/MARKDOWN besides its summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tables[5][0].to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Simplified Summary of the Model Comparison Table:\n",
       "\n",
       "This table compares several large language models (LLMs) across different aspects of language understanding and generation:\n",
       "\n",
       "* **Morphology:** Understanding the structure of words. \n",
       "* **Syntax:** Understanding the arrangement of words and phrases to create well-formed sentences.\n",
       "* **Semantics:** Understanding the meaning of words and sentences.\n",
       "* **Reasoning:** The ability to draw logical conclusions and make inferences.\n",
       "* **Discourse:** Understanding the flow and context of language in a larger piece of text.\n",
       "\n",
       "**Key Observations:**\n",
       "\n",
       "* **Most models show improvement in morphology and syntax compared to their predecessors.** This suggests progress in understanding the structure and grammar of language.\n",
       "* **Semantics and discourse are areas where several models struggle.** This indicates difficulty in comprehending meaning and context, leading to potential issues with factual accuracy and logical coherence.\n",
       "* **Overall performance varies significantly across models.** Some models like Vicuna-v1.5 and FLAN-UL2 show strong overall improvements, while others like Dolly-v2 and Tülu-2 exhibit declines compared to their base models.\n",
       "* **The average across all models suggests a slight improvement in overall language capabilities.** However, there's still much room for advancement, particularly in semantics and discourse understanding.\n",
       "\n",
       "**Important Notes:**\n",
       "\n",
       "* The table uses percentages to show relative improvements or declines compared to a base model or a specific parameter count.\n",
       "* The \"Overall\" column provides a general indication of performance, but it's essential to consider individual strengths and weaknesses based on the specific task or application. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_tables = model.generate_content(\n",
    "    ['# Here is a table extracted from a research paper that you have summarized:']+\n",
    "    [table] +\n",
    "    [\"# This is the summary:\"] +\n",
    "    [response.text] +\n",
    "    [\"[END]\\n\\nPlease sumarize the table in a simplified way\"]\n",
    ")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(response_tables.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract urls from section text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " ['hessian.AI',\n",
       "  'www.ukp.tu-darmstadt.de',\n",
       "  'www.hslu.ch',\n",
       "  'holmes-benchmark.github.io'],\n",
       " ['holmes-benchmark.github.io'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['OpenReview.net',\n",
       "  'OpenReview.net',\n",
       "  'https://huggingface.co/spaces/',\n",
       "  'OpenReview.net',\n",
       "  'OpenReview.net',\n",
       "  'OpenReview.net',\n",
       "  'OpenReview.net',\n",
       "  'OpenReview.net',\n",
       "  'view.net',\n",
       "  'OpenReview.net',\n",
       "  'OpenReview.net',\n",
       "  'CEUR-WS.org',\n",
       "  'OpenReview.net'],\n",
       " ['OpenReview.net'],\n",
       " ['OpenReview.net',\n",
       "  'OpenReview.net',\n",
       "  'OpenReview.net',\n",
       "  'OpenReview.net',\n",
       "  'view.net',\n",
       "  'OpenReview.net',\n",
       "  'OpenReview.net',\n",
       "  'CEUR-WS.org',\n",
       "  'OpenReview.net'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['stylus-diffusion.github.io', 'https://civitai.com/'],\n",
       " ['stylus-diffusion.github.io', 'https://civitai.com/'],\n",
       " ['stylus-diffusion.github.io', 'https://civitai.com/'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['https://github.com/huggingface/peft,'],\n",
       " ['https://github.com/huggingface/peft,'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['https://github.com/showlab/Awesome-MLLM-Hallucination.'],\n",
       " [],\n",
       " ['https://doi.org/XXXXXXX.XXXXXXX', 'https://doi.org/XXXXXXX.XXXXXXX'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['https://www.adept.ai/blog/fuyu-8b',\n",
       "  'https://www.jstor.org/stable/2334029',\n",
       "  'https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html',\n",
       "  'http://jmlr.org/papers/v24/22-1144.html',\n",
       "  'https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html',\n",
       "  'https://arxiv.org/abs/2309.15402',\n",
       "  'cs.CV',\n",
       "  'cs.CV',\n",
       "  'https://bard.google.com/',\n",
       "  'https://arxiv.org/abs/2308.06394',\n",
       "  'OpenReview.net',\n",
       "  'https://openreview.net/forum?id=d7KBjmI3GmQ',\n",
       "  'https://arxiv.org/abs/2305.08322',\n",
       "  'https://twitter.com/karpathy/status/1733299213503787018',\n",
       "  'https://arxiv.org/abs/2310.14566',\n",
       "  'https://openai.com/blog/chatgpt',\n",
       "  'https://arxiv.org/abs/2303.08774',\n",
       "  'https://cdn.openai.com/papers/GPTV_System_Card.pdf',\n",
       "  'http://papers.nips.cc/paper_files/paper/2022/hash/',\n",
       "  'https://arxiv.org/abs/2306.01116',\n",
       "  'https://arxiv.org/abs/2304.03277',\n",
       "  'https://arxiv.org/abs/2212.09597',\n",
       "  'https://arxiv.org/abs/1707.06347',\n",
       "  'https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html',\n",
       "  'https://arxiv.org/abs/2307.09288',\n",
       "  'arxiv.org/abs/2310.16045',\n",
       "  'https://arxiv.org/abs/2303.14725',\n",
       "  'https://arxiv.org/abs/2308.10792',\n",
       "  'arxiv.org/abs/2301.13848',\n",
       "  'https://arxiv.org/abs/2309.01219',\n",
       "  'https://arxiv.org/abs/2303.18223',\n",
       "  'https://arxiv.org/abs/2305.11206',\n",
       "  'cs.CV',\n",
       "  'https://arxiv.org/abs/2304.04675']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urlextract import URLExtract\n",
    "\n",
    "extractor = URLExtract()\n",
    "urls = []\n",
    "for doc in docs_sections:\n",
    "    for section in doc:\n",
    "        url = extractor.find_urls(section[\"text\"])\n",
    "        urls.append(url) \n",
    "        \n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://github.com/Holmes-Benchmark/holmes-evaluation'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def find_github_repo_inwebpage(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Regular expression to match GitHub repository homepage URLs\n",
    "    github_repo_url_pattern = re.compile(r'https?://github\\.com/[\\w-]+/[\\w-]+/?$')\n",
    "    \n",
    "    # Find all links in the parsed HTML\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Filter and return GitHub repository homepage URLs\n",
    "    github_repo_urls = {link['href'] for link in links if github_repo_url_pattern.match(link['href'])}\n",
    "    return github_repo_urls\n",
    "\n",
    "# Example usage:\n",
    "url_to_scrape = 'https://holmes-benchmark.github.io/'  # Replace with the URL of the website you want to scrape\n",
    "github_urls = find_github_repo_inwebpage(url_to_scrape)\n",
    "print(github_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need an agent to filter the urls to get the paper's code URLS only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_github_repo_urls_in_text(text):\n",
    "    # Regular expression to match GitHub repository homepage URLs\n",
    "    github_repo_url_pattern = re.compile(r'https?://github\\.com/[\\w-]+/[\\w-]+/?$')\n",
    "    \n",
    "    # Search the text for matches\n",
    "    github_repo_urls = github_repo_url_pattern.findall(text)\n",
    "    \n",
    "    # Remove duplicates and return the results\n",
    "    return set(github_repo_urls)\n",
    "\n",
    "# Example usage:\n",
    "text_to_scan = \"\"\"\n",
    "Here are some GitHub repos you might find interesting: \n",
    "https://github.com/Holmes-Benchmark/holmes-evaluation, and also check out https://github.com/octocat/Spoon-Knife/.\n",
    "Don't forget https://github.com/octocat/Spoon-Knife/issues which is not what we want.\n",
    "\"\"\"\n",
    "github_repo_urls = find_github_repo_urls_in_text(text_to_scan)\n",
    "print(github_repo_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need an agent to analyze a github repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summary of the Holmes Benchmark Repository\n",
       "\n",
       "The Holmes Benchmark repository provides code and resources for evaluating and analyzing hallucination in Multimodal Large Language Models (MLLMs). It includes implementations of various evaluation metrics, datasets for benchmarking, and tools for visualization and analysis. \n",
       "\n",
       "Here's a breakdown of the key components:\n",
       "\n",
       "* **Evaluation Metrics:**\n",
       "    * **CHAIR**: Implementation for measuring the proportion of generated words grounded in the image.\n",
       "    * **POPE**: Code for evaluating object hallucination using yes/no questions.\n",
       "    * **AMBER**: Scripts for assessing both generative and discriminative tasks related to object existence, attributes, and relationships.\n",
       "    * **HallusionBench**: Tools for diagnosing potential failure modes by evaluating visual commonsense and reasoning.\n",
       "    * **FaithScore**: Code for analyzing free-form responses to open-ended questions and identifying hallucinated entities, attributes, and relations.\n",
       "* **Datasets:**\n",
       "    * Scripts for downloading and preparing various datasets used for hallucination benchmarking, such as COCO-Captions and Visual Genome.\n",
       "* **Visualization and Analysis Tools:** \n",
       "    * Jupyter notebooks for visualizing hallucination examples and analyzing evaluation results.\n",
       "    * Scripts for generating reports and comparing different models or metrics. \n",
       "\n",
       "## Guide for AI Engineers: Using Holmes Benchmark for Hallucination Analysis\n",
       "\n",
       "**Scenario:** You are an AI Engineer working on developing a new MLLM for image captioning. You want to evaluate its performance and understand the extent of hallucination issues in its generated captions. \n",
       "\n",
       "**Here's how you can utilize the Holmes Benchmark repository:**\n",
       "\n",
       "**1. Setup:**\n",
       "\n",
       "* Clone the repository: `git clone https://github.com/Holmes-Benchmark/holmes-evaluation.git`\n",
       "* Install the required dependencies using `pip install -r requirements.txt`.\n",
       "\n",
       "**2. Data Preparation:**\n",
       "\n",
       "* Choose a relevant dataset for your task, e.g., COCO-Captions.\n",
       "* Use the provided scripts to download and prepare the dataset. \n",
       "\n",
       "**3. Model Evaluation:**\n",
       "\n",
       "* Select appropriate metrics based on your focus. For example:\n",
       "    * Use **CHAIR** to measure the overall grounding of generated captions.\n",
       "    * Use **POPE** to assess object hallucination specifically.\n",
       "    * Use **HallusionBench** to analyze potential failure modes in visual reasoning.\n",
       "* Run your MLLM on the chosen dataset and generate captions.\n",
       "* Use the corresponding scripts to calculate the chosen metrics for your generated captions.\n",
       "\n",
       "**4. Analysis and Visualization:**\n",
       "\n",
       "* Utilize the provided Jupyter notebooks to visualize examples of hallucinations and analyze the distribution of scores across different categories.\n",
       "* Compare your MLLM's performance with other models or baselines using the available tools.\n",
       "\n",
       "**5. Mitigation Strategies:**\n",
       "\n",
       "* Based on the insights gained from the evaluation, identify areas where your MLLM struggles with hallucinations. \n",
       "* Consider implementing mitigation strategies mentioned in the paper, such as:\n",
       "    * **Data-centric:** Improve data quality and diversity by adding negative examples or counterfactual data.\n",
       "    * **Model-centric:** Enhance visual understanding by using stronger vision models or incorporating dedicated modules to control language priors.\n",
       "    * **Training-centric:**  Improve cross-modal alignment using auxiliary supervision signals or reinforcement learning techniques. \n",
       "    * **Inference-centric:**  Explore techniques like contrastive decoding or guided decoding to steer generation towards factual representations.\n",
       "\n",
       "**6. Iterate and Improve:**\n",
       "\n",
       "* Continue to evaluate your MLLM as you refine your model and training process. \n",
       "* Monitor the progress in mitigating hallucinations and track the improvement in relevant metrics.\n",
       "\n",
       "**Additional Tips:**\n",
       "\n",
       "* The repository provides flexibility to customize the evaluation process based on your specific needs. \n",
       "* Explore the various configuration options and parameters available for each metric.\n",
       "* Consider contributing to the repository by adding new metrics, datasets, or analysis tools. \n",
       "\n",
       "By leveraging the Holmes Benchmark repository, AI Engineers can gain valuable insights into the hallucination behavior of their MLLMs and take steps to improve their accuracy and reliability for real-world applications.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_code= model.generate_content(\n",
    "    ['# Here is a github repository containing code for a research paper you summarized']+\n",
    "    [\"https://github.com/Holmes-Benchmark/holmes-evaluation\"] +\n",
    "    [\"# This is the summary:\"] +\n",
    "    [response.text] +\n",
    "    [\"[END]\\n\\nPlease sumarize the code in the repository and provide a detailed guide showing an AI Engineering how they may use this code for a relevant task\"]\n",
    ")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(response_code.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
